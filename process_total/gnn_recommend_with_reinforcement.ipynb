{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ê°œì„ ëœ í”¼ë“œë°± ê¸°ë°˜ ê²½ë¡œ ëŒ€ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘!\n",
      "============================================================\n",
      "ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: cpu\n",
      "ğŸ“… ì—¬í–‰ ê¸°ê°„: 2025-09-28 - 2025-09-29 (2ì¼)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1027\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- improved_travel_data.pkl: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë° ìŠ¤ì¼€ì¼ëŸ¬\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1027\u001b[0m     \u001b[43mmain_feedback_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 889\u001b[0m, in \u001b[0;36mmain_feedback_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m    886\u001b[0m recommender \u001b[38;5;241m=\u001b[39m SmartRecommendationEngine(model, visit_area_df, device)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# ì´ˆê¸° ì¶”ì²œ (í•„í„°ë§ ì ìš©, ê±°ë¦¬ ê³ ë ¤)\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m recommendations, embeddings, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrecommender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_recommendations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtravel_context_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiversity_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_useless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsider_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# ì¤‘ë³µ ë°©ë¬¸ì§€ ì œê±° ë° ìœ íš¨ì„± ê²€ì‚¬\u001b[39;00m\n\u001b[1;32m    895\u001b[0m unique_recommendations, seen_ids \u001b[38;5;241m=\u001b[39m [], \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 255\u001b[0m, in \u001b[0;36mSmartRecommendationEngine.get_recommendations\u001b[0;34m(self, data, travel_context, top_k, diversity_weight, excluded_ids, filter_useless, consider_distance)\u001b[0m\n\u001b[1;32m    252\u001b[0m min_allowed_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m    253\u001b[0m scores[scores \u001b[38;5;241m<\u001b[39m min_allowed_score] \u001b[38;5;241m=\u001b[39m min_allowed_score\n\u001b[0;32m--> 255\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distance_aware_recommendation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiversity_weight\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m recommendations, embeddings, preference_scores\n",
      "Cell \u001b[0;32mIn[4], line 303\u001b[0m, in \u001b[0;36mSmartRecommendationEngine._distance_aware_recommendation\u001b[0;34m(self, scores, embeddings, top_k, diversity_weight)\u001b[0m\n\u001b[1;32m    301\u001b[0m similarities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rec_idx \u001b[38;5;129;01min\u001b[39;00m recommendations:\n\u001b[0;32m--> 303\u001b[0m     sim \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m:\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrec_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mrec_idx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    307\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mappend(sim)\n\u001b[1;32m    309\u001b[0m diversity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mmax\u001b[39m(similarities) \u001b[38;5;28;01mif\u001b[39;00m similarities \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, GATConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cdist\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ImprovedTravelGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, travel_context_dim, \n",
    "                 num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # GNN layers with edge features\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True, \n",
    "                           edge_dim=5)  # edge features ê³ ë ¤\n",
    "        self.gat2 = GATConv(hidden_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True,\n",
    "                           edge_dim=5)\n",
    "        self.gat3 = GATConv(hidden_channels, out_channels, \n",
    "                           heads=1, dropout=dropout, concat=False,\n",
    "                           edge_dim=5)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Travel context encoder\n",
    "        self.travel_encoder = nn.Sequential(\n",
    "            nn.Linear(travel_context_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # Distance-aware attention module\n",
    "        self.distance_attention = nn.Sequential(\n",
    "            nn.Linear(2, hidden_channels // 2),  # x, y coordinates\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Fusion network\n",
    "        self.fusion_net = nn.Sequential(\n",
    "            nn.Linear(out_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # Preference head\n",
    "        self.preference_head = nn.Sequential(\n",
    "            nn.Linear(out_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, data, travel_context, return_attention=False):\n",
    "        x = data['visit_area'].x\n",
    "        edge_index = data['visit_area', 'moved_to', 'visit_area'].edge_index\n",
    "        edge_attr = data['visit_area', 'moved_to', 'visit_area'].edge_attr\n",
    "        \n",
    "        # Extract coordinates for distance attention\n",
    "        coords = x[:, :2]  # Assuming first two features are X_COORD, Y_COORD\n",
    "        \n",
    "        # GNN layers with edge features\n",
    "        x1 = self.gat1(x, edge_index, edge_attr)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        x2 = self.gat2(x1, edge_index, edge_attr)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2 + x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        graph_embedding = self.gat3(x2, edge_index, edge_attr)\n",
    "        graph_embedding = self.bn3(graph_embedding)\n",
    "        \n",
    "        # Apply distance-based attention\n",
    "        distance_weights = self.distance_attention(coords)\n",
    "        graph_embedding = graph_embedding * distance_weights\n",
    "        \n",
    "        # Travel context encoding\n",
    "        travel_embedding = self.travel_encoder(travel_context)\n",
    "        travel_embedding_expanded = travel_embedding.expand(graph_embedding.size(0), -1)\n",
    "        \n",
    "        # Fusion\n",
    "        fused_features = torch.cat([graph_embedding, travel_embedding_expanded], dim=1)\n",
    "        final_embedding = self.fusion_net(fused_features)\n",
    "        \n",
    "        # Preference scores\n",
    "        preference_scores = self.preference_head(final_embedding)\n",
    "        \n",
    "        return final_embedding, preference_scores\n",
    "\n",
    "class EnhancedDataProcessor:\n",
    "    def __init__(self):\n",
    "        self.visit_scaler = RobustScaler()\n",
    "        self.travel_scaler = StandardScaler()\n",
    "        # ì œì™¸í•  í‚¤ì›Œë“œ ëª©ë¡\n",
    "        self.exclude_keywords = [\n",
    "            'ì—­', 'í„°ë¯¸ë„', 'ê³µí•­', 'íœ´ê²Œì†Œ', 'ì •ë¥˜ì¥', 'í†¨ê²Œì´íŠ¸', 'êµì°¨ë¡œ', 'ì¶œêµ¬', 'ì…êµ¬',\n",
    "            'IC', 'JC', 'ë‚˜ë“¤ëª©', 'ë¶„ê¸°ì ', 'ìš”ê¸ˆì†Œ', 'ì£¼ì°¨ì¥', 'ì£¼ìœ ì†Œ', 'ì¶©ì „ì†Œ',\n",
    "            'ì•„íŒŒíŠ¸', 'ì›ë£¸', 'ì˜¤í”¼ìŠ¤í…”', 'ë¹Œë¼', 'ì£¼íƒ', 'ë¹Œë”©', 'ìƒê°€', 'ëª¨í…”'\n",
    "        ]\n",
    "        \n",
    "    def should_exclude_location(self, name):\n",
    "        \"\"\"ìœ„ì¹˜ë¥¼ ì œì™¸í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸\"\"\"\n",
    "        if pd.isna(name):\n",
    "            return False\n",
    "        name_str = str(name).lower()\n",
    "        \n",
    "        # íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ë˜ê³ , ë‹¤ë¥¸ ìœ ìš©í•œ ë‹¨ì–´ê°€ ì—†ëŠ” ê²½ìš° ì œì™¸\n",
    "        for keyword in self.exclude_keywords:\n",
    "            if keyword.lower() in name_str:\n",
    "                # ì˜ˆì™¸ ì²˜ë¦¬: ê´€ê´‘ì§€ë¡œì„œì˜ ì—­í• ì´ ìˆëŠ” ê²½ìš°\n",
    "                tourist_keywords = ['ê´€ê´‘', 'í…Œë§ˆ', 'íŒŒí¬', 'ëœë“œ', 'ì›”ë“œ', 'ë¦¬ì¡°íŠ¸', 'í˜¸í…”', \n",
    "                                   'ë§›ì§‘', 'ì‹ë‹¹', 'ì¹´í˜', 'ë°•ë¬¼ê´€', 'ì „ì‹œ', 'ê°¤ëŸ¬ë¦¬', 'ë¬¸í™”']\n",
    "                if any(tk in name_str for tk in tourist_keywords):\n",
    "                    continue\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def process_visit_area_features(self, visit_area_df):\n",
    "        visit_area_df = visit_area_df.copy()\n",
    "        \n",
    "        # ì¢Œí‘œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "        visit_area_df['X_COORD'] = visit_area_df['X_COORD'].fillna(visit_area_df['X_COORD'].mean())\n",
    "        visit_area_df['Y_COORD'] = visit_area_df['Y_COORD'].fillna(visit_area_df['Y_COORD'].mean())\n",
    "        visit_area_df['VISIT_CHC_REASON_CD'] = visit_area_df['VISIT_CHC_REASON_CD'].fillna(0)\n",
    "        \n",
    "        features = visit_area_df[['X_COORD', 'Y_COORD']].copy()\n",
    "        \n",
    "        # One-hot encoding\n",
    "        type_onehot = pd.get_dummies(visit_area_df['VISIT_AREA_TYPE_CD'], prefix='type')\n",
    "        reason_onehot = pd.get_dummies(visit_area_df['VISIT_CHC_REASON_CD'].fillna(0), prefix='reason')\n",
    "        \n",
    "        # ì •ê·œí™”ëœ ë§Œì¡±ë„ ì ìˆ˜\n",
    "        for col in ['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']:\n",
    "            visit_area_df[col] = visit_area_df[col].fillna(3)\n",
    "            visit_area_df[f'{col}_norm'] = (visit_area_df[col] - 1) / 4.0\n",
    "        \n",
    "        # ì¸ê¸°ë„ ì ìˆ˜\n",
    "        visit_area_df['popularity_score'] = (\n",
    "            visit_area_df['DGSTFN_norm'] * 0.4 + \n",
    "            visit_area_df['REVISIT_INTENTION_norm'] * 0.3 + \n",
    "            visit_area_df['RCMDTN_INTENTION_norm'] * 0.3\n",
    "        )\n",
    "        \n",
    "        # ì œì™¸í•  ì¥ì†Œì— ëŒ€í•œ í˜ë„í‹° ì¶”ê°€\n",
    "        exclude_penalty = visit_area_df['VISIT_AREA_NM'].apply(self.should_exclude_location).astype(float) * -0.5\n",
    "        \n",
    "        # ëª¨ë“  íŠ¹ì„± ê²°í•©\n",
    "        features = pd.concat([\n",
    "            features, type_onehot, reason_onehot,\n",
    "            visit_area_df[['DGSTFN_norm', 'REVISIT_INTENTION_norm', 'RCMDTN_INTENTION_norm', 'popularity_score']],\n",
    "            pd.DataFrame({'exclude_penalty': exclude_penalty})\n",
    "        ], axis=1)\n",
    "        \n",
    "        return self.visit_scaler.fit_transform(features.values.astype(np.float32))\n",
    "    \n",
    "    def create_enhanced_edges(self, move_df, visit_area_df):\n",
    "        edges = []\n",
    "        edge_weights = []\n",
    "        \n",
    "        for travel_id, group in move_df.groupby(\"TRAVEL_ID\"):\n",
    "            group = group.sort_values(\"TRIP_ID\").reset_index(drop=True)\n",
    "            \n",
    "            for i in range(1, len(group)):\n",
    "                from_id = group.loc[i-1, \"NEW_END_VISIT_AREA_ID\"]\n",
    "                to_id = group.loc[i, \"NEW_END_VISIT_AREA_ID\"]\n",
    "                \n",
    "                if pd.notna(from_id) and pd.notna(to_id):\n",
    "                    duration = group.loc[i, \"DURATION_MINUTES\"] if \"DURATION_MINUTES\" in group.columns else 0\n",
    "                    transport = group.loc[i, \"MVMN_CD_1\"] if \"MVMN_CD_1\" in group.columns else 0\n",
    "                    \n",
    "                    edges.append([int(from_id), int(to_id), duration, transport])\n",
    "                    edge_weights.append(1.0)\n",
    "        \n",
    "        edges_df = pd.DataFrame(edges, columns=[\"FROM_ID\", \"TO_ID\", \"DURATION_MINUTES\", \"MVMN_CD_1\"])\n",
    "        \n",
    "        # êµí†µìˆ˜ë‹¨ ì›í•« ì¸ì½”ë”©\n",
    "        edges_df[\"MVMN_TYPE\"] = edges_df[\"MVMN_CD_1\"].apply(\n",
    "            lambda code: \"drive\" if code in [1,2,3] else \"public\" if code in [4,5,6,7,8,9,10,11,12,13,50] else \"other\"\n",
    "        )\n",
    "        edges_df[\"is_drive\"] = (edges_df[\"MVMN_TYPE\"] == \"drive\").astype(int)\n",
    "        edges_df[\"is_public\"] = (edges_df[\"MVMN_TYPE\"] == \"public\").astype(int)\n",
    "        edges_df[\"is_other\"] = (edges_df[\"MVMN_TYPE\"] == \"other\").astype(int)\n",
    "        \n",
    "        edge_index = torch.tensor(edges_df[[\"FROM_ID\", \"TO_ID\"]].values.T, dtype=torch.long)\n",
    "        edge_attr = torch.tensor(np.column_stack([\n",
    "            edges_df[[\"DURATION_MINUTES\"]].fillna(0).values,\n",
    "            edges_df[[\"is_drive\", \"is_public\", \"is_other\"]].values,\n",
    "            np.array(edge_weights).reshape(-1, 1)\n",
    "        ]), dtype=torch.float32)\n",
    "        \n",
    "        return edge_index, edge_attr\n",
    "\n",
    "class SmartRecommendationEngine:\n",
    "    def __init__(self, model, visit_area_df, device):\n",
    "        self.model = model\n",
    "        self.visit_area_df = visit_area_df\n",
    "        self.device = device\n",
    "        self.user_feedback_history = []\n",
    "        self.preference_weights = None\n",
    "        self.processor = EnhancedDataProcessor()\n",
    "        \n",
    "    # SmartRecommendationEngine í´ë˜ìŠ¤ ë‚´ get_recommendations ë©”ì†Œë“œ\n",
    "    def get_recommendations(self, data, travel_context, top_k=10, diversity_weight=0.3, \n",
    "                            excluded_ids=None, filter_useless=True, consider_distance=True):\n",
    "\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings, preference_scores = self.model(data, travel_context)\n",
    "        \n",
    "        scores = preference_scores.squeeze()\n",
    "\n",
    "        if filter_useless:\n",
    "            for idx in range(len(scores)):\n",
    "                name = self.visit_area_df.iloc[idx]['VISIT_AREA_NM']\n",
    "                if self.processor.should_exclude_location(name):\n",
    "                    scores[idx] *= 0.1  # ë„ˆë¬´ ë‚®ì§€ ì•Šë„ë¡ ìˆ˜ì • ê°€ëŠ¥ (ì˜ˆ: 0.3)\n",
    "\n",
    "        if excluded_ids:\n",
    "            for exclude_id in excluded_ids:\n",
    "                matching_indices = self.visit_area_df[\n",
    "                    self.visit_area_df['NEW_VISIT_AREA_ID'] == exclude_id\n",
    "                ].index.tolist()\n",
    "                for idx in matching_indices:\n",
    "                    scores[idx] = -1.0\n",
    "        \n",
    "        # ì¶”ì²œ ì ìˆ˜ ìµœì†Œ ì„ê³„ê°’ ì„¤ì •\n",
    "        min_allowed_score = 0.01\n",
    "        scores[scores < min_allowed_score] = min_allowed_score\n",
    "\n",
    "        recommendations = self._distance_aware_recommendation(\n",
    "            scores, embeddings, top_k, diversity_weight\n",
    "        )\n",
    "        \n",
    "        return recommendations, embeddings, preference_scores\n",
    "\n",
    "    \n",
    "    def _distance_aware_recommendation(self, scores, embeddings, top_k, diversity_weight):\n",
    "        \"\"\"ê±°ë¦¬ë¥¼ ê³ ë ¤í•œ ìˆœì°¨ì  ì¶”ì²œ\"\"\"\n",
    "        recommendations = []\n",
    "        remaining_indices = [i for i in range(len(scores)) if scores[i] > 0]\n",
    "        \n",
    "        if not remaining_indices:\n",
    "            return recommendations\n",
    "        \n",
    "        # ì ìˆ˜ê°€ ë†’ì€ ìƒìœ„ í›„ë³´ ì¤‘ì—ì„œ ì‹œì‘ì  ì„ íƒ\n",
    "        valid_scores = [(i, scores[i].item()) for i in remaining_indices]\n",
    "        valid_scores = sorted(valid_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # ìƒìœ„ 10ê°œ ì¤‘ì—ì„œ ëœë¤í•˜ê²Œ ì‹œì‘\n",
    "        top_candidates = valid_scores[:10]\n",
    "        if top_candidates:\n",
    "            start_idx = random.choice(top_candidates)[0]\n",
    "            recommendations.append(start_idx)\n",
    "            remaining_indices.remove(start_idx)\n",
    "        \n",
    "        # ë‚˜ë¨¸ì§€ ì¶”ì²œ: ê±°ë¦¬ì™€ ì ìˆ˜ë¥¼ ë™ì‹œì— ê³ ë ¤\n",
    "        while len(recommendations) < top_k and remaining_indices:\n",
    "            last_idx = recommendations[-1]\n",
    "            last_coords = self.visit_area_df.iloc[last_idx][['X_COORD', 'Y_COORD']].values\n",
    "            \n",
    "            best_score = -float('inf')\n",
    "            best_idx = None\n",
    "            \n",
    "            for idx in remaining_indices:\n",
    "                if scores[idx] <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # ê±°ë¦¬ ê³„ì‚°\n",
    "                curr_coords = self.visit_area_df.iloc[idx][['X_COORD', 'Y_COORD']].values\n",
    "                distance = np.sqrt(np.sum((last_coords - curr_coords) ** 2))\n",
    "                \n",
    "                # ê±°ë¦¬ í˜ë„í‹° (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)\n",
    "                distance_score = 1 / (1 + distance * 0.1)  # ê±°ë¦¬ê°€ ë©€ìˆ˜ë¡ ì ìˆ˜ ê°ì†Œ\n",
    "                \n",
    "                # ë‹¤ì–‘ì„± ê³„ì‚°\n",
    "                similarities = []\n",
    "                for rec_idx in recommendations:\n",
    "                    sim = F.cosine_similarity(\n",
    "                        embeddings[idx:idx+1], \n",
    "                        embeddings[rec_idx:rec_idx+1]\n",
    "                    ).item()\n",
    "                    similarities.append(sim)\n",
    "                \n",
    "                diversity = 1 - max(similarities) if similarities else 1\n",
    "                \n",
    "                # ìµœì¢… ì ìˆ˜: ì„ í˜¸ë„ + ê±°ë¦¬ + ë‹¤ì–‘ì„±\n",
    "                relevance = scores[idx].item()\n",
    "                final_score = (\n",
    "                    0.4 * relevance +  # ì„ í˜¸ë„\n",
    "                    0.4 * distance_score +  # ê±°ë¦¬\n",
    "                    0.2 * diversity  # ë‹¤ì–‘ì„±\n",
    "                )\n",
    "                \n",
    "                if final_score > best_score:\n",
    "                    best_score = final_score\n",
    "                    best_idx = idx\n",
    "            \n",
    "            if best_idx is not None:\n",
    "                recommendations.append(best_idx)\n",
    "                remaining_indices.remove(best_idx)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _mmr_recommendation(self, scores, embeddings, top_k, diversity_weight):\n",
    "        \"\"\"ê¸°ì¡´ MMR ê¸°ë°˜ ì¶”ì²œ (fallback)\"\"\"\n",
    "        recommendations = []\n",
    "        remaining_indices = list(range(len(scores)))\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ ì¶”ì²œ\n",
    "        if remaining_indices:\n",
    "            valid_scores = [(i, scores[i].item()) for i in remaining_indices if scores[i] > 0]\n",
    "            if valid_scores:\n",
    "                valid_scores = sorted(valid_scores, key=lambda x: x[1], reverse=True)\n",
    "                top_candidates = valid_scores[:5]\n",
    "                if top_candidates:\n",
    "                    best_idx = random.choice(top_candidates)[0]\n",
    "                    recommendations.append(best_idx)\n",
    "                    remaining_indices.remove(best_idx)\n",
    "        \n",
    "        # ë‚˜ë¨¸ì§€ ì¶”ì²œ\n",
    "        for _ in range(min(top_k - 1, len(remaining_indices))):\n",
    "            if not remaining_indices:\n",
    "                break\n",
    "                \n",
    "            best_score = -float('inf')\n",
    "            best_idx = None\n",
    "            \n",
    "            for idx in remaining_indices:\n",
    "                relevance = scores[idx].item()\n",
    "                \n",
    "                if relevance < 0:\n",
    "                    continue\n",
    "                \n",
    "                # ë‹¤ì–‘ì„± ê³„ì‚°\n",
    "                similarities = []\n",
    "                for rec_idx in recommendations:\n",
    "                    sim = F.cosine_similarity(\n",
    "                        embeddings[idx:idx+1], \n",
    "                        embeddings[rec_idx:rec_idx+1]\n",
    "                    ).item()\n",
    "                    similarities.append(sim)\n",
    "                \n",
    "                diversity = 1 - max(similarities) if similarities else 1\n",
    "                final_score = (1 - diversity_weight) * relevance + diversity_weight * diversity\n",
    "                \n",
    "                if final_score > best_score:\n",
    "                    best_score = final_score\n",
    "                    best_idx = idx\n",
    "            \n",
    "            if best_idx is not None:\n",
    "                recommendations.append(best_idx)\n",
    "                remaining_indices.remove(best_idx)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _apply_preference_weights(self, scores, embeddings):\n",
    "        \"\"\"í”¼ë“œë°± ê¸°ë°˜ ì ìˆ˜ ì¡°ì •\"\"\"\n",
    "        if not self.preference_weights:\n",
    "            return scores\n",
    "            \n",
    "        adjusted_scores = scores.clone()\n",
    "        \n",
    "        for i, row in self.visit_area_df.iterrows():\n",
    "            if i >= len(adjusted_scores):\n",
    "                break\n",
    "                \n",
    "            area_type = row.get('VISIT_AREA_TYPE_CD', 0)\n",
    "            \n",
    "            # ì„ í˜¸ íƒ€ì…ì´ë©´ ì ìˆ˜ ì¦ê°€\n",
    "            if area_type in self.preference_weights.get('preferred_types', []):\n",
    "                adjusted_scores[i] *= 1.2\n",
    "            \n",
    "            # ë¹„ì„ í˜¸ íƒ€ì…ì´ë©´ ì ìˆ˜ ê°ì†Œ\n",
    "            if area_type in self.preference_weights.get('avoided_types', []):\n",
    "                adjusted_scores[i] *= 0.8\n",
    "        \n",
    "        return adjusted_scores\n",
    "    \n",
    "    def update_with_feedback(self, liked_items, disliked_items, embeddings):\n",
    "        \"\"\"ì‚¬ìš©ì í”¼ë“œë°± ì—…ë°ì´íŠ¸\"\"\"\n",
    "        feedback = {\n",
    "            'liked': liked_items,\n",
    "            'disliked': disliked_items,\n",
    "            'embeddings': embeddings.cpu().numpy()\n",
    "        }\n",
    "        self.user_feedback_history.append(feedback)\n",
    "        \n",
    "        self.preference_weights = self._calculate_preference_weights()\n",
    "        \n",
    "        print(f\"âœ… í”¼ë“œë°± ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì¢‹ì•„ìš” {len(liked_items)}ê°œ, ì‹«ì–´ìš” {len(disliked_items)}ê°œ\")\n",
    "        \n",
    "        return self.preference_weights\n",
    "    \n",
    "    def _calculate_preference_weights(self):\n",
    "        \"\"\"í”¼ë“œë°± íˆìŠ¤í† ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„ í˜¸ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°\"\"\"\n",
    "        if not self.user_feedback_history:\n",
    "            return None\n",
    "            \n",
    "        liked_features = []\n",
    "        disliked_features = []\n",
    "        \n",
    "        for feedback in self.user_feedback_history:\n",
    "            for item_idx in feedback['liked']:\n",
    "                if item_idx < len(self.visit_area_df):\n",
    "                    liked_features.append(self.visit_area_df.iloc[item_idx])\n",
    "            \n",
    "            for item_idx in feedback['disliked']:\n",
    "                if item_idx < len(self.visit_area_df):\n",
    "                    disliked_features.append(self.visit_area_df.iloc[item_idx])\n",
    "        \n",
    "        preferred_types = [item.get('VISIT_AREA_TYPE_CD', 0) for item in liked_features]\n",
    "        avoided_types = [item.get('VISIT_AREA_TYPE_CD', 0) for item in disliked_features]\n",
    "        \n",
    "        return {\n",
    "            'preferred_types': list(set(preferred_types)),\n",
    "            'avoided_types': list(set(avoided_types)),\n",
    "            'preferred_regions': [(item.get('X_COORD', 0), item.get('Y_COORD', 0)) for item in liked_features]\n",
    "        }\n",
    "\n",
    "class OptimizedRouteGenerator:\n",
    "    def __init__(self, distance_threshold_km=50):  # ì„ê³„ê°’ì„ 50kmë¡œ ì¤„ì„\n",
    "        self.distance_threshold_km = distance_threshold_km\n",
    "        \n",
    "    \n",
    "    def calculate_distance(self, coord1, coord2):\n",
    "        from math import radians, cos, sin, sqrt, atan2\n",
    "        try:\n",
    "            R = 6371  # ì§€êµ¬ ë°˜ê²½(km)\n",
    "\n",
    "            lat1, lon1 = radians(coord1[1]), radians(coord1[0])\n",
    "            lat2, lon2 = radians(coord2[1]), radians(coord2[0])\n",
    "\n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "\n",
    "            a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "            c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "            distance = R * c\n",
    "            return distance\n",
    "        except:\n",
    "            # ì˜ˆì™¸ ë°œìƒ ì‹œ ë§¤ìš° í° ê°’ì„ ë°˜í™˜í•´ ì´ìƒì¹˜ ì²˜ë¦¬\n",
    "            return float('inf')\n",
    "        \n",
    "        \n",
    "    def _two_opt_improvement(self, route, coords):\n",
    "        \"\"\"2-opt ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²½ë¡œ ê°œì„ \"\"\"\n",
    "        n = len(route)\n",
    "        if n <= 3:\n",
    "            return route\n",
    "            \n",
    "        improved = True\n",
    "        best_route = route[:]\n",
    "        \n",
    "        while improved:\n",
    "            improved = False\n",
    "            for i in range(1, n - 2):\n",
    "                for j in range(i + 1, n):\n",
    "                    if j - i == 1:\n",
    "                        continue\n",
    "                    \n",
    "                    new_route = best_route[:]\n",
    "                    new_route[i:j] = reversed(new_route[i:j])\n",
    "                    \n",
    "                    if self._route_distance(new_route, coords) < self._route_distance(best_route, coords):\n",
    "                        best_route = new_route\n",
    "                        improved = True\n",
    "        \n",
    "        return best_route\n",
    "    \n",
    "    def _route_distance(self, route, coords):\n",
    "        \"\"\"ê²½ë¡œì˜ ì´ ê±°ë¦¬ ê³„ì‚°\"\"\"\n",
    "        total_distance = 0\n",
    "        for i in range(len(route) - 1):\n",
    "            total_distance += self.calculate_distance(\n",
    "                coords[route[i]], \n",
    "                coords[route[i + 1]]\n",
    "            )\n",
    "        return total_distance\n",
    "    \n",
    "    def _solve_tsp_simple(self, locations):\n",
    "        \"\"\"ê°„ë‹¨í•œ TSP í•´ë²•\"\"\"\n",
    "        if len(locations) <= 2:\n",
    "            return locations\n",
    "            \n",
    "        coords = np.array([loc['coords'] for loc in locations])\n",
    "        n = len(coords)\n",
    "        \n",
    "        # Nearest Neighbor\n",
    "        unvisited = set(range(1, n))\n",
    "        current = 0\n",
    "        route = [0]\n",
    "        \n",
    "        while unvisited:\n",
    "            nearest = min(unvisited, \n",
    "                         key=lambda x: self.calculate_distance(coords[current], coords[x]))\n",
    "            route.append(nearest)\n",
    "            unvisited.remove(nearest)\n",
    "            current = nearest\n",
    "        \n",
    "        # 2-opt improvement\n",
    "        route = self._two_opt_improvement(route, coords)\n",
    "        \n",
    "        return [locations[i] for i in route]\n",
    "        \n",
    "    def remove_outliers(self, coords, locations):\n",
    "        \"\"\"ê±°ë¦¬ ì´ìƒì¹˜ ì œê±°\"\"\"\n",
    "        if len(coords) <= 2:\n",
    "            return coords, locations\n",
    "            \n",
    "        # ì¤‘ì‹¬ì  ê³„ì‚°\n",
    "        center = np.mean(coords, axis=0)\n",
    "        \n",
    "        # ê° ì ê³¼ ì¤‘ì‹¬ì ì˜ ê±°ë¦¬ ê³„ì‚°\n",
    "        distances = [self.calculate_distance(coord, center) for coord in coords]\n",
    "        \n",
    "        # ê±°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ (ë„ˆë¬´ ë¨¼ ê³³ ì œì™¸)\n",
    "        filtered_indices = []\n",
    "        for i, dist in enumerate(distances):\n",
    "            if dist <= self.distance_threshold_km:\n",
    "                filtered_indices.append(i)\n",
    "        \n",
    "        # ìµœì†Œ ì¥ì†Œ ìˆ˜ ìœ ì§€\n",
    "        if len(filtered_indices) < 6:\n",
    "            # ê±°ë¦¬ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ê¹Œìš´ ê³³ë¶€í„° ì„ íƒ\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            filtered_indices = sorted_indices[:min(10, len(distances))].tolist()\n",
    "        \n",
    "        filtered_coords = [coords[i] for i in filtered_indices]\n",
    "        filtered_locations = [locations[i] for i in filtered_indices]\n",
    "        \n",
    "        return np.array(filtered_coords), filtered_locations\n",
    "        \n",
    "    def generate_daily_routes(self, recommendations, visit_area_df, travel_duration, \n",
    "                            optimization_method='region_based'):\n",
    "        \"\"\"ì§€ì—­ ê¸°ë°˜ ì¼ë³„ ìµœì  ê²½ë¡œ ìƒì„±\"\"\"\n",
    "        if travel_duration <= 0:\n",
    "            travel_duration = 1\n",
    "            \n",
    "        coords = []\n",
    "        locations = []\n",
    "        \n",
    "        for idx in recommendations:\n",
    "            if idx < len(visit_area_df):\n",
    "                row = visit_area_df.iloc[idx]\n",
    "                coords.append([row['X_COORD'], row['Y_COORD']])\n",
    "                locations.append({\n",
    "                    'id': row['NEW_VISIT_AREA_ID'],\n",
    "                    'name': row['VISIT_AREA_NM'],\n",
    "                    'coords': [row['X_COORD'], row['Y_COORD']],\n",
    "                    'idx': idx,\n",
    "                    'type': row.get('VISIT_AREA_TYPE_CD', 0)\n",
    "                })\n",
    "        \n",
    "        if len(coords) == 0:\n",
    "            return {}\n",
    "            \n",
    "        coords = np.array(coords)\n",
    "        coords = np.nan_to_num(coords, nan=0.0)\n",
    "        \n",
    "        # ì´ìƒì¹˜ ì œê±°\n",
    "        coords, locations = self.remove_outliers(coords, locations)\n",
    "        \n",
    "        # ì¼ìˆ˜ì— ë§ê²Œ ì¥ì†Œ ìˆ˜ ì¡°ì •\n",
    "        places_per_day = max(3, min(5, len(locations) // travel_duration))\n",
    "        total_places = min(places_per_day * travel_duration, len(locations))\n",
    "        \n",
    "        # ì§€ì—­ë³„ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì¼ì • ìƒì„±\n",
    "        if travel_duration == 1:\n",
    "            # 1ì¼ ì—¬í–‰: TSPë¡œ ìµœì  ê²½ë¡œë§Œ ìƒì„±\n",
    "            optimized_order = self._solve_tsp_with_start(locations)\n",
    "            return {0: optimized_order[:places_per_day]}\n",
    "        else:\n",
    "            # ë‹¤ì¼ ì—¬í–‰: ì§€ì—­ë³„ë¡œ ë¬¶ê¸°\n",
    "            return self._create_regional_routes(coords, locations, travel_duration, places_per_day)\n",
    "    \n",
    "    def _create_regional_routes(self, coords, locations, travel_duration, places_per_day):\n",
    "        \"\"\"ì§€ì—­ë³„ë¡œ ë¬¶ì–´ì„œ ì¼ì • ìƒì„±\"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        \n",
    "        # ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •\n",
    "        n_clusters = min(travel_duration, len(locations) // 2)\n",
    "        \n",
    "        if n_clusters < 2:\n",
    "            # ì¥ì†Œê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê· ë“± ë¶„ë°°\n",
    "            return self._equal_distribution(locations, travel_duration, places_per_day)\n",
    "        \n",
    "        # K-means í´ëŸ¬ìŠ¤í„°ë§\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(coords)\n",
    "        \n",
    "        # í´ëŸ¬ìŠ¤í„°ë³„ ê·¸ë£¹í™”\n",
    "        clusters = {}\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append((i, locations[i]))\n",
    "        \n",
    "        # í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ ê³„ì‚° (ìˆœì„œ ê²°ì •ìš©)\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        cluster_order = self._order_clusters(cluster_centers)\n",
    "        \n",
    "        # ì¼ìë³„ ë°°ì •\n",
    "        daily_routes = {}\n",
    "        locations_per_day = len(locations) // travel_duration\n",
    "        remainder = len(locations) % travel_duration\n",
    "        \n",
    "        current_day = 0\n",
    "        current_day_locations = []\n",
    "        \n",
    "        for cluster_idx in cluster_order:\n",
    "            if cluster_idx in clusters:\n",
    "                cluster_locations = [loc for _, loc in clusters[cluster_idx]]\n",
    "                \n",
    "                # TSPë¡œ í´ëŸ¬ìŠ¤í„° ë‚´ ìµœì  ê²½ë¡œ\n",
    "                if len(cluster_locations) > 1:\n",
    "                    cluster_locations = self._solve_tsp_simple(cluster_locations)\n",
    "                \n",
    "                for loc in cluster_locations:\n",
    "                    current_day_locations.append(loc)\n",
    "                    \n",
    "                    # ì¼ìë³„ í• ë‹¹ëŸ‰ ì²´í¬\n",
    "                    day_quota = locations_per_day + (1 if current_day < remainder else 0)\n",
    "                    if len(current_day_locations) >= day_quota:\n",
    "                        daily_routes[current_day] = current_day_locations\n",
    "                        current_day += 1\n",
    "                        current_day_locations = []\n",
    "                        \n",
    "                        if current_day >= travel_duration:\n",
    "                            break\n",
    "                \n",
    "                if current_day >= travel_duration:\n",
    "                    break\n",
    "        \n",
    "        # ë‚¨ì€ ì¥ì†Œ ì²˜ë¦¬\n",
    "        if current_day_locations and current_day < travel_duration:\n",
    "            daily_routes[current_day] = current_day_locations\n",
    "        \n",
    "        # ê° ì¼ìë³„ ì¥ì†Œ ìˆ˜ ì¡°ì •\n",
    "        return self._adjust_daily_balance(daily_routes, places_per_day)\n",
    "    \n",
    "    def _order_clusters(self, cluster_centers):\n",
    "        \"\"\"í´ëŸ¬ìŠ¤í„°ë¥¼ ê°€ê¹Œìš´ ìˆœì„œë¡œ ì •ë ¬\"\"\"\n",
    "        n_clusters = len(cluster_centers)\n",
    "        if n_clusters <= 1:\n",
    "            return list(range(n_clusters))\n",
    "        \n",
    "        # ì²« í´ëŸ¬ìŠ¤í„°ëŠ” ê°€ì¥ ë‚¨ìª½ (ë˜ëŠ” ì„œìª½)\n",
    "        start_idx = np.argmin(cluster_centers[:, 1])  # Y ì¢Œí‘œ ê¸°ì¤€\n",
    "        \n",
    "        visited = [start_idx]\n",
    "        current = start_idx\n",
    "        \n",
    "        while len(visited) < n_clusters:\n",
    "            min_dist = float('inf')\n",
    "            next_idx = None\n",
    "            \n",
    "            for i in range(n_clusters):\n",
    "                if i not in visited:\n",
    "                    dist = self.calculate_distance(\n",
    "                        cluster_centers[current], \n",
    "                        cluster_centers[i]\n",
    "                    )\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        next_idx = i\n",
    "            \n",
    "            if next_idx is not None:\n",
    "                visited.append(next_idx)\n",
    "                current = next_idx\n",
    "        \n",
    "        return visited\n",
    "    \n",
    "    def _equal_distribution(self, locations, travel_duration, places_per_day):\n",
    "        \"\"\"ê· ë“± ë¶„ë°°\"\"\"\n",
    "        daily_routes = {}\n",
    "        locations_per_day = len(locations) // travel_duration\n",
    "        remainder = len(locations) % travel_duration\n",
    "        \n",
    "        start_idx = 0\n",
    "        for day in range(travel_duration):\n",
    "            count = locations_per_day + (1 if day < remainder else 0)\n",
    "            count = min(count, places_per_day)  # ì¼ë³„ ìµœëŒ€ ì¥ì†Œ ìˆ˜ ì œí•œ\n",
    "            daily_routes[day] = locations[start_idx:start_idx + count]\n",
    "            start_idx += count\n",
    "        \n",
    "        return daily_routes\n",
    "    \n",
    "    def _adjust_daily_balance(self, daily_routes, target_size):\n",
    "        \"\"\"ì¼ë³„ ì¥ì†Œ ìˆ˜ ê· í˜• ì¡°ì •\"\"\"\n",
    "        adjusted = {}\n",
    "        \n",
    "        for day, locations in daily_routes.items():\n",
    "            if len(locations) > target_size:\n",
    "                # ë„ˆë¬´ ë§ìœ¼ë©´ ì˜ë¼ë‚´ê¸°\n",
    "                adjusted[day] = locations[:target_size]\n",
    "            elif len(locations) < 2:\n",
    "                # ë„ˆë¬´ ì ìœ¼ë©´ ë‹¤ë¥¸ ë‚ ì—ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "                continue\n",
    "            else:\n",
    "                adjusted[day] = locations\n",
    "        \n",
    "        return adjusted\n",
    "    \n",
    "    def _solve_tsp_with_start(self, locations):\n",
    "        \"\"\"ì‹œì‘ì ì„ ê³ ë ¤í•œ TSP\"\"\"\n",
    "        if len(locations) <= 2:\n",
    "            return locations\n",
    "        \n",
    "        coords = np.array([loc['coords'] for loc in locations])\n",
    "        n = len(coords)\n",
    "        \n",
    "        # ê°€ì¥ ì ‘ê·¼í•˜ê¸° ì‰¬ìš´ ê³³ì„ ì‹œì‘ì ìœ¼ë¡œ (ê°€ì¥ ì„œìª½ ë˜ëŠ” ë‚¨ìª½)\n",
    "        start = np.argmin(coords[:, 1])  # Y ì¢Œí‘œ ê¸°ì¤€\n",
    "        \n",
    "        # Nearest Neighbor from start\n",
    "        unvisited = set(range(n))\n",
    "        unvisited.remove(start)\n",
    "        current = start\n",
    "        route = [start]\n",
    "        \n",
    "        while unvisited:\n",
    "            nearest = min(unvisited, \n",
    "                         key=lambda x: self.calculate_distance(coords[current], coords[x]))\n",
    "            route.append(nearest)\n",
    "            unvisited.remove(nearest)\n",
    "            current = nearest\n",
    "        \n",
    "        # 2-opt improvement\n",
    "        route = self._two_opt_improvement(route, coords)\n",
    "        \n",
    "        return [locations[i] for i in route]\n",
    "        \n",
    "\n",
    "def process_travel_input(travel_info: dict):\n",
    "    \"\"\"ì—¬í–‰ ì •ë³´ ì „ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
    "    travel_feature_cols = [\n",
    "        'TOTAL_COST_BINNED_ENCODED', 'WITH_PET', 'MONTH', 'DURATION',\n",
    "        'MVMN_ê¸°íƒ€', 'MVMN_ëŒ€ì¤‘êµí†µ', 'MVMN_ìê°€ìš©',\n",
    "        'TRAVEL_PURPOSE_1', 'TRAVEL_PURPOSE_2', 'TRAVEL_PURPOSE_3',\n",
    "        'TRAVEL_PURPOSE_4', 'TRAVEL_PURPOSE_5', 'TRAVEL_PURPOSE_6',\n",
    "        'TRAVEL_PURPOSE_7', 'TRAVEL_PURPOSE_8', 'TRAVEL_PURPOSE_9',\n",
    "        'WHOWITH_2ì¸ì—¬í–‰', 'WHOWITH_ê°€ì¡±ì—¬í–‰', 'WHOWITH_ê¸°íƒ€',\n",
    "        'WHOWITH_ë‹¨ë…ì—¬í–‰', 'WHOWITH_ì¹œêµ¬/ì§€ì¸ ì—¬í–‰', 'LOCATION'\n",
    "    ]\n",
    "    \n",
    "    # ë°˜ë ¤ë™ë¬¼ ë™ë°˜\n",
    "    travel_info['mission_ENC'] = travel_info['mission_ENC'].strip().split(',')\n",
    "    travel_info['WITH_PET'] = 1 if '0' in travel_info['mission_ENC'] else 0\n",
    "        \n",
    "    # ì—¬í–‰ ëª©ì \n",
    "    for i in range(1, 10):\n",
    "        travel_info[f'TRAVEL_PURPOSE_{i}'] = 1 if str(i) in travel_info['mission_ENC'] else 0\n",
    "        \n",
    "    # ë‚ ì§œ ì²˜ë¦¬\n",
    "    dates = travel_info['date_range'].split(' - ')\n",
    "    start_date = datetime.strptime(dates[0].strip(), \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(dates[1].strip(), \"%Y-%m-%d\")\n",
    "    \n",
    "    travel_info['MONTH'] = end_date.month\n",
    "    travel_info['DURATION'] = (end_date - start_date).days + 1  # +1 ì¶”ê°€\n",
    "    \n",
    "    # êµí†µìˆ˜ë‹¨\n",
    "    for m in ['ìê°€ìš©', 'ëŒ€ì¤‘êµí†µ', 'ê¸°íƒ€']:\n",
    "        travel_info[f\"MVMN_{m}\"] = 0\n",
    "    \n",
    "    if travel_info['MVMN_NM_ENC'] == '1':\n",
    "        travel_info['MVMN_ìê°€ìš©'] = 1\n",
    "    elif travel_info['MVMN_NM_ENC'] == '2':\n",
    "        travel_info['MVMN_ëŒ€ì¤‘êµí†µ'] = 1\n",
    "    else:\n",
    "        travel_info['MVMN_ê¸°íƒ€'] = 1\n",
    "    \n",
    "    # ë™í–‰ì\n",
    "    whowith_onehot = [0] * 5\n",
    "    idx = int(travel_info['whowith_ENC']) - 1\n",
    "    if 0 <= idx < 5:\n",
    "        whowith_onehot[idx] = 1\n",
    "    \n",
    "    travel_info.update({\n",
    "        'WHOWITH_ë‹¨ë…ì—¬í–‰': whowith_onehot[0],\n",
    "        'WHOWITH_2ì¸ì—¬í–‰': whowith_onehot[1],\n",
    "        'WHOWITH_ê°€ì¡±ì—¬í–‰': whowith_onehot[2],\n",
    "        'WHOWITH_ì¹œêµ¬/ì§€ì¸ ì—¬í–‰': whowith_onehot[3],\n",
    "        'WHOWITH_ê¸°íƒ€': whowith_onehot[4],\n",
    "    })\n",
    "    \n",
    "    # ë¹„ìš©\n",
    "    travel_info['TOTAL_COST_BINNED_ENCODED'] = int(travel_info['TOTAL_COST'])\n",
    "    \n",
    "    # ìµœì¢… ë²¡í„° ìƒì„±\n",
    "    travel_vector = [int(travel_info.get(k, 0)) for k in travel_feature_cols]\n",
    "    \n",
    "    return np.array([travel_vector]).astype(np.float32)\n",
    "\n",
    "def simulate_user_feedback():\n",
    "    \"\"\"ì‚¬ìš©ì í”¼ë“œë°± ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
    "    feedback_options = [\n",
    "        {\"liked\": [], \"disliked\": [0, 2]},  # ì²« ë²ˆì§¸ì™€ ì„¸ ë²ˆì§¸ ì¥ì†Œ ì‹«ì–´ìš”\n",
    "        {\"liked\": [1], \"disliked\": [4, 7]},  # ë‘ ë²ˆì§¸ ì¥ì†Œ ì¢‹ì•„ìš”, ë‹¤ë¥¸ ê³³ë“¤ ì‹«ì–´ìš”\n",
    "        {\"liked\": [0, 3], \"disliked\": [5]},  # ë³µìˆ˜ ì¢‹ì•„ìš”/ì‹«ì–´ìš”\n",
    "    ]\n",
    "    \n",
    "    return random.choice(feedback_options)\n",
    "\n",
    "def main_feedback_test():\n",
    "    print(\"ğŸš€ ê°œì„ ëœ í”¼ë“œë°± ê¸°ë°˜ ê²½ë¡œ ëŒ€ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ê¸°ì¡´ ë°ì´í„° ë¡œë”©\n",
    "    move_path = \"./merged_csv/fin/ì´ë™ë‚´ì—­_fin.csv\"\n",
    "    travel_path = \"./merged_csv/fin/ì—¬í–‰_fin.csv\"\n",
    "    visit_area_path = \"./merged_csv/fin/ë°©ë¬¸ì§€_fin.csv\"\n",
    "    \n",
    "    move_df = pd.read_csv(move_path)\n",
    "    travel_df = pd.read_csv(travel_path)\n",
    "    visit_area_df = pd.read_csv(visit_area_path)\n",
    "    \n",
    "    processor = EnhancedDataProcessor()\n",
    "    visit_area_tensor = processor.process_visit_area_features(visit_area_df)\n",
    "    edge_index, edge_attr = processor.create_enhanced_edges(move_df, visit_area_df)\n",
    "    \n",
    "    data = HeteroData()\n",
    "    data['visit_area'].x = torch.tensor(visit_area_tensor, dtype=torch.float32)\n",
    "    data['visit_area', 'moved_to', 'visit_area'].edge_index = edge_index\n",
    "    data['visit_area', 'moved_to', 'visit_area'].edge_attr = edge_attr\n",
    "    \n",
    "    # ì—¬í–‰ ì •ë³´ (2ì¼ ì—¬í–‰)\n",
    "    travel_example = {\n",
    "        'mission_ENC': '0,1,2',\n",
    "        'date_range': '2025-09-28 - 2025-09-29',  # 2ì¼ ì—¬í–‰ìœ¼ë¡œ ë³€ê²½\n",
    "        'start_date': '',\n",
    "        'end_date': '',\n",
    "        'TOTAL_COST': '2',\n",
    "        'MVMN_NM_ENC': '2',\n",
    "        'whowith_ENC': '2',\n",
    "        'mission_type': 'normal',\n",
    "        'location': 0\n",
    "    }\n",
    "    \n",
    "    travel_tensor = process_travel_input(travel_example)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "    print(f\"ğŸ“… ì—¬í–‰ ê¸°ê°„: {travel_example['date_range']} ({travel_tensor[0, 3]:.0f}ì¼)\")\n",
    "    \n",
    "    model = ImprovedTravelGNN(\n",
    "        in_channels=visit_area_tensor.shape[1],\n",
    "        hidden_channels=128,\n",
    "        out_channels=64,\n",
    "        travel_context_dim=travel_tensor.shape[1],\n",
    "        num_heads=4,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    data = data.to(device)\n",
    "    travel_context_tensor = torch.tensor(travel_tensor, dtype=torch.float32).to(device)\n",
    "    \n",
    "    recommender = SmartRecommendationEngine(model, visit_area_df, device)\n",
    "    print(\"ì—”ì§„ ì´ˆê¸°í™”\")\n",
    "    # ì´ˆê¸° ì¶”ì²œ (í•„í„°ë§ ì ìš©, ê±°ë¦¬ ê³ ë ¤)\n",
    "    recommendations, embeddings, _ = recommender.get_recommendations(\n",
    "        data, travel_context_tensor, top_k=30, diversity_weight=0.3, \n",
    "        filter_useless=True, consider_distance=True\n",
    "    )\n",
    "    print(\"ì¶”ì²œ ì‹œì‘\")\n",
    "    # ì¤‘ë³µ ë°©ë¬¸ì§€ ì œê±° ë° ìœ íš¨ì„± ê²€ì‚¬\n",
    "    unique_recommendations, seen_ids = [], set()\n",
    "    for idx in recommendations:\n",
    "        if idx < len(visit_area_df):\n",
    "            row = visit_area_df.iloc[idx]\n",
    "            area_id = row['NEW_VISIT_AREA_ID']\n",
    "            name = row['VISIT_AREA_NM']\n",
    "            \n",
    "            # ì¤‘ë³µ ì²´í¬ ë° ì“¸ëª¨ì—†ëŠ” ì¥ì†Œ ì¬í™•ì¸\n",
    "            if (area_id not in seen_ids and \n",
    "                area_id != 0 and \n",
    "                not processor.should_exclude_location(name)):\n",
    "                unique_recommendations.append(idx)\n",
    "                seen_ids.add(area_id)\n",
    "            \n",
    "            if len(unique_recommendations) >= 15:  # ì—¬ìœ ìˆê²Œ ì„ íƒ\n",
    "                break\n",
    "    print(\"ìœ íš¨ ë°©ë¬¸ì§€ ê²€ì‚¬\")\n",
    "    # ìµœì í™” ê²½ë¡œ ìƒì„±\n",
    "    route_generator = OptimizedRouteGenerator(distance_threshold_km=50)  # ê±°ë¦¬ ì„ê³„ê°’ ì¤„ì„\n",
    "    travel_duration = int(travel_tensor[0, 3])\n",
    "    optimized_routes = route_generator.generate_daily_routes(\n",
    "        unique_recommendations, visit_area_df, travel_duration\n",
    "    )\n",
    "    print(\"ê²½ë¡œ ìƒì„± ì™„\")\n",
    "    print(\"\\nğŸ—“ï¸ ì´ˆê¸° ì—¬í–‰ ì¼ì • (ìµœì í™” ë° í•„í„°ë§ ì ìš©):\")\n",
    "    total_places = 0\n",
    "    for day, route in sorted(optimized_routes.items()):\n",
    "        print(f\"\\nğŸ“… Day {day + 1}:\")\n",
    "        for i, loc in enumerate(route):\n",
    "            print(f\" {i+1}. [{loc['id']:3d}] {loc['name']}\")\n",
    "        total_places += len(route)\n",
    "    print(f\"\\nì´ {total_places}ê°œ ì¥ì†Œ ì¶”ì²œ\")\n",
    "    \n",
    "    # í”¼ë“œë°± ë¼ìš´ë“œ ë°˜ë³µ\n",
    "    for round_num in range(1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ”„ í”¼ë“œë°± ë¼ìš´ë“œ {round_num + 1}\")\n",
    "        \n",
    "        feedback = simulate_user_feedback()\n",
    "        liked_indices = [unique_recommendations[i] for i in feedback[\"liked\"] if i < len(unique_recommendations)]\n",
    "        disliked_indices = [unique_recommendations[i] for i in feedback[\"disliked\"] if i < len(unique_recommendations)]\n",
    "        \n",
    "        if liked_indices:\n",
    "            print(f\"ğŸ‘ ì¢‹ì•„ìš”: {[visit_area_df.iloc[idx]['VISIT_AREA_NM'] for idx in liked_indices]}\")\n",
    "        if disliked_indices:\n",
    "            print(f\"ğŸ‘ ì‹«ì–´ìš”: {[visit_area_df.iloc[idx]['VISIT_AREA_NM'] for idx in disliked_indices]}\")\n",
    "        \n",
    "        recommender.update_with_feedback(liked_indices, disliked_indices, embeddings)\n",
    "        \n",
    "        # ì œì™¸ëœ í•­ëª© ë°˜ì˜\n",
    "        excluded_ids = {visit_area_df.iloc[idx]['NEW_VISIT_AREA_ID'] for idx in disliked_indices}\n",
    "        \n",
    "        recommendations, embeddings, _ = recommender.get_recommendations(\n",
    "            data, travel_context_tensor, top_k=30, diversity_weight=0.3, \n",
    "            excluded_ids=excluded_ids, filter_useless=True, consider_distance=True\n",
    "        )\n",
    "        \n",
    "        unique_recommendations, seen_ids = [], set()\n",
    "        for idx in recommendations:\n",
    "            if idx < len(visit_area_df):\n",
    "                row = visit_area_df.iloc[idx]\n",
    "                area_id = row['NEW_VISIT_AREA_ID']\n",
    "                name = row['VISIT_AREA_NM']\n",
    "                \n",
    "                if (area_id not in seen_ids and \n",
    "                    area_id not in excluded_ids and \n",
    "                    area_id != 0 and\n",
    "                    not processor.should_exclude_location(name)):\n",
    "                    unique_recommendations.append(idx)\n",
    "                    seen_ids.add(area_id)\n",
    "                \n",
    "                if len(unique_recommendations) >= 15:\n",
    "                    break\n",
    "        \n",
    "        optimized_routes = route_generator.generate_daily_routes(\n",
    "            unique_recommendations, visit_area_df, travel_duration\n",
    "        )\n",
    "        \n",
    "        print(\"\\nğŸ¯ í”¼ë“œë°± ë°˜ì˜ í›„ ìµœì í™”ëœ ì—¬í–‰ ì¼ì •:\")\n",
    "        total_places = 0\n",
    "        for day, route in sorted(optimized_routes.items()):\n",
    "            print(f\"\\nğŸ“… Day {day + 1}:\")\n",
    "            for i, loc in enumerate(route):\n",
    "                print(f\" {i+1}. [{loc['id']:3d}] {loc['name']}\")\n",
    "            total_places += len(route)\n",
    "        print(f\"\\nì´ {total_places}ê°œ ì¥ì†Œ ì¶”ì²œ\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… ê°œì„ ëœ ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ê²½ë¡œ ëŒ€ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ê°œì„  ì‚¬í•­ ìš”ì•½\n",
    "    print(\"\\nğŸ“Š ì£¼ìš” ê°œì„  ì‚¬í•­:\")\n",
    "    print(\"1. âœ… ì—­, í„°ë¯¸ë„, ê³µí•­ ë“± ë¶ˆí•„ìš”í•œ ì¥ì†Œ í•„í„°ë§\")\n",
    "    print(\"2. âœ… ê±°ë¦¬ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±°ë¡œ ì§€ì—­ë³„ ê·¸ë£¹í™” ê°œì„ \")\n",
    "    print(\"3. âœ… ì¼ìë³„ ê· í˜•ìˆëŠ” ì¥ì†Œ ë°°ë¶„ (3-5ê°œ/ì¼)\")\n",
    "    print(\"4. âœ… DBSCAN í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ë” ì •í™•í•œ ì§€ì—­ êµ¬ë¶„\")\n",
    "    print(\"5. âœ… 2-opt ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²½ë¡œ ìµœì í™” ê°œì„ \")\n",
    "    \n",
    "    # ë°ì´í„° ì €ì¥ ì¤€ë¹„\n",
    "    save_data = {\n",
    "        'visit_area_df': visit_area_df,\n",
    "        'graph_data': data,\n",
    "        'visit_scaler': processor.visit_scaler,\n",
    "        'travel_scaler': processor.travel_scaler,\n",
    "        'device': str(device)\n",
    "    }\n",
    "    \n",
    "    # íŒŒì¼ ì €ì¥\n",
    "    print(\"\\nğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...\")\n",
    "    \n",
    "    # ëª¨ë¸ ì €ì¥ (.pt)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'in_channels': visit_area_tensor.shape[1],\n",
    "            'hidden_channels': 128,\n",
    "            'out_channels': 64,\n",
    "            'travel_context_dim': 22,\n",
    "            'num_heads': 4,\n",
    "            'dropout': 0.2\n",
    "        }\n",
    "    }, 'improved_travel_recommendation_model.pt')\n",
    "    \n",
    "    # ë°ì´í„° ì €ì¥ (.pkl)\n",
    "    with open('improved_travel_data.pkl', 'wb') as f:\n",
    "        pickle.dump(save_data, f)\n",
    "    \n",
    "    print(\"âœ… ì €ì¥ ì™„ë£Œ!\")\n",
    "    print(\"- improved_travel_recommendation_model.pt: ê°œì„ ëœ ëª¨ë¸ íŒŒë¼ë¯¸í„°\")\n",
    "    print(\"- improved_travel_data.pkl: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë° ìŠ¤ì¼€ì¼ëŸ¬\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_feedback_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸš€ ê°œì„ ëœ í”¼ë“œë°± ê¸°ë°˜ ê²½ë¡œ ëŒ€ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘!\n",
    "============================================================\n",
    "ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: cpu\n",
    "ğŸ“… ì—¬í–‰ ê¸°ê°„: 2025-09-28 - 2025-09-29 (2ì¼)\n",
    "\n",
    "ğŸ—“ï¸ ì´ˆê¸° ì—¬í–‰ ì¼ì • (ìµœì í™” ë° í•„í„°ë§ ì ìš©):\n",
    "\n",
    "ğŸ“… Day 1:\n",
    " 1. [799] í˜„ëŒ€ë°±í™”ì  ì¤‘ë™ì \n",
    " 2. [941] ì•ˆì¤‘ê·¼ê³µì›\n",
    " 3. [1002] ì‹ ì‹  ë¶„ì‹\n",
    " 4. [1578] ì‹ í¬êµ­ì œì‹œì¥\n",
    " 5. [7035] ì´ë””ì•¼ì»¤í”¼ ì¸ì²œ ì°¨ì´ë‚˜íƒ€ìš´ì \n",
    "\n",
    "ğŸ“… Day 2:\n",
    " 1. [2726] íŒŒì£¼ ë‹­ êµ­ìˆ˜ íŒŒì£¼ ë³¸ì \n",
    " 2. [8027] í™”ê°œ ì •ì›\n",
    " 3. [8167] í™”ê°œì‚° ì†ì¹¼êµ­ìˆ˜\n",
    " 4. [8028] ëŒ€ë£¡ì‹œì¥\n",
    " 5. [8026] í–‰ë³µí•œ ì‹œê³¨ë°¥ìƒ\n",
    "\n",
    "ì´ 10ê°œ ì¥ì†Œ ì¶”ì²œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
