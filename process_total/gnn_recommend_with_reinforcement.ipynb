{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Í∞úÏÑ†Îêú ÌîºÎìúÎ∞± Í∏∞Î∞ò Í≤ΩÎ°ú ÎåÄÏ≤¥ ÌÖåÏä§Ìä∏ ÏãúÏûë!\n",
      "============================================================\n",
      "üì± ÏÇ¨Ïö© ÎîîÎ∞îÏù¥Ïä§: cpu\n",
      "üìÖ Ïó¨Ìñâ Í∏∞Í∞Ñ: 2025-09-28 - 2025-09-29 (2Ïùº)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1027\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- improved_travel_data.pkl: Ï†ÑÏ≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Î∞è Ïä§ÏºÄÏùºÎü¨\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1027\u001b[0m     \u001b[43mmain_feedback_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 889\u001b[0m, in \u001b[0;36mmain_feedback_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m    886\u001b[0m recommender \u001b[38;5;241m=\u001b[39m SmartRecommendationEngine(model, visit_area_df, device)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# Ï¥àÍ∏∞ Ï∂îÏ≤ú (ÌïÑÌÑ∞ÎßÅ Ï†ÅÏö©, Í±∞Î¶¨ Í≥†Î†§)\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m recommendations, embeddings, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrecommender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_recommendations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtravel_context_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiversity_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_useless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsider_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# Ï§ëÎ≥µ Î∞©Î¨∏ÏßÄ Ï†úÍ±∞ Î∞è Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨\u001b[39;00m\n\u001b[1;32m    895\u001b[0m unique_recommendations, seen_ids \u001b[38;5;241m=\u001b[39m [], \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 255\u001b[0m, in \u001b[0;36mSmartRecommendationEngine.get_recommendations\u001b[0;34m(self, data, travel_context, top_k, diversity_weight, excluded_ids, filter_useless, consider_distance)\u001b[0m\n\u001b[1;32m    252\u001b[0m min_allowed_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m    253\u001b[0m scores[scores \u001b[38;5;241m<\u001b[39m min_allowed_score] \u001b[38;5;241m=\u001b[39m min_allowed_score\n\u001b[0;32m--> 255\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distance_aware_recommendation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiversity_weight\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m recommendations, embeddings, preference_scores\n",
      "Cell \u001b[0;32mIn[4], line 303\u001b[0m, in \u001b[0;36mSmartRecommendationEngine._distance_aware_recommendation\u001b[0;34m(self, scores, embeddings, top_k, diversity_weight)\u001b[0m\n\u001b[1;32m    301\u001b[0m similarities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rec_idx \u001b[38;5;129;01min\u001b[39;00m recommendations:\n\u001b[0;32m--> 303\u001b[0m     sim \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m:\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrec_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mrec_idx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    307\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mappend(sim)\n\u001b[1;32m    309\u001b[0m diversity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mmax\u001b[39m(similarities) \u001b[38;5;28;01mif\u001b[39;00m similarities \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, GATConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cdist\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ImprovedTravelGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, travel_context_dim, \n",
    "                 num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # GNN layers with edge features\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True, \n",
    "                           edge_dim=5)  # edge features Í≥†Î†§\n",
    "        self.gat2 = GATConv(hidden_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True,\n",
    "                           edge_dim=5)\n",
    "        self.gat3 = GATConv(hidden_channels, out_channels, \n",
    "                           heads=1, dropout=dropout, concat=False,\n",
    "                           edge_dim=5)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Travel context encoder\n",
    "        self.travel_encoder = nn.Sequential(\n",
    "            nn.Linear(travel_context_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # Distance-aware attention module\n",
    "        self.distance_attention = nn.Sequential(\n",
    "            nn.Linear(2, hidden_channels // 2),  # x, y coordinates\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Fusion network\n",
    "        self.fusion_net = nn.Sequential(\n",
    "            nn.Linear(out_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # Preference head\n",
    "        self.preference_head = nn.Sequential(\n",
    "            nn.Linear(out_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, data, travel_context, return_attention=False):\n",
    "        x = data['visit_area'].x\n",
    "        edge_index = data['visit_area', 'moved_to', 'visit_area'].edge_index\n",
    "        edge_attr = data['visit_area', 'moved_to', 'visit_area'].edge_attr\n",
    "        \n",
    "        # Extract coordinates for distance attention\n",
    "        coords = x[:, :2]  # Assuming first two features are X_COORD, Y_COORD\n",
    "        \n",
    "        # GNN layers with edge features\n",
    "        x1 = self.gat1(x, edge_index, edge_attr)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        x2 = self.gat2(x1, edge_index, edge_attr)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2 + x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        graph_embedding = self.gat3(x2, edge_index, edge_attr)\n",
    "        graph_embedding = self.bn3(graph_embedding)\n",
    "        \n",
    "        # Apply distance-based attention\n",
    "        distance_weights = self.distance_attention(coords)\n",
    "        graph_embedding = graph_embedding * distance_weights\n",
    "        \n",
    "        # Travel context encoding\n",
    "        travel_embedding = self.travel_encoder(travel_context)\n",
    "        travel_embedding_expanded = travel_embedding.expand(graph_embedding.size(0), -1)\n",
    "        \n",
    "        # Fusion\n",
    "        fused_features = torch.cat([graph_embedding, travel_embedding_expanded], dim=1)\n",
    "        final_embedding = self.fusion_net(fused_features)\n",
    "        \n",
    "        # Preference scores\n",
    "        preference_scores = self.preference_head(final_embedding)\n",
    "        \n",
    "        return final_embedding, preference_scores\n",
    "\n",
    "class EnhancedDataProcessor:\n",
    "    def __init__(self):\n",
    "        self.visit_scaler = RobustScaler()\n",
    "        self.travel_scaler = StandardScaler()\n",
    "        # Ï†úÏô∏Ìï† ÌÇ§ÏõåÎìú Î™©Î°ù\n",
    "        self.exclude_keywords = [\n",
    "            'Ïó≠', 'ÌÑ∞ÎØ∏ÎÑê', 'Í≥µÌï≠', 'Ìú¥Í≤åÏÜå', 'Ï†ïÎ•òÏû•', 'ÌÜ®Í≤åÏù¥Ìä∏', 'ÍµêÏ∞®Î°ú', 'Ï∂úÍµ¨', 'ÏûÖÍµ¨',\n",
    "            'IC', 'JC', 'ÎÇòÎì§Î™©', 'Î∂ÑÍ∏∞Ï†ê', 'ÏöîÍ∏àÏÜå', 'Ï£ºÏ∞®Ïû•', 'Ï£ºÏú†ÏÜå', 'Ï∂©Ï†ÑÏÜå',\n",
    "            'ÏïÑÌååÌä∏', 'ÏõêÎ£∏', 'Ïò§ÌîºÏä§ÌÖî', 'ÎπåÎùº', 'Ï£ºÌÉù', 'ÎπåÎî©', 'ÏÉÅÍ∞Ä', 'Î™®ÌÖî'\n",
    "        ]\n",
    "        \n",
    "    def should_exclude_location(self, name):\n",
    "        \"\"\"ÏúÑÏπòÎ•º Ï†úÏô∏Ìï¥Ïïº ÌïòÎäîÏßÄ ÌôïÏù∏\"\"\"\n",
    "        if pd.isna(name):\n",
    "            return False\n",
    "        name_str = str(name).lower()\n",
    "        \n",
    "        # ÌäπÏ†ï ÌÇ§ÏõåÎìúÍ∞Ä Ìè¨Ìï®ÎêòÍ≥†, Îã§Î•∏ Ïú†Ïö©Ìïú Îã®Ïñ¥Í∞Ä ÏóÜÎäî Í≤ΩÏö∞ Ï†úÏô∏\n",
    "        for keyword in self.exclude_keywords:\n",
    "            if keyword.lower() in name_str:\n",
    "                # ÏòàÏô∏ Ï≤òÎ¶¨: Í¥ÄÍ¥ëÏßÄÎ°úÏÑúÏùò Ïó≠Ìï†Ïù¥ ÏûàÎäî Í≤ΩÏö∞\n",
    "                tourist_keywords = ['Í¥ÄÍ¥ë', 'ÌÖåÎßà', 'ÌååÌÅ¨', 'ÎûúÎìú', 'ÏõîÎìú', 'Î¶¨Ï°∞Ìä∏', 'Ìò∏ÌÖî', \n",
    "                                   'ÎßõÏßë', 'ÏãùÎãπ', 'Ïπ¥Ìéò', 'Î∞ïÎ¨ºÍ¥Ä', 'Ï†ÑÏãú', 'Í∞§Îü¨Î¶¨', 'Î¨∏Ìôî']\n",
    "                if any(tk in name_str for tk in tourist_keywords):\n",
    "                    continue\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def process_visit_area_features(self, visit_area_df):\n",
    "        visit_area_df = visit_area_df.copy()\n",
    "        \n",
    "        # Ï¢åÌëú Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨\n",
    "        visit_area_df['X_COORD'] = visit_area_df['X_COORD'].fillna(visit_area_df['X_COORD'].mean())\n",
    "        visit_area_df['Y_COORD'] = visit_area_df['Y_COORD'].fillna(visit_area_df['Y_COORD'].mean())\n",
    "        visit_area_df['VISIT_CHC_REASON_CD'] = visit_area_df['VISIT_CHC_REASON_CD'].fillna(0)\n",
    "        \n",
    "        features = visit_area_df[['X_COORD', 'Y_COORD']].copy()\n",
    "        \n",
    "        # One-hot encoding\n",
    "        type_onehot = pd.get_dummies(visit_area_df['VISIT_AREA_TYPE_CD'], prefix='type')\n",
    "        reason_onehot = pd.get_dummies(visit_area_df['VISIT_CHC_REASON_CD'].fillna(0), prefix='reason')\n",
    "        \n",
    "        # Ï†ïÍ∑úÌôîÎêú ÎßåÏ°±ÎèÑ Ï†êÏàò\n",
    "        for col in ['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']:\n",
    "            visit_area_df[col] = visit_area_df[col].fillna(3)\n",
    "            visit_area_df[f'{col}_norm'] = (visit_area_df[col] - 1) / 4.0\n",
    "        \n",
    "        # Ïù∏Í∏∞ÎèÑ Ï†êÏàò\n",
    "        visit_area_df['popularity_score'] = (\n",
    "            visit_area_df['DGSTFN_norm'] * 0.4 + \n",
    "            visit_area_df['REVISIT_INTENTION_norm'] * 0.3 + \n",
    "            visit_area_df['RCMDTN_INTENTION_norm'] * 0.3\n",
    "        )\n",
    "        \n",
    "        # Ï†úÏô∏Ìï† Ïû•ÏÜåÏóê ÎåÄÌïú ÌéòÎÑêÌã∞ Ï∂îÍ∞Ä\n",
    "        exclude_penalty = visit_area_df['VISIT_AREA_NM'].apply(self.should_exclude_location).astype(float) * -0.5\n",
    "        \n",
    "        # Î™®Îì† ÌäπÏÑ± Í≤∞Ìï©\n",
    "        features = pd.concat([\n",
    "            features, type_onehot, reason_onehot,\n",
    "            visit_area_df[['DGSTFN_norm', 'REVISIT_INTENTION_norm', 'RCMDTN_INTENTION_norm', 'popularity_score']],\n",
    "            pd.DataFrame({'exclude_penalty': exclude_penalty})\n",
    "        ], axis=1)\n",
    "        \n",
    "        return self.visit_scaler.fit_transform(features.values.astype(np.float32))\n",
    "    \n",
    "    def create_enhanced_edges(self, move_df, visit_area_df):\n",
    "        edges = []\n",
    "        edge_weights = []\n",
    "        \n",
    "        for travel_id, group in move_df.groupby(\"TRAVEL_ID\"):\n",
    "            group = group.sort_values(\"TRIP_ID\").reset_index(drop=True)\n",
    "            \n",
    "            for i in range(1, len(group)):\n",
    "                from_id = group.loc[i-1, \"NEW_END_VISIT_AREA_ID\"]\n",
    "                to_id = group.loc[i, \"NEW_END_VISIT_AREA_ID\"]\n",
    "                \n",
    "                if pd.notna(from_id) and pd.notna(to_id):\n",
    "                    duration = group.loc[i, \"DURATION_MINUTES\"] if \"DURATION_MINUTES\" in group.columns else 0\n",
    "                    transport = group.loc[i, \"MVMN_CD_1\"] if \"MVMN_CD_1\" in group.columns else 0\n",
    "                    \n",
    "                    edges.append([int(from_id), int(to_id), duration, transport])\n",
    "                    edge_weights.append(1.0)\n",
    "        \n",
    "        edges_df = pd.DataFrame(edges, columns=[\"FROM_ID\", \"TO_ID\", \"DURATION_MINUTES\", \"MVMN_CD_1\"])\n",
    "        \n",
    "        # ÍµêÌÜµÏàòÎã® ÏõêÌï´ Ïù∏ÏΩîÎî©\n",
    "        edges_df[\"MVMN_TYPE\"] = edges_df[\"MVMN_CD_1\"].apply(\n",
    "            lambda code: \"drive\" if code in [1,2,3] else \"public\" if code in [4,5,6,7,8,9,10,11,12,13,50] else \"other\"\n",
    "        )\n",
    "        edges_df[\"is_drive\"] = (edges_df[\"MVMN_TYPE\"] == \"drive\").astype(int)\n",
    "        edges_df[\"is_public\"] = (edges_df[\"MVMN_TYPE\"] == \"public\").astype(int)\n",
    "        edges_df[\"is_other\"] = (edges_df[\"MVMN_TYPE\"] == \"other\").astype(int)\n",
    "        \n",
    "        edge_index = torch.tensor(edges_df[[\"FROM_ID\", \"TO_ID\"]].values.T, dtype=torch.long)\n",
    "        edge_attr = torch.tensor(np.column_stack([\n",
    "            edges_df[[\"DURATION_MINUTES\"]].fillna(0).values,\n",
    "            edges_df[[\"is_drive\", \"is_public\", \"is_other\"]].values,\n",
    "            np.array(edge_weights).reshape(-1, 1)\n",
    "        ]), dtype=torch.float32)\n",
    "        \n",
    "        return edge_index, edge_attr\n",
    "\n",
    "class SmartRecommendationEngine:\n",
    "    def __init__(self, model, visit_area_df, device):\n",
    "        self.model = model\n",
    "        self.visit_area_df = visit_area_df\n",
    "        self.device = device\n",
    "        self.user_feedback_history = []\n",
    "        self.preference_weights = None\n",
    "        self.processor = EnhancedDataProcessor()\n",
    "        \n",
    "    # SmartRecommendationEngine ÌÅ¥ÎûòÏä§ ÎÇ¥ get_recommendations Î©îÏÜåÎìú\n",
    "    def get_recommendations(self, data, travel_context, top_k=10, diversity_weight=0.3, \n",
    "                            excluded_ids=None, filter_useless=True, consider_distance=True):\n",
    "\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings, preference_scores = self.model(data, travel_context)\n",
    "        \n",
    "        scores = preference_scores.squeeze()\n",
    "\n",
    "        if filter_useless:\n",
    "            for idx in range(len(scores)):\n",
    "                name = self.visit_area_df.iloc[idx]['VISIT_AREA_NM']\n",
    "                if self.processor.should_exclude_location(name):\n",
    "                    scores[idx] *= 0.1  # ÎÑàÎ¨¥ ÎÇÆÏßÄ ÏïäÎèÑÎ°ù ÏàòÏ†ï Í∞ÄÎä• (Ïòà: 0.3)\n",
    "\n",
    "        if excluded_ids:\n",
    "            for exclude_id in excluded_ids:\n",
    "                matching_indices = self.visit_area_df[\n",
    "                    self.visit_area_df['NEW_VISIT_AREA_ID'] == exclude_id\n",
    "                ].index.tolist()\n",
    "                for idx in matching_indices:\n",
    "                    scores[idx] = -1.0\n",
    "        \n",
    "        # Ï∂îÏ≤ú Ï†êÏàò ÏµúÏÜå ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï\n",
    "        min_allowed_score = 0.01\n",
    "        scores[scores < min_allowed_score] = min_allowed_score\n",
    "\n",
    "        recommendations = self._distance_aware_recommendation(\n",
    "            scores, embeddings, top_k, diversity_weight\n",
    "        )\n",
    "        \n",
    "        return recommendations, embeddings, preference_scores\n",
    "\n",
    "    \n",
    "    def _distance_aware_recommendation(self, scores, embeddings, top_k, diversity_weight):\n",
    "        \"\"\"Í±∞Î¶¨Î•º Í≥†Î†§Ìïú ÏàúÏ∞®Ï†Å Ï∂îÏ≤ú\"\"\"\n",
    "        recommendations = []\n",
    "        remaining_indices = [i for i in range(len(scores)) if scores[i] > 0]\n",
    "        \n",
    "        if not remaining_indices:\n",
    "            return recommendations\n",
    "        \n",
    "        # Ï†êÏàòÍ∞Ä ÎÜíÏùÄ ÏÉÅÏúÑ ÌõÑÎ≥¥ Ï§ëÏóêÏÑú ÏãúÏûëÏ†ê ÏÑ†ÌÉù\n",
    "        valid_scores = [(i, scores[i].item()) for i in remaining_indices]\n",
    "        valid_scores = sorted(valid_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # ÏÉÅÏúÑ 10Í∞ú Ï§ëÏóêÏÑú ÎûúÎç§ÌïòÍ≤å ÏãúÏûë\n",
    "        top_candidates = valid_scores[:10]\n",
    "        if top_candidates:\n",
    "            start_idx = random.choice(top_candidates)[0]\n",
    "            recommendations.append(start_idx)\n",
    "            remaining_indices.remove(start_idx)\n",
    "        \n",
    "        # ÎÇòÎ®∏ÏßÄ Ï∂îÏ≤ú: Í±∞Î¶¨ÏôÄ Ï†êÏàòÎ•º ÎèôÏãúÏóê Í≥†Î†§\n",
    "        while len(recommendations) < top_k and remaining_indices:\n",
    "            last_idx = recommendations[-1]\n",
    "            last_coords = self.visit_area_df.iloc[last_idx][['X_COORD', 'Y_COORD']].values\n",
    "            \n",
    "            best_score = -float('inf')\n",
    "            best_idx = None\n",
    "            \n",
    "            for idx in remaining_indices:\n",
    "                if scores[idx] <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # Í±∞Î¶¨ Í≥ÑÏÇ∞\n",
    "                curr_coords = self.visit_area_df.iloc[idx][['X_COORD', 'Y_COORD']].values\n",
    "                distance = np.sqrt(np.sum((last_coords - curr_coords) ** 2))\n",
    "                \n",
    "                # Í±∞Î¶¨ ÌéòÎÑêÌã∞ (Í∞ÄÍπåÏö∏ÏàòÎ°ù ÎÜíÏùÄ Ï†êÏàò)\n",
    "                distance_score = 1 / (1 + distance * 0.1)  # Í±∞Î¶¨Í∞Ä Î©ÄÏàòÎ°ù Ï†êÏàò Í∞êÏÜå\n",
    "                \n",
    "                # Îã§ÏñëÏÑ± Í≥ÑÏÇ∞\n",
    "                similarities = []\n",
    "                for rec_idx in recommendations:\n",
    "                    sim = F.cosine_similarity(\n",
    "                        embeddings[idx:idx+1], \n",
    "                        embeddings[rec_idx:rec_idx+1]\n",
    "                    ).item()\n",
    "                    similarities.append(sim)\n",
    "                \n",
    "                diversity = 1 - max(similarities) if similarities else 1\n",
    "                \n",
    "                # ÏµúÏ¢Ö Ï†êÏàò: ÏÑ†Ìò∏ÎèÑ + Í±∞Î¶¨ + Îã§ÏñëÏÑ±\n",
    "                relevance = scores[idx].item()\n",
    "                final_score = (\n",
    "                    0.4 * relevance +  # ÏÑ†Ìò∏ÎèÑ\n",
    "                    0.4 * distance_score +  # Í±∞Î¶¨\n",
    "                    0.2 * diversity  # Îã§ÏñëÏÑ±\n",
    "                )\n",
    "                \n",
    "                if final_score > best_score:\n",
    "                    best_score = final_score\n",
    "                    best_idx = idx\n",
    "            \n",
    "            if best_idx is not None:\n",
    "                recommendations.append(best_idx)\n",
    "                remaining_indices.remove(best_idx)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _mmr_recommendation(self, scores, embeddings, top_k, diversity_weight):\n",
    "        \"\"\"Í∏∞Ï°¥ MMR Í∏∞Î∞ò Ï∂îÏ≤ú (fallback)\"\"\"\n",
    "        recommendations = []\n",
    "        remaining_indices = list(range(len(scores)))\n",
    "        \n",
    "        # Ï≤´ Î≤àÏß∏ Ï∂îÏ≤ú\n",
    "        if remaining_indices:\n",
    "            valid_scores = [(i, scores[i].item()) for i in remaining_indices if scores[i] > 0]\n",
    "            if valid_scores:\n",
    "                valid_scores = sorted(valid_scores, key=lambda x: x[1], reverse=True)\n",
    "                top_candidates = valid_scores[:5]\n",
    "                if top_candidates:\n",
    "                    best_idx = random.choice(top_candidates)[0]\n",
    "                    recommendations.append(best_idx)\n",
    "                    remaining_indices.remove(best_idx)\n",
    "        \n",
    "        # ÎÇòÎ®∏ÏßÄ Ï∂îÏ≤ú\n",
    "        for _ in range(min(top_k - 1, len(remaining_indices))):\n",
    "            if not remaining_indices:\n",
    "                break\n",
    "                \n",
    "            best_score = -float('inf')\n",
    "            best_idx = None\n",
    "            \n",
    "            for idx in remaining_indices:\n",
    "                relevance = scores[idx].item()\n",
    "                \n",
    "                if relevance < 0:\n",
    "                    continue\n",
    "                \n",
    "                # Îã§ÏñëÏÑ± Í≥ÑÏÇ∞\n",
    "                similarities = []\n",
    "                for rec_idx in recommendations:\n",
    "                    sim = F.cosine_similarity(\n",
    "                        embeddings[idx:idx+1], \n",
    "                        embeddings[rec_idx:rec_idx+1]\n",
    "                    ).item()\n",
    "                    similarities.append(sim)\n",
    "                \n",
    "                diversity = 1 - max(similarities) if similarities else 1\n",
    "                final_score = (1 - diversity_weight) * relevance + diversity_weight * diversity\n",
    "                \n",
    "                if final_score > best_score:\n",
    "                    best_score = final_score\n",
    "                    best_idx = idx\n",
    "            \n",
    "            if best_idx is not None:\n",
    "                recommendations.append(best_idx)\n",
    "                remaining_indices.remove(best_idx)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _apply_preference_weights(self, scores, embeddings):\n",
    "        \"\"\"ÌîºÎìúÎ∞± Í∏∞Î∞ò Ï†êÏàò Ï°∞Ï†ï\"\"\"\n",
    "        if not self.preference_weights:\n",
    "            return scores\n",
    "            \n",
    "        adjusted_scores = scores.clone()\n",
    "        \n",
    "        for i, row in self.visit_area_df.iterrows():\n",
    "            if i >= len(adjusted_scores):\n",
    "                break\n",
    "                \n",
    "            area_type = row.get('VISIT_AREA_TYPE_CD', 0)\n",
    "            \n",
    "            # ÏÑ†Ìò∏ ÌÉÄÏûÖÏù¥Î©¥ Ï†êÏàò Ï¶ùÍ∞Ä\n",
    "            if area_type in self.preference_weights.get('preferred_types', []):\n",
    "                adjusted_scores[i] *= 1.2\n",
    "            \n",
    "            # ÎπÑÏÑ†Ìò∏ ÌÉÄÏûÖÏù¥Î©¥ Ï†êÏàò Í∞êÏÜå\n",
    "            if area_type in self.preference_weights.get('avoided_types', []):\n",
    "                adjusted_scores[i] *= 0.8\n",
    "        \n",
    "        return adjusted_scores\n",
    "    \n",
    "    def update_with_feedback(self, liked_items, disliked_items, embeddings):\n",
    "        \"\"\"ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± ÏóÖÎç∞Ïù¥Ìä∏\"\"\"\n",
    "        feedback = {\n",
    "            'liked': liked_items,\n",
    "            'disliked': disliked_items,\n",
    "            'embeddings': embeddings.cpu().numpy()\n",
    "        }\n",
    "        self.user_feedback_history.append(feedback)\n",
    "        \n",
    "        self.preference_weights = self._calculate_preference_weights()\n",
    "        \n",
    "        print(f\"‚úÖ ÌîºÎìúÎ∞± ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å: Ï¢ãÏïÑÏöî {len(liked_items)}Í∞ú, Ïã´Ïñ¥Ïöî {len(disliked_items)}Í∞ú\")\n",
    "        \n",
    "        return self.preference_weights\n",
    "    \n",
    "    def _calculate_preference_weights(self):\n",
    "        \"\"\"ÌîºÎìúÎ∞± ÌûàÏä§ÌÜ†Î¶¨Î•º Î∞îÌÉïÏúºÎ°ú ÏÑ†Ìò∏ÎèÑ Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞\"\"\"\n",
    "        if not self.user_feedback_history:\n",
    "            return None\n",
    "            \n",
    "        liked_features = []\n",
    "        disliked_features = []\n",
    "        \n",
    "        for feedback in self.user_feedback_history:\n",
    "            for item_idx in feedback['liked']:\n",
    "                if item_idx < len(self.visit_area_df):\n",
    "                    liked_features.append(self.visit_area_df.iloc[item_idx])\n",
    "            \n",
    "            for item_idx in feedback['disliked']:\n",
    "                if item_idx < len(self.visit_area_df):\n",
    "                    disliked_features.append(self.visit_area_df.iloc[item_idx])\n",
    "        \n",
    "        preferred_types = [item.get('VISIT_AREA_TYPE_CD', 0) for item in liked_features]\n",
    "        avoided_types = [item.get('VISIT_AREA_TYPE_CD', 0) for item in disliked_features]\n",
    "        \n",
    "        return {\n",
    "            'preferred_types': list(set(preferred_types)),\n",
    "            'avoided_types': list(set(avoided_types)),\n",
    "            'preferred_regions': [(item.get('X_COORD', 0), item.get('Y_COORD', 0)) for item in liked_features]\n",
    "        }\n",
    "\n",
    "class OptimizedRouteGenerator:\n",
    "    def __init__(self, distance_threshold_km=50):  # ÏûÑÍ≥ÑÍ∞íÏùÑ 50kmÎ°ú Ï§ÑÏûÑ\n",
    "        self.distance_threshold_km = distance_threshold_km\n",
    "        \n",
    "    \n",
    "    def calculate_distance(self, coord1, coord2):\n",
    "        from math import radians, cos, sin, sqrt, atan2\n",
    "        try:\n",
    "            R = 6371  # ÏßÄÍµ¨ Î∞òÍ≤Ω(km)\n",
    "\n",
    "            lat1, lon1 = radians(coord1[1]), radians(coord1[0])\n",
    "            lat2, lon2 = radians(coord2[1]), radians(coord2[0])\n",
    "\n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "\n",
    "            a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "            c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "            distance = R * c\n",
    "            return distance\n",
    "        except:\n",
    "            # ÏòàÏô∏ Î∞úÏÉù Ïãú Îß§Ïö∞ ÌÅ∞ Í∞íÏùÑ Î∞òÌôòÌï¥ Ïù¥ÏÉÅÏπò Ï≤òÎ¶¨\n",
    "            return float('inf')\n",
    "        \n",
    "        \n",
    "    def _two_opt_improvement(self, route, coords):\n",
    "        \"\"\"2-opt ÏïåÍ≥†Î¶¨Ï¶òÏúºÎ°ú Í≤ΩÎ°ú Í∞úÏÑ†\"\"\"\n",
    "        n = len(route)\n",
    "        if n <= 3:\n",
    "            return route\n",
    "            \n",
    "        improved = True\n",
    "        best_route = route[:]\n",
    "        \n",
    "        while improved:\n",
    "            improved = False\n",
    "            for i in range(1, n - 2):\n",
    "                for j in range(i + 1, n):\n",
    "                    if j - i == 1:\n",
    "                        continue\n",
    "                    \n",
    "                    new_route = best_route[:]\n",
    "                    new_route[i:j] = reversed(new_route[i:j])\n",
    "                    \n",
    "                    if self._route_distance(new_route, coords) < self._route_distance(best_route, coords):\n",
    "                        best_route = new_route\n",
    "                        improved = True\n",
    "        \n",
    "        return best_route\n",
    "    \n",
    "    def _route_distance(self, route, coords):\n",
    "        \"\"\"Í≤ΩÎ°úÏùò Ï¥ù Í±∞Î¶¨ Í≥ÑÏÇ∞\"\"\"\n",
    "        total_distance = 0\n",
    "        for i in range(len(route) - 1):\n",
    "            total_distance += self.calculate_distance(\n",
    "                coords[route[i]], \n",
    "                coords[route[i + 1]]\n",
    "            )\n",
    "        return total_distance\n",
    "    \n",
    "    def _solve_tsp_simple(self, locations):\n",
    "        \"\"\"Í∞ÑÎã®Ìïú TSP Ìï¥Î≤ï\"\"\"\n",
    "        if len(locations) <= 2:\n",
    "            return locations\n",
    "            \n",
    "        coords = np.array([loc['coords'] for loc in locations])\n",
    "        n = len(coords)\n",
    "        \n",
    "        # Nearest Neighbor\n",
    "        unvisited = set(range(1, n))\n",
    "        current = 0\n",
    "        route = [0]\n",
    "        \n",
    "        while unvisited:\n",
    "            nearest = min(unvisited, \n",
    "                         key=lambda x: self.calculate_distance(coords[current], coords[x]))\n",
    "            route.append(nearest)\n",
    "            unvisited.remove(nearest)\n",
    "            current = nearest\n",
    "        \n",
    "        # 2-opt improvement\n",
    "        route = self._two_opt_improvement(route, coords)\n",
    "        \n",
    "        return [locations[i] for i in route]\n",
    "        \n",
    "    def remove_outliers(self, coords, locations):\n",
    "        \"\"\"Í±∞Î¶¨ Ïù¥ÏÉÅÏπò Ï†úÍ±∞\"\"\"\n",
    "        if len(coords) <= 2:\n",
    "            return coords, locations\n",
    "            \n",
    "        # Ï§ëÏã¨Ï†ê Í≥ÑÏÇ∞\n",
    "        center = np.mean(coords, axis=0)\n",
    "        \n",
    "        # Í∞Å Ï†êÍ≥º Ï§ëÏã¨Ï†êÏùò Í±∞Î¶¨ Í≥ÑÏÇ∞\n",
    "        distances = [self.calculate_distance(coord, center) for coord in coords]\n",
    "        \n",
    "        # Í±∞Î¶¨ Í∏∞Ï§ÄÏúºÎ°ú ÌïÑÌÑ∞ÎßÅ (ÎÑàÎ¨¥ Î®º Í≥≥ Ï†úÏô∏)\n",
    "        filtered_indices = []\n",
    "        for i, dist in enumerate(distances):\n",
    "            if dist <= self.distance_threshold_km:\n",
    "                filtered_indices.append(i)\n",
    "        \n",
    "        # ÏµúÏÜå Ïû•ÏÜå Ïàò Ïú†ÏßÄ\n",
    "        if len(filtered_indices) < 6:\n",
    "            # Í±∞Î¶¨ÏàúÏúºÎ°ú Ï†ïÎ†¨ÌïòÏó¨ Í∞ÄÍπåÏö¥ Í≥≥Î∂ÄÌÑ∞ ÏÑ†ÌÉù\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            filtered_indices = sorted_indices[:min(10, len(distances))].tolist()\n",
    "        \n",
    "        filtered_coords = [coords[i] for i in filtered_indices]\n",
    "        filtered_locations = [locations[i] for i in filtered_indices]\n",
    "        \n",
    "        return np.array(filtered_coords), filtered_locations\n",
    "        \n",
    "    def generate_daily_routes(self, recommendations, visit_area_df, travel_duration, \n",
    "                            optimization_method='region_based'):\n",
    "        \"\"\"ÏßÄÏó≠ Í∏∞Î∞ò ÏùºÎ≥Ñ ÏµúÏ†Å Í≤ΩÎ°ú ÏÉùÏÑ±\"\"\"\n",
    "        if travel_duration <= 0:\n",
    "            travel_duration = 1\n",
    "            \n",
    "        coords = []\n",
    "        locations = []\n",
    "        \n",
    "        for idx in recommendations:\n",
    "            if idx < len(visit_area_df):\n",
    "                row = visit_area_df.iloc[idx]\n",
    "                coords.append([row['X_COORD'], row['Y_COORD']])\n",
    "                locations.append({\n",
    "                    'id': row['NEW_VISIT_AREA_ID'],\n",
    "                    'name': row['VISIT_AREA_NM'],\n",
    "                    'coords': [row['X_COORD'], row['Y_COORD']],\n",
    "                    'idx': idx,\n",
    "                    'type': row.get('VISIT_AREA_TYPE_CD', 0)\n",
    "                })\n",
    "        \n",
    "        if len(coords) == 0:\n",
    "            return {}\n",
    "            \n",
    "        coords = np.array(coords)\n",
    "        coords = np.nan_to_num(coords, nan=0.0)\n",
    "        \n",
    "        # Ïù¥ÏÉÅÏπò Ï†úÍ±∞\n",
    "        coords, locations = self.remove_outliers(coords, locations)\n",
    "        \n",
    "        # ÏùºÏàòÏóê ÎßûÍ≤å Ïû•ÏÜå Ïàò Ï°∞Ï†ï\n",
    "        places_per_day = max(3, min(5, len(locations) // travel_duration))\n",
    "        total_places = min(places_per_day * travel_duration, len(locations))\n",
    "        \n",
    "        # ÏßÄÏó≠Î≥Ñ ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅÏúºÎ°ú ÏùºÏ†ï ÏÉùÏÑ±\n",
    "        if travel_duration == 1:\n",
    "            # 1Ïùº Ïó¨Ìñâ: TSPÎ°ú ÏµúÏ†Å Í≤ΩÎ°úÎßå ÏÉùÏÑ±\n",
    "            optimized_order = self._solve_tsp_with_start(locations)\n",
    "            return {0: optimized_order[:places_per_day]}\n",
    "        else:\n",
    "            # Îã§Ïùº Ïó¨Ìñâ: ÏßÄÏó≠Î≥ÑÎ°ú Î¨∂Í∏∞\n",
    "            return self._create_regional_routes(coords, locations, travel_duration, places_per_day)\n",
    "    \n",
    "    def _create_regional_routes(self, coords, locations, travel_duration, places_per_day):\n",
    "        \"\"\"ÏßÄÏó≠Î≥ÑÎ°ú Î¨∂Ïñ¥ÏÑú ÏùºÏ†ï ÏÉùÏÑ±\"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        \n",
    "        # Ï†ÅÏ†àÌïú ÌÅ¥Îü¨Ïä§ÌÑ∞ Ïàò Í≤∞Ï†ï\n",
    "        n_clusters = min(travel_duration, len(locations) // 2)\n",
    "        \n",
    "        if n_clusters < 2:\n",
    "            # Ïû•ÏÜåÍ∞Ä ÎÑàÎ¨¥ Ï†ÅÏúºÎ©¥ Í∑†Îì± Î∂ÑÎ∞∞\n",
    "            return self._equal_distribution(locations, travel_duration, places_per_day)\n",
    "        \n",
    "        # K-means ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(coords)\n",
    "        \n",
    "        # ÌÅ¥Îü¨Ïä§ÌÑ∞Î≥Ñ Í∑∏Î£πÌôî\n",
    "        clusters = {}\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append((i, locations[i]))\n",
    "        \n",
    "        # ÌÅ¥Îü¨Ïä§ÌÑ∞ Í∞Ñ Í±∞Î¶¨ Í≥ÑÏÇ∞ (ÏàúÏÑú Í≤∞Ï†ïÏö©)\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        cluster_order = self._order_clusters(cluster_centers)\n",
    "        \n",
    "        # ÏùºÏûêÎ≥Ñ Î∞∞Ï†ï\n",
    "        daily_routes = {}\n",
    "        locations_per_day = len(locations) // travel_duration\n",
    "        remainder = len(locations) % travel_duration\n",
    "        \n",
    "        current_day = 0\n",
    "        current_day_locations = []\n",
    "        \n",
    "        for cluster_idx in cluster_order:\n",
    "            if cluster_idx in clusters:\n",
    "                cluster_locations = [loc for _, loc in clusters[cluster_idx]]\n",
    "                \n",
    "                # TSPÎ°ú ÌÅ¥Îü¨Ïä§ÌÑ∞ ÎÇ¥ ÏµúÏ†Å Í≤ΩÎ°ú\n",
    "                if len(cluster_locations) > 1:\n",
    "                    cluster_locations = self._solve_tsp_simple(cluster_locations)\n",
    "                \n",
    "                for loc in cluster_locations:\n",
    "                    current_day_locations.append(loc)\n",
    "                    \n",
    "                    # ÏùºÏûêÎ≥Ñ Ìï†ÎãπÎüâ Ï≤¥ÌÅ¨\n",
    "                    day_quota = locations_per_day + (1 if current_day < remainder else 0)\n",
    "                    if len(current_day_locations) >= day_quota:\n",
    "                        daily_routes[current_day] = current_day_locations\n",
    "                        current_day += 1\n",
    "                        current_day_locations = []\n",
    "                        \n",
    "                        if current_day >= travel_duration:\n",
    "                            break\n",
    "                \n",
    "                if current_day >= travel_duration:\n",
    "                    break\n",
    "        \n",
    "        # ÎÇ®ÏùÄ Ïû•ÏÜå Ï≤òÎ¶¨\n",
    "        if current_day_locations and current_day < travel_duration:\n",
    "            daily_routes[current_day] = current_day_locations\n",
    "        \n",
    "        # Í∞Å ÏùºÏûêÎ≥Ñ Ïû•ÏÜå Ïàò Ï°∞Ï†ï\n",
    "        return self._adjust_daily_balance(daily_routes, places_per_day)\n",
    "    \n",
    "    def _order_clusters(self, cluster_centers):\n",
    "        \"\"\"ÌÅ¥Îü¨Ïä§ÌÑ∞Î•º Í∞ÄÍπåÏö¥ ÏàúÏÑúÎ°ú Ï†ïÎ†¨\"\"\"\n",
    "        n_clusters = len(cluster_centers)\n",
    "        if n_clusters <= 1:\n",
    "            return list(range(n_clusters))\n",
    "        \n",
    "        # Ï≤´ ÌÅ¥Îü¨Ïä§ÌÑ∞Îäî Í∞ÄÏû• ÎÇ®Ï™Ω (ÎòêÎäî ÏÑúÏ™Ω)\n",
    "        start_idx = np.argmin(cluster_centers[:, 1])  # Y Ï¢åÌëú Í∏∞Ï§Ä\n",
    "        \n",
    "        visited = [start_idx]\n",
    "        current = start_idx\n",
    "        \n",
    "        while len(visited) < n_clusters:\n",
    "            min_dist = float('inf')\n",
    "            next_idx = None\n",
    "            \n",
    "            for i in range(n_clusters):\n",
    "                if i not in visited:\n",
    "                    dist = self.calculate_distance(\n",
    "                        cluster_centers[current], \n",
    "                        cluster_centers[i]\n",
    "                    )\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        next_idx = i\n",
    "            \n",
    "            if next_idx is not None:\n",
    "                visited.append(next_idx)\n",
    "                current = next_idx\n",
    "        \n",
    "        return visited\n",
    "    \n",
    "    def _equal_distribution(self, locations, travel_duration, places_per_day):\n",
    "        \"\"\"Í∑†Îì± Î∂ÑÎ∞∞\"\"\"\n",
    "        daily_routes = {}\n",
    "        locations_per_day = len(locations) // travel_duration\n",
    "        remainder = len(locations) % travel_duration\n",
    "        \n",
    "        start_idx = 0\n",
    "        for day in range(travel_duration):\n",
    "            count = locations_per_day + (1 if day < remainder else 0)\n",
    "            count = min(count, places_per_day)  # ÏùºÎ≥Ñ ÏµúÎåÄ Ïû•ÏÜå Ïàò Ï†úÌïú\n",
    "            daily_routes[day] = locations[start_idx:start_idx + count]\n",
    "            start_idx += count\n",
    "        \n",
    "        return daily_routes\n",
    "    \n",
    "    def _adjust_daily_balance(self, daily_routes, target_size):\n",
    "        \"\"\"ÏùºÎ≥Ñ Ïû•ÏÜå Ïàò Í∑†Ìòï Ï°∞Ï†ï\"\"\"\n",
    "        adjusted = {}\n",
    "        \n",
    "        for day, locations in daily_routes.items():\n",
    "            if len(locations) > target_size:\n",
    "                # ÎÑàÎ¨¥ ÎßéÏúºÎ©¥ ÏûòÎùºÎÇ¥Í∏∞\n",
    "                adjusted[day] = locations[:target_size]\n",
    "            elif len(locations) < 2:\n",
    "                # ÎÑàÎ¨¥ Ï†ÅÏúºÎ©¥ Îã§Î•∏ ÎÇ†ÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "                continue\n",
    "            else:\n",
    "                adjusted[day] = locations\n",
    "        \n",
    "        return adjusted\n",
    "    \n",
    "    def _solve_tsp_with_start(self, locations):\n",
    "        \"\"\"ÏãúÏûëÏ†êÏùÑ Í≥†Î†§Ìïú TSP\"\"\"\n",
    "        if len(locations) <= 2:\n",
    "            return locations\n",
    "        \n",
    "        coords = np.array([loc['coords'] for loc in locations])\n",
    "        n = len(coords)\n",
    "        \n",
    "        # Í∞ÄÏû• Ï†ëÍ∑ºÌïòÍ∏∞ Ïâ¨Ïö¥ Í≥≥ÏùÑ ÏãúÏûëÏ†êÏúºÎ°ú (Í∞ÄÏû• ÏÑúÏ™Ω ÎòêÎäî ÎÇ®Ï™Ω)\n",
    "        start = np.argmin(coords[:, 1])  # Y Ï¢åÌëú Í∏∞Ï§Ä\n",
    "        \n",
    "        # Nearest Neighbor from start\n",
    "        unvisited = set(range(n))\n",
    "        unvisited.remove(start)\n",
    "        current = start\n",
    "        route = [start]\n",
    "        \n",
    "        while unvisited:\n",
    "            nearest = min(unvisited, \n",
    "                         key=lambda x: self.calculate_distance(coords[current], coords[x]))\n",
    "            route.append(nearest)\n",
    "            unvisited.remove(nearest)\n",
    "            current = nearest\n",
    "        \n",
    "        # 2-opt improvement\n",
    "        route = self._two_opt_improvement(route, coords)\n",
    "        \n",
    "        return [locations[i] for i in route]\n",
    "        \n",
    "\n",
    "def process_travel_input(travel_info: dict):\n",
    "    \"\"\"Ïó¨Ìñâ Ï†ïÎ≥¥ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò\"\"\"\n",
    "    travel_feature_cols = [\n",
    "        'TOTAL_COST_BINNED_ENCODED', 'WITH_PET', 'MONTH', 'DURATION',\n",
    "        'MVMN_Í∏∞ÌÉÄ', 'MVMN_ÎåÄÏ§ëÍµêÌÜµ', 'MVMN_ÏûêÍ∞ÄÏö©',\n",
    "        'TRAVEL_PURPOSE_1', 'TRAVEL_PURPOSE_2', 'TRAVEL_PURPOSE_3',\n",
    "        'TRAVEL_PURPOSE_4', 'TRAVEL_PURPOSE_5', 'TRAVEL_PURPOSE_6',\n",
    "        'TRAVEL_PURPOSE_7', 'TRAVEL_PURPOSE_8', 'TRAVEL_PURPOSE_9',\n",
    "        'WHOWITH_2Ïù∏Ïó¨Ìñâ', 'WHOWITH_Í∞ÄÏ°±Ïó¨Ìñâ', 'WHOWITH_Í∏∞ÌÉÄ',\n",
    "        'WHOWITH_Îã®ÎèÖÏó¨Ìñâ', 'WHOWITH_ÏπúÍµ¨/ÏßÄÏù∏ Ïó¨Ìñâ', 'LOCATION'\n",
    "    ]\n",
    "    \n",
    "    # Î∞òÎ†§ÎèôÎ¨º ÎèôÎ∞ò\n",
    "    travel_info['mission_ENC'] = travel_info['mission_ENC'].strip().split(',')\n",
    "    travel_info['WITH_PET'] = 1 if '0' in travel_info['mission_ENC'] else 0\n",
    "        \n",
    "    # Ïó¨Ìñâ Î™©Ï†Å\n",
    "    for i in range(1, 10):\n",
    "        travel_info[f'TRAVEL_PURPOSE_{i}'] = 1 if str(i) in travel_info['mission_ENC'] else 0\n",
    "        \n",
    "    # ÎÇ†Ïßú Ï≤òÎ¶¨\n",
    "    dates = travel_info['date_range'].split(' - ')\n",
    "    start_date = datetime.strptime(dates[0].strip(), \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(dates[1].strip(), \"%Y-%m-%d\")\n",
    "    \n",
    "    travel_info['MONTH'] = end_date.month\n",
    "    travel_info['DURATION'] = (end_date - start_date).days + 1  # +1 Ï∂îÍ∞Ä\n",
    "    \n",
    "    # ÍµêÌÜµÏàòÎã®\n",
    "    for m in ['ÏûêÍ∞ÄÏö©', 'ÎåÄÏ§ëÍµêÌÜµ', 'Í∏∞ÌÉÄ']:\n",
    "        travel_info[f\"MVMN_{m}\"] = 0\n",
    "    \n",
    "    if travel_info['MVMN_NM_ENC'] == '1':\n",
    "        travel_info['MVMN_ÏûêÍ∞ÄÏö©'] = 1\n",
    "    elif travel_info['MVMN_NM_ENC'] == '2':\n",
    "        travel_info['MVMN_ÎåÄÏ§ëÍµêÌÜµ'] = 1\n",
    "    else:\n",
    "        travel_info['MVMN_Í∏∞ÌÉÄ'] = 1\n",
    "    \n",
    "    # ÎèôÌñâÏûê\n",
    "    whowith_onehot = [0] * 5\n",
    "    idx = int(travel_info['whowith_ENC']) - 1\n",
    "    if 0 <= idx < 5:\n",
    "        whowith_onehot[idx] = 1\n",
    "    \n",
    "    travel_info.update({\n",
    "        'WHOWITH_Îã®ÎèÖÏó¨Ìñâ': whowith_onehot[0],\n",
    "        'WHOWITH_2Ïù∏Ïó¨Ìñâ': whowith_onehot[1],\n",
    "        'WHOWITH_Í∞ÄÏ°±Ïó¨Ìñâ': whowith_onehot[2],\n",
    "        'WHOWITH_ÏπúÍµ¨/ÏßÄÏù∏ Ïó¨Ìñâ': whowith_onehot[3],\n",
    "        'WHOWITH_Í∏∞ÌÉÄ': whowith_onehot[4],\n",
    "    })\n",
    "    \n",
    "    # ÎπÑÏö©\n",
    "    travel_info['TOTAL_COST_BINNED_ENCODED'] = int(travel_info['TOTAL_COST'])\n",
    "    \n",
    "    # ÏµúÏ¢Ö Î≤°ÌÑ∞ ÏÉùÏÑ±\n",
    "    travel_vector = [int(travel_info.get(k, 0)) for k in travel_feature_cols]\n",
    "    \n",
    "    return np.array([travel_vector]).astype(np.float32)\n",
    "\n",
    "def simulate_user_feedback():\n",
    "    \"\"\"ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± ÏãúÎÆ¨Î†àÏù¥ÏÖò\"\"\"\n",
    "    feedback_options = [\n",
    "        {\"liked\": [], \"disliked\": [0, 2]},  # Ï≤´ Î≤àÏß∏ÏôÄ ÏÑ∏ Î≤àÏß∏ Ïû•ÏÜå Ïã´Ïñ¥Ïöî\n",
    "        {\"liked\": [1], \"disliked\": [4, 7]},  # Îëê Î≤àÏß∏ Ïû•ÏÜå Ï¢ãÏïÑÏöî, Îã§Î•∏ Í≥≥Îì§ Ïã´Ïñ¥Ïöî\n",
    "        {\"liked\": [0, 3], \"disliked\": [5]},  # Î≥µÏàò Ï¢ãÏïÑÏöî/Ïã´Ïñ¥Ïöî\n",
    "    ]\n",
    "    \n",
    "    return random.choice(feedback_options)\n",
    "\n",
    "def main_feedback_test():\n",
    "    print(\"üöÄ Í∞úÏÑ†Îêú ÌîºÎìúÎ∞± Í∏∞Î∞ò Í≤ΩÎ°ú ÎåÄÏ≤¥ ÌÖåÏä§Ìä∏ ÏãúÏûë!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ Î°úÎî©\n",
    "    move_path = \"./merged_csv/fin/Ïù¥ÎèôÎÇ¥Ïó≠_fin.csv\"\n",
    "    travel_path = \"./merged_csv/fin/Ïó¨Ìñâ_fin.csv\"\n",
    "    visit_area_path = \"./merged_csv/fin/Î∞©Î¨∏ÏßÄ_fin.csv\"\n",
    "    \n",
    "    move_df = pd.read_csv(move_path)\n",
    "    travel_df = pd.read_csv(travel_path)\n",
    "    visit_area_df = pd.read_csv(visit_area_path)\n",
    "    \n",
    "    processor = EnhancedDataProcessor()\n",
    "    visit_area_tensor = processor.process_visit_area_features(visit_area_df)\n",
    "    edge_index, edge_attr = processor.create_enhanced_edges(move_df, visit_area_df)\n",
    "    \n",
    "    data = HeteroData()\n",
    "    data['visit_area'].x = torch.tensor(visit_area_tensor, dtype=torch.float32)\n",
    "    data['visit_area', 'moved_to', 'visit_area'].edge_index = edge_index\n",
    "    data['visit_area', 'moved_to', 'visit_area'].edge_attr = edge_attr\n",
    "    \n",
    "    # Ïó¨Ìñâ Ï†ïÎ≥¥ (2Ïùº Ïó¨Ìñâ)\n",
    "    travel_example = {\n",
    "        'mission_ENC': '0,1,2',\n",
    "        'date_range': '2025-09-28 - 2025-09-29',  # 2Ïùº Ïó¨ÌñâÏúºÎ°ú Î≥ÄÍ≤Ω\n",
    "        'start_date': '',\n",
    "        'end_date': '',\n",
    "        'TOTAL_COST': '2',\n",
    "        'MVMN_NM_ENC': '2',\n",
    "        'whowith_ENC': '2',\n",
    "        'mission_type': 'normal',\n",
    "        'location': 0\n",
    "    }\n",
    "    \n",
    "    travel_tensor = process_travel_input(travel_example)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"üì± ÏÇ¨Ïö© ÎîîÎ∞îÏù¥Ïä§: {device}\")\n",
    "    print(f\"üìÖ Ïó¨Ìñâ Í∏∞Í∞Ñ: {travel_example['date_range']} ({travel_tensor[0, 3]:.0f}Ïùº)\")\n",
    "    \n",
    "    model = ImprovedTravelGNN(\n",
    "        in_channels=visit_area_tensor.shape[1],\n",
    "        hidden_channels=128,\n",
    "        out_channels=64,\n",
    "        travel_context_dim=travel_tensor.shape[1],\n",
    "        num_heads=4,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    data = data.to(device)\n",
    "    travel_context_tensor = torch.tensor(travel_tensor, dtype=torch.float32).to(device)\n",
    "    \n",
    "    recommender = SmartRecommendationEngine(model, visit_area_df, device)\n",
    "    print(\"ÏóîÏßÑ Ï¥àÍ∏∞Ìôî\")\n",
    "    # Ï¥àÍ∏∞ Ï∂îÏ≤ú (ÌïÑÌÑ∞ÎßÅ Ï†ÅÏö©, Í±∞Î¶¨ Í≥†Î†§)\n",
    "    recommendations, embeddings, _ = recommender.get_recommendations(\n",
    "        data, travel_context_tensor, top_k=30, diversity_weight=0.3, \n",
    "        filter_useless=True, consider_distance=True\n",
    "    )\n",
    "    print(\"Ï∂îÏ≤ú ÏãúÏûë\")\n",
    "    # Ï§ëÎ≥µ Î∞©Î¨∏ÏßÄ Ï†úÍ±∞ Î∞è Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨\n",
    "    unique_recommendations, seen_ids = [], set()\n",
    "    for idx in recommendations:\n",
    "        if idx < len(visit_area_df):\n",
    "            row = visit_area_df.iloc[idx]\n",
    "            area_id = row['NEW_VISIT_AREA_ID']\n",
    "            name = row['VISIT_AREA_NM']\n",
    "            \n",
    "            # Ï§ëÎ≥µ Ï≤¥ÌÅ¨ Î∞è Ïì∏Î™®ÏóÜÎäî Ïû•ÏÜå Ïû¨ÌôïÏù∏\n",
    "            if (area_id not in seen_ids and \n",
    "                area_id != 0 and \n",
    "                not processor.should_exclude_location(name)):\n",
    "                unique_recommendations.append(idx)\n",
    "                seen_ids.add(area_id)\n",
    "            \n",
    "            if len(unique_recommendations) >= 15:  # Ïó¨Ïú†ÏûàÍ≤å ÏÑ†ÌÉù\n",
    "                break\n",
    "    print(\"Ïú†Ìö® Î∞©Î¨∏ÏßÄ Í≤ÄÏÇ¨\")\n",
    "    # ÏµúÏ†ÅÌôî Í≤ΩÎ°ú ÏÉùÏÑ±\n",
    "    route_generator = OptimizedRouteGenerator(distance_threshold_km=50)  # Í±∞Î¶¨ ÏûÑÍ≥ÑÍ∞í Ï§ÑÏûÑ\n",
    "    travel_duration = int(travel_tensor[0, 3])\n",
    "    optimized_routes = route_generator.generate_daily_routes(\n",
    "        unique_recommendations, visit_area_df, travel_duration\n",
    "    )\n",
    "    print(\"Í≤ΩÎ°ú ÏÉùÏÑ± ÏôÑ\")\n",
    "    print(\"\\nüóìÔ∏è Ï¥àÍ∏∞ Ïó¨Ìñâ ÏùºÏ†ï (ÏµúÏ†ÅÌôî Î∞è ÌïÑÌÑ∞ÎßÅ Ï†ÅÏö©):\")\n",
    "    total_places = 0\n",
    "    for day, route in sorted(optimized_routes.items()):\n",
    "        print(f\"\\nüìÖ Day {day + 1}:\")\n",
    "        for i, loc in enumerate(route):\n",
    "            print(f\" {i+1}. [{loc['id']:3d}] {loc['name']}\")\n",
    "        total_places += len(route)\n",
    "    print(f\"\\nÏ¥ù {total_places}Í∞ú Ïû•ÏÜå Ï∂îÏ≤ú\")\n",
    "    \n",
    "    # ÌîºÎìúÎ∞± ÎùºÏö¥Îìú Î∞òÎ≥µ\n",
    "    for round_num in range(1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîÑ ÌîºÎìúÎ∞± ÎùºÏö¥Îìú {round_num + 1}\")\n",
    "        \n",
    "        feedback = simulate_user_feedback()\n",
    "        liked_indices = [unique_recommendations[i] for i in feedback[\"liked\"] if i < len(unique_recommendations)]\n",
    "        disliked_indices = [unique_recommendations[i] for i in feedback[\"disliked\"] if i < len(unique_recommendations)]\n",
    "        \n",
    "        if liked_indices:\n",
    "            print(f\"üëç Ï¢ãÏïÑÏöî: {[visit_area_df.iloc[idx]['VISIT_AREA_NM'] for idx in liked_indices]}\")\n",
    "        if disliked_indices:\n",
    "            print(f\"üëé Ïã´Ïñ¥Ïöî: {[visit_area_df.iloc[idx]['VISIT_AREA_NM'] for idx in disliked_indices]}\")\n",
    "        \n",
    "        recommender.update_with_feedback(liked_indices, disliked_indices, embeddings)\n",
    "        \n",
    "        # Ï†úÏô∏Îêú Ìï≠Î™© Î∞òÏòÅ\n",
    "        excluded_ids = {visit_area_df.iloc[idx]['NEW_VISIT_AREA_ID'] for idx in disliked_indices}\n",
    "        \n",
    "        recommendations, embeddings, _ = recommender.get_recommendations(\n",
    "            data, travel_context_tensor, top_k=30, diversity_weight=0.3, \n",
    "            excluded_ids=excluded_ids, filter_useless=True, consider_distance=True\n",
    "        )\n",
    "        \n",
    "        unique_recommendations, seen_ids = [], set()\n",
    "        for idx in recommendations:\n",
    "            if idx < len(visit_area_df):\n",
    "                row = visit_area_df.iloc[idx]\n",
    "                area_id = row['NEW_VISIT_AREA_ID']\n",
    "                name = row['VISIT_AREA_NM']\n",
    "                \n",
    "                if (area_id not in seen_ids and \n",
    "                    area_id not in excluded_ids and \n",
    "                    area_id != 0 and\n",
    "                    not processor.should_exclude_location(name)):\n",
    "                    unique_recommendations.append(idx)\n",
    "                    seen_ids.add(area_id)\n",
    "                \n",
    "                if len(unique_recommendations) >= 15:\n",
    "                    break\n",
    "        \n",
    "        optimized_routes = route_generator.generate_daily_routes(\n",
    "            unique_recommendations, visit_area_df, travel_duration\n",
    "        )\n",
    "        \n",
    "        print(\"\\nüéØ ÌîºÎìúÎ∞± Î∞òÏòÅ ÌõÑ ÏµúÏ†ÅÌôîÎêú Ïó¨Ìñâ ÏùºÏ†ï:\")\n",
    "        total_places = 0\n",
    "        for day, route in sorted(optimized_routes.items()):\n",
    "            print(f\"\\nüìÖ Day {day + 1}:\")\n",
    "            for i, loc in enumerate(route):\n",
    "                print(f\" {i+1}. [{loc['id']:3d}] {loc['name']}\")\n",
    "            total_places += len(route)\n",
    "        print(f\"\\nÏ¥ù {total_places}Í∞ú Ïû•ÏÜå Ï∂îÏ≤ú\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Í∞úÏÑ†Îêú ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± Í∏∞Î∞ò Í≤ΩÎ°ú ÎåÄÏ≤¥ ÌÖåÏä§Ìä∏ ÏôÑÎ£å!\")\n",
    "    \n",
    "    # Í∞úÏÑ† ÏÇ¨Ìï≠ ÏöîÏïΩ\n",
    "    print(\"\\nüìä Ï£ºÏöî Í∞úÏÑ† ÏÇ¨Ìï≠:\")\n",
    "    print(\"1. ‚úÖ Ïó≠, ÌÑ∞ÎØ∏ÎÑê, Í≥µÌï≠ Îì± Î∂àÌïÑÏöîÌïú Ïû•ÏÜå ÌïÑÌÑ∞ÎßÅ\")\n",
    "    print(\"2. ‚úÖ Í±∞Î¶¨ Í∏∞Î∞ò Ïù¥ÏÉÅÏπò Ï†úÍ±∞Î°ú ÏßÄÏó≠Î≥Ñ Í∑∏Î£πÌôî Í∞úÏÑ†\")\n",
    "    print(\"3. ‚úÖ ÏùºÏûêÎ≥Ñ Í∑†ÌòïÏûàÎäî Ïû•ÏÜå Î∞∞Î∂Ñ (3-5Í∞ú/Ïùº)\")\n",
    "    print(\"4. ‚úÖ DBSCAN ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅÏúºÎ°ú Îçî Ï†ïÌôïÌïú ÏßÄÏó≠ Íµ¨Î∂Ñ\")\n",
    "    print(\"5. ‚úÖ 2-opt ÏïåÍ≥†Î¶¨Ï¶òÏúºÎ°ú Í≤ΩÎ°ú ÏµúÏ†ÅÌôî Í∞úÏÑ†\")\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ï§ÄÎπÑ\n",
    "    save_data = {\n",
    "        'visit_area_df': visit_area_df,\n",
    "        'graph_data': data,\n",
    "        'visit_scaler': processor.visit_scaler,\n",
    "        'travel_scaler': processor.travel_scaler,\n",
    "        'device': str(device)\n",
    "    }\n",
    "    \n",
    "    # ÌååÏùº Ï†ÄÏû•\n",
    "    print(\"\\nüíæ ÌååÏùº Ï†ÄÏû• Ï§ë...\")\n",
    "    \n",
    "    # Î™®Îç∏ Ï†ÄÏû• (.pt)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'in_channels': visit_area_tensor.shape[1],\n",
    "            'hidden_channels': 128,\n",
    "            'out_channels': 64,\n",
    "            'travel_context_dim': 22,\n",
    "            'num_heads': 4,\n",
    "            'dropout': 0.2\n",
    "        }\n",
    "    }, 'improved_travel_recommendation_model.pt')\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• (.pkl)\n",
    "    with open('improved_travel_data.pkl', 'wb') as f:\n",
    "        pickle.dump(save_data, f)\n",
    "    \n",
    "    print(\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å!\")\n",
    "    print(\"- improved_travel_recommendation_model.pt: Í∞úÏÑ†Îêú Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞\")\n",
    "    print(\"- improved_travel_data.pkl: Ï†ÑÏ≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Î∞è Ïä§ÏºÄÏùºÎü¨\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_feedback_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ Í∞úÏÑ†Îêú ÌîºÎìúÎ∞± Í∏∞Î∞ò Í≤ΩÎ°ú ÎåÄÏ≤¥ ÌÖåÏä§Ìä∏ ÏãúÏûë!\n",
    "============================================================\n",
    "üì± ÏÇ¨Ïö© ÎîîÎ∞îÏù¥Ïä§: cpu\n",
    "üìÖ Ïó¨Ìñâ Í∏∞Í∞Ñ: 2025-09-28 - 2025-09-29 (2Ïùº)\n",
    "\n",
    "üóìÔ∏è Ï¥àÍ∏∞ Ïó¨Ìñâ ÏùºÏ†ï (ÏµúÏ†ÅÌôî Î∞è ÌïÑÌÑ∞ÎßÅ Ï†ÅÏö©):\n",
    "\n",
    "üìÖ Day 1:\n",
    " 1. [799] ÌòÑÎåÄÎ∞±ÌôîÏ†ê Ï§ëÎèôÏ†ê\n",
    " 2. [941] ÏïàÏ§ëÍ∑ºÍ≥µÏõê\n",
    " 3. [1002] Ïã†Ïã† Î∂ÑÏãù\n",
    " 4. [1578] Ïã†Ìè¨Íµ≠Ï†úÏãúÏû•\n",
    " 5. [7035] Ïù¥ÎîîÏïºÏª§Ìîº Ïù∏Ï≤ú Ï∞®Ïù¥ÎÇòÌÉÄÏö¥Ï†ê\n",
    "\n",
    "üìÖ Day 2:\n",
    " 1. [2726] ÌååÏ£º Îã≠ Íµ≠Ïàò ÌååÏ£º Î≥∏Ï†ê\n",
    " 2. [8027] ÌôîÍ∞ú Ï†ïÏõê\n",
    " 3. [8167] ÌôîÍ∞úÏÇ∞ ÏÜêÏπºÍµ≠Ïàò\n",
    " 4. [8028] ÎåÄÎ£°ÏãúÏû•\n",
    " 5. [8026] ÌñâÎ≥µÌïú ÏãúÍ≥®Î∞•ÏÉÅ\n",
    "\n",
    "Ï¥ù 10Í∞ú Ïû•ÏÜå Ï∂îÏ≤ú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
