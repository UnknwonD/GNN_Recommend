{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-08 15:12:08,199 - INFO - 🌍 전체 지역 GNN 모델 학습 시작!\n",
      "2025-06-08 15:12:08,200 - INFO - 📍 총 4개 지역 발견: ['서부권', '동부권', '제주도 및 도서지역', '수도권']\n",
      "2025-06-08 15:12:08,200 - INFO - \n",
      "🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-08 15:12:08,200 - INFO - 📍 [1/4] 지역 '서부권' 처리 시작!\n",
      "2025-06-08 15:12:08,200 - INFO - 🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-08 15:12:08,201 - INFO - \n",
      "============================================================\n",
      "2025-06-08 15:12:08,201 - INFO - 🚀 지역 '서부권' GNN 모델 학습 시작!\n",
      "2025-06-08 15:12:08,201 - INFO - ============================================================\n",
      "2025-06-08 15:12:08,201 - INFO - 📂 데이터 로드 중...\n",
      "2025-06-08 15:12:08,365 - INFO - 🔍 데이터 검증 중...\n",
      "2025-06-08 15:12:08,373 - INFO - ✅ 여행정보 데이터 로드: 3222개 레코드\n",
      "2025-06-08 15:12:08,374 - INFO - ✅ 데이터 로드 및 검증 완료:\n",
      "2025-06-08 15:12:08,374 - INFO -   - 이동내역: 36226개 레코드 (유효 이동: 35865개)\n",
      "2025-06-08 15:12:08,374 - INFO -   - 방문지: 36226개 레코드 (유효 방문지: 36226개)\n",
      "2025-06-08 15:12:08,374 - INFO - 🔧 데이터 전처리 중...\n",
      "2025-06-08 15:12:08,375 - INFO - 데이터 준비 시작...\n",
      "2025-06-08 15:12:08,375 - INFO - 엣지 생성 시작...\n",
      "2025-06-08 15:12:08,377 - INFO - 방문지 데이터에서 사용 가능한 ID: 13061개\n",
      "2025-06-08 15:12:08,385 - INFO - 최종 사용할 방문지 ID: 185개\n",
      "2025-06-08 15:12:08,385 - INFO - 방문지 ID 매핑 생성: 185개\n",
      "Processing travel groups: 100%|██████████| 3222/3222 [00:01<00:00, 1754.95it/s]\n",
      "2025-06-08 15:12:10,271 - INFO - 엣지 생성 통계:\n",
      "2025-06-08 15:12:10,271 - INFO -   - 총 이동 시도: 33004개\n",
      "2025-06-08 15:12:10,272 - INFO -   - 유효한 엣지: 1574개\n",
      "2025-06-08 15:12:10,272 - INFO -   - TRIP_ID 누락: 509개\n",
      "2025-06-08 15:12:10,272 - INFO -   - 유효하지 않은 FROM_ID: 22750개\n",
      "2025-06-08 15:12:10,272 - INFO -   - 유효하지 않은 TO_ID: 6626개\n",
      "2025-06-08 15:12:10,281 - INFO - ✅ 엣지 생성 완료:\n",
      "2025-06-08 15:12:10,281 - INFO -   - 최종 엣지 수: 1574개\n",
      "2025-06-08 15:12:10,281 - INFO -   - 노드 인덱스 범위: 0 ~ 184\n",
      "2025-06-08 15:12:10,282 - INFO -   - 엣지 인덱스 최대값: 184\n",
      "2025-06-08 15:12:10,296 - INFO - NEW_VISIT_AREA_ID가 있는 방문지: 36226개\n",
      "2025-06-08 15:12:10,300 - INFO - ID 매핑에 있는 방문지: 8962개\n",
      "2025-06-08 15:12:10,301 - INFO - 중복된 방문지 ID 집계 중...\n",
      "2025-06-08 15:12:10,306 - INFO - 집계 후 유니크 방문지: 185개\n",
      "2025-06-08 15:12:10,310 - INFO - 최종 정렬된 방문지: 185개\n",
      "2025-06-08 15:12:10,316 - INFO - 방문지 특성 처리 완료: (185, 35)\n",
      "2025-06-08 15:12:10,320 - INFO - 학습 타겟 생성 시작...\n",
      "2025-06-08 15:12:10,327 - INFO - 중복된 방문지 타겟 정보 집계 중...\n",
      "2025-06-08 15:12:10,334 - INFO - 집계된 유니크 방문지: 13061개\n",
      "2025-06-08 15:12:10,337 - INFO - 방문 빈도 계산 완료: 1371개 방문지, 최대 방문 8211회\n",
      "2025-06-08 15:12:10,354 - INFO - 타겟 생성 완료: 185개\n",
      "2025-06-08 15:12:10,354 - INFO - 타겟 점수 범위: 0.000 ~ 0.630\n",
      "2025-06-08 15:12:10,357 - INFO - 여행 컨텍스트 처리 시작...\n",
      "2025-06-08 15:12:10,363 - INFO - ✅ 데이터 준비 완료:\n",
      "2025-06-08 15:12:10,363 - INFO -   - 노드 수: 185\n",
      "2025-06-08 15:12:10,363 - INFO -   - 특성 수: 35\n",
      "2025-06-08 15:12:10,364 - INFO -   - 엣지 수: 1574\n",
      "2025-06-08 15:12:10,364 - INFO -   - 여행 컨텍스트: (3222, 21)\n",
      "2025-06-08 15:12:10,364 - INFO -   - ID 매핑 수: 185\n",
      "2025-06-08 15:12:10,364 - INFO -   - 타겟 수: 185\n",
      "2025-06-08 15:12:10,364 - INFO -   - 엣지 인덱스 범위: 0 ~ 184\n",
      "2025-06-08 15:12:10,365 - INFO - 🎯 모델 학습 시작...\n",
      "2025-06-08 15:12:10,365 - INFO - 모델 학습 시작...\n",
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]2025-06-08 15:12:10,568 - INFO - Epoch 000, Loss: 0.1057, Best: 0.1057, LR: 0.001000\n",
      "Training:   8%|▊         | 17/200 [00:00<00:04, 41.43it/s]2025-06-08 15:12:10,972 - INFO - Epoch 020, Loss: 0.0152, Best: 0.0152, LR: 0.001000\n",
      "Training:  18%|█▊        | 35/200 [00:00<00:03, 50.86it/s]2025-06-08 15:12:11,351 - INFO - Epoch 040, Loss: 0.0090, Best: 0.0090, LR: 0.001000\n",
      "Training:  30%|███       | 60/200 [00:01<00:02, 57.06it/s]2025-06-08 15:12:11,684 - INFO - Epoch 060, Loss: 0.0089, Best: 0.0076, LR: 0.001000\n",
      "Training:  39%|███▉      | 78/200 [00:01<00:02, 56.88it/s]2025-06-08 15:12:12,032 - INFO - Epoch 080, Loss: 0.0077, Best: 0.0063, LR: 0.001000\n",
      "Training:  49%|████▉     | 98/200 [00:01<00:01, 58.46it/s]2025-06-08 15:12:12,401 - INFO - Epoch 100, Loss: 0.0057, Best: 0.0057, LR: 0.001000\n",
      "Training:  58%|█████▊    | 116/200 [00:02<00:01, 54.72it/s]2025-06-08 15:12:12,754 - INFO - Epoch 120, Loss: 0.0074, Best: 0.0052, LR: 0.001000\n",
      "Training:  68%|██████▊   | 135/200 [00:02<00:01, 58.11it/s]2025-06-08 15:12:13,089 - INFO - Epoch 140, Loss: 0.0044, Best: 0.0044, LR: 0.001000\n",
      "Training:  78%|███████▊  | 155/200 [00:02<00:00, 58.90it/s]2025-06-08 15:12:13,424 - INFO - Epoch 160, Loss: 0.0043, Best: 0.0035, LR: 0.001000\n",
      "Training:  81%|████████  | 162/200 [00:03<00:00, 59.68it/s]2025-06-08 15:12:13,526 - INFO - Epoch 166: Learning rate reduced from 0.001000 to 0.000500\n",
      "Training:  90%|█████████ | 180/200 [00:03<00:00, 58.70it/s]2025-06-08 15:12:13,766 - INFO - Epoch 180, Loss: 0.0042, Best: 0.0035, LR: 0.000500\n",
      "Training:  94%|█████████▎| 187/200 [00:03<00:00, 59.31it/s]2025-06-08 15:12:13,881 - INFO - Epoch 187: Learning rate reduced from 0.000500 to 0.000250\n",
      "Training:  96%|█████████▋| 193/200 [00:03<00:00, 58.92it/s]2025-06-08 15:12:14,022 - INFO - Early stopping at epoch 195\n",
      "Training:  98%|█████████▊| 195/200 [00:03<00:00, 54.07it/s]\n",
      "2025-06-08 15:12:14,023 - INFO - 학습 완료! 최종 손실: 0.0035\n",
      "2025-06-08 15:12:14,024 - INFO - 💾 데이터 저장 중...\n",
      "2025-06-08 15:12:14,024 - INFO - 처리된 데이터 저장 시작...\n",
      "2025-06-08 15:12:14,046 - INFO - 데이터 저장 완료: ./pickle/서부권/\n",
      "2025-06-08 15:12:14,046 - INFO - 저장된 데이터 구조:\n",
      "2025-06-08 15:12:14,046 - INFO -   - visit_area_df: (36226, 25)\n",
      "2025-06-08 15:12:14,046 - INFO -   - graph_data: 노드 185개, 엣지 1574개\n",
      "2025-06-08 15:12:14,047 - INFO -   - visit_scaler: RobustScaler\n",
      "2025-06-08 15:12:14,047 - INFO -   - travel_scaler: StandardScaler\n",
      "2025-06-08 15:12:14,047 - INFO -   - id_to_index: 185개 매핑\n",
      "2025-06-08 15:12:14,047 - INFO -   - device: cpu\n",
      "2025-06-08 15:12:14,047 - INFO - \n",
      "============================================================\n",
      "2025-06-08 15:12:14,048 - INFO - ✅ 지역 '서부권' 학습 완료!\n",
      "2025-06-08 15:12:14,048 - INFO - 📁 모델 저장: ./models/서부권/\n",
      "2025-06-08 15:12:14,048 - INFO - 📁 데이터 저장: ./pickle/서부권/\n",
      "2025-06-08 15:12:14,048 - INFO - 📊 최종 손실: 0.0035\n",
      "2025-06-08 15:12:14,048 - INFO - 📊 총 에포크: 196\n",
      "2025-06-08 15:12:14,048 - INFO - 🎯 노드 수: 185, 엣지 수: 1574\n",
      "2025-06-08 15:12:14,049 - INFO - ============================================================\n",
      "2025-06-08 15:12:14,053 - INFO - ✅ 지역 '서부권' 처리 완료!\n",
      "2025-06-08 15:12:14,053 - INFO - \n",
      "🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-08 15:12:14,053 - INFO - 📍 [2/4] 지역 '동부권' 처리 시작!\n",
      "2025-06-08 15:12:14,054 - INFO - 🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-08 15:12:14,054 - INFO - \n",
      "============================================================\n",
      "2025-06-08 15:12:14,054 - INFO - 🚀 지역 '동부권' GNN 모델 학습 시작!\n",
      "2025-06-08 15:12:14,054 - INFO - ============================================================\n",
      "2025-06-08 15:12:14,054 - INFO - 📂 데이터 로드 중...\n",
      "2025-06-08 15:12:14,197 - INFO - 🔍 데이터 검증 중...\n",
      "2025-06-08 15:12:14,205 - INFO - ✅ 여행정보 데이터 로드: 3223개 레코드\n",
      "2025-06-08 15:12:14,206 - INFO - ✅ 데이터 로드 및 검증 완료:\n",
      "2025-06-08 15:12:14,206 - INFO -   - 이동내역: 37164개 레코드 (유효 이동: 36835개)\n",
      "2025-06-08 15:12:14,206 - INFO -   - 방문지: 37164개 레코드 (유효 방문지: 37164개)\n",
      "2025-06-08 15:12:14,207 - INFO - 🔧 데이터 전처리 중...\n",
      "2025-06-08 15:12:14,207 - INFO - 데이터 준비 시작...\n",
      "2025-06-08 15:12:14,207 - INFO - 엣지 생성 시작...\n",
      "2025-06-08 15:12:14,209 - INFO - 방문지 데이터에서 사용 가능한 ID: 14081개\n",
      "2025-06-08 15:12:14,216 - INFO - 최종 사용할 방문지 ID: 258개\n",
      "2025-06-08 15:12:14,216 - INFO - 방문지 ID 매핑 생성: 258개\n",
      "Processing travel groups: 100%|██████████| 3223/3223 [00:01<00:00, 1652.77it/s]\n",
      "2025-06-08 15:12:16,299 - INFO - 엣지 생성 통계:\n",
      "2025-06-08 15:12:16,299 - INFO -   - 총 이동 시도: 33941개\n",
      "2025-06-08 15:12:16,299 - INFO -   - 유효한 엣지: 1418개\n",
      "2025-06-08 15:12:16,299 - INFO -   - TRIP_ID 누락: 445개\n",
      "2025-06-08 15:12:16,300 - INFO -   - 유효하지 않은 FROM_ID: 23703개\n",
      "2025-06-08 15:12:16,300 - INFO -   - 유효하지 않은 TO_ID: 6870개\n",
      "2025-06-08 15:12:16,304 - INFO - ✅ 엣지 생성 완료:\n",
      "2025-06-08 15:12:16,304 - INFO -   - 최종 엣지 수: 1418개\n",
      "2025-06-08 15:12:16,304 - INFO -   - 노드 인덱스 범위: 0 ~ 257\n",
      "2025-06-08 15:12:16,306 - INFO -   - 엣지 인덱스 최대값: 257\n",
      "2025-06-08 15:12:16,328 - INFO - NEW_VISIT_AREA_ID가 있는 방문지: 37164개\n",
      "2025-06-08 15:12:16,334 - INFO - ID 매핑에 있는 방문지: 10414개\n",
      "2025-06-08 15:12:16,334 - INFO - 중복된 방문지 ID 집계 중...\n",
      "2025-06-08 15:12:16,338 - INFO - 집계 후 유니크 방문지: 258개\n",
      "2025-06-08 15:12:16,341 - INFO - 최종 정렬된 방문지: 258개\n",
      "2025-06-08 15:12:16,346 - INFO - 방문지 특성 처리 완료: (258, 35)\n",
      "2025-06-08 15:12:16,351 - INFO - 학습 타겟 생성 시작...\n",
      "2025-06-08 15:12:16,357 - INFO - 중복된 방문지 타겟 정보 집계 중...\n",
      "2025-06-08 15:12:16,364 - INFO - 집계된 유니크 방문지: 14081개\n",
      "2025-06-08 15:12:16,367 - INFO - 방문 빈도 계산 완료: 1392개 방문지, 최대 방문 8171회\n",
      "2025-06-08 15:12:16,385 - INFO - 타겟 생성 완료: 258개\n",
      "2025-06-08 15:12:16,386 - INFO - 타겟 점수 범위: 0.000 ~ 0.655\n",
      "2025-06-08 15:12:16,388 - INFO - 여행 컨텍스트 처리 시작...\n",
      "2025-06-08 15:12:16,393 - INFO - ✅ 데이터 준비 완료:\n",
      "2025-06-08 15:12:16,394 - INFO -   - 노드 수: 258\n",
      "2025-06-08 15:12:16,394 - INFO -   - 특성 수: 35\n",
      "2025-06-08 15:12:16,394 - INFO -   - 엣지 수: 1418\n",
      "2025-06-08 15:12:16,394 - INFO -   - 여행 컨텍스트: (3223, 21)\n",
      "2025-06-08 15:12:16,394 - INFO -   - ID 매핑 수: 258\n",
      "2025-06-08 15:12:16,395 - INFO -   - 타겟 수: 258\n",
      "2025-06-08 15:12:16,395 - INFO -   - 엣지 인덱스 범위: 0 ~ 257\n",
      "2025-06-08 15:12:16,395 - INFO - 🎯 모델 학습 시작...\n",
      "2025-06-08 15:12:16,395 - INFO - 모델 학습 시작...\n",
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]2025-06-08 15:12:16,433 - INFO - Epoch 000, Loss: 0.0903, Best: 0.0903, LR: 0.001000\n",
      "Training:  10%|█         | 20/200 [00:00<00:03, 48.72it/s]2025-06-08 15:12:16,862 - INFO - Epoch 020, Loss: 0.0163, Best: 0.0163, LR: 0.001000\n",
      "Training:  18%|█▊        | 37/200 [00:00<00:03, 50.54it/s]2025-06-08 15:12:17,253 - INFO - Epoch 040, Loss: 0.0127, Best: 0.0127, LR: 0.001000\n",
      "Training:  28%|██▊       | 55/200 [00:01<00:02, 50.81it/s]2025-06-08 15:12:17,644 - INFO - Epoch 060, Loss: 0.0110, Best: 0.0108, LR: 0.001000\n",
      "Training:  39%|███▉      | 78/200 [00:01<00:02, 48.40it/s]2025-06-08 15:12:18,075 - INFO - Epoch 080, Loss: 0.0092, Best: 0.0092, LR: 0.001000\n",
      "Training:  50%|█████     | 100/200 [00:02<00:02, 49.92it/s]2025-06-08 15:12:18,468 - INFO - Epoch 100, Loss: 0.0088, Best: 0.0074, LR: 0.001000\n",
      "Training:  56%|█████▌    | 112/200 [00:02<00:01, 51.33it/s]2025-06-08 15:12:18,774 - INFO - Epoch 116: Learning rate reduced from 0.001000 to 0.000500\n",
      "Training:  59%|█████▉    | 118/200 [00:02<00:01, 51.13it/s]2025-06-08 15:12:18,856 - INFO - Epoch 120, Loss: 0.0087, Best: 0.0071, LR: 0.000500\n",
      "Training:  68%|██████▊   | 136/200 [00:02<00:01, 53.32it/s]2025-06-08 15:12:19,223 - INFO - Epoch 140, Loss: 0.0071, Best: 0.0062, LR: 0.000500\n",
      "Training:  80%|████████  | 160/200 [00:03<00:00, 55.77it/s]2025-06-08 15:12:19,580 - INFO - Epoch 160: Learning rate reduced from 0.000500 to 0.000250\n",
      "2025-06-08 15:12:19,580 - INFO - Epoch 160, Loss: 0.0075, Best: 0.0062, LR: 0.000250\n",
      "Training:  90%|████████▉ | 179/200 [00:03<00:00, 56.02it/s]2025-06-08 15:12:19,941 - INFO - Epoch 180, Loss: 0.0063, Best: 0.0060, LR: 0.000250\n",
      "Training: 100%|██████████| 200/200 [00:03<00:00, 51.64it/s]\n",
      "2025-06-08 15:12:20,276 - INFO - 학습 완료! 최종 손실: 0.0059\n",
      "2025-06-08 15:12:20,276 - INFO - 💾 데이터 저장 중...\n",
      "2025-06-08 15:12:20,276 - INFO - 처리된 데이터 저장 시작...\n",
      "2025-06-08 15:12:20,296 - INFO - 데이터 저장 완료: ./pickle/동부권/\n",
      "2025-06-08 15:12:20,296 - INFO - 저장된 데이터 구조:\n",
      "2025-06-08 15:12:20,297 - INFO -   - visit_area_df: (37164, 25)\n",
      "2025-06-08 15:12:20,297 - INFO -   - graph_data: 노드 258개, 엣지 1418개\n",
      "2025-06-08 15:12:20,297 - INFO -   - visit_scaler: RobustScaler\n",
      "2025-06-08 15:12:20,297 - INFO -   - travel_scaler: StandardScaler\n",
      "2025-06-08 15:12:20,297 - INFO -   - id_to_index: 258개 매핑\n",
      "2025-06-08 15:12:20,298 - INFO -   - device: cpu\n",
      "2025-06-08 15:12:20,298 - INFO - \n",
      "============================================================\n",
      "2025-06-08 15:12:20,298 - INFO - ✅ 지역 '동부권' 학습 완료!\n",
      "2025-06-08 15:12:20,298 - INFO - 📁 모델 저장: ./models/동부권/\n",
      "2025-06-08 15:12:20,298 - INFO - 📁 데이터 저장: ./pickle/동부권/\n",
      "2025-06-08 15:12:20,299 - INFO - 📊 최종 손실: 0.0059\n",
      "2025-06-08 15:12:20,299 - INFO - 📊 총 에포크: 200\n",
      "2025-06-08 15:12:20,299 - INFO - 🎯 노드 수: 258, 엣지 수: 1418\n",
      "2025-06-08 15:12:20,299 - INFO - ============================================================\n",
      "2025-06-08 15:12:20,304 - INFO - ✅ 지역 '동부권' 처리 완료!\n",
      "2025-06-08 15:12:20,304 - INFO - \n",
      "🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-08 15:12:20,304 - INFO - 📍 [3/4] 지역 '제주도 및 도서지역' 처리 시작!\n",
      "2025-06-08 15:12:20,304 - INFO - 🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-08 15:12:20,304 - INFO - \n",
      "============================================================\n",
      "2025-06-08 15:12:20,305 - INFO - 🚀 지역 '제주도 및 도서지역' GNN 모델 학습 시작!\n",
      "2025-06-08 15:12:20,305 - INFO - ============================================================\n",
      "2025-06-08 15:12:20,305 - INFO - 📂 데이터 로드 중...\n",
      "2025-06-08 15:12:20,461 - INFO - 🔍 데이터 검증 중...\n",
      "2025-06-08 15:12:20,465 - INFO - ✅ 여행정보 데이터 로드: 1777개 레코드\n",
      "2025-06-08 15:12:20,465 - INFO - ✅ 데이터 로드 및 검증 완료:\n",
      "2025-06-08 15:12:20,465 - INFO -   - 이동내역: 38865개 레코드 (유효 이동: 38530개)\n",
      "2025-06-08 15:12:20,465 - INFO -   - 방문지: 38865개 레코드 (유효 방문지: 38865개)\n",
      "2025-06-08 15:12:20,466 - INFO - 🔧 데이터 전처리 중...\n",
      "2025-06-08 15:12:20,466 - INFO - 데이터 준비 시작...\n",
      "2025-06-08 15:12:20,466 - INFO - 엣지 생성 시작...\n",
      "2025-06-08 15:12:20,469 - INFO - 방문지 데이터에서 사용 가능한 ID: 9319개\n",
      "2025-06-08 15:12:20,472 - INFO - 최종 사용할 방문지 ID: 349개\n",
      "2025-06-08 15:12:20,473 - INFO - 방문지 ID 매핑 생성: 349개\n",
      "Processing travel groups: 100%|██████████| 1777/1777 [00:01<00:00, 922.56it/s]\n",
      "2025-06-08 15:12:22,413 - INFO - 엣지 생성 통계:\n",
      "2025-06-08 15:12:22,413 - INFO -   - 총 이동 시도: 37088개\n",
      "2025-06-08 15:12:22,413 - INFO -   - 유효한 엣지: 2413개\n",
      "2025-06-08 15:12:22,413 - INFO -   - TRIP_ID 누락: 512개\n",
      "2025-06-08 15:12:22,413 - INFO -   - 유효하지 않은 FROM_ID: 25712개\n",
      "2025-06-08 15:12:22,414 - INFO -   - 유효하지 않은 TO_ID: 6936개\n",
      "2025-06-08 15:12:22,418 - INFO - ✅ 엣지 생성 완료:\n",
      "2025-06-08 15:12:22,418 - INFO -   - 최종 엣지 수: 2413개\n",
      "2025-06-08 15:12:22,418 - INFO -   - 노드 인덱스 범위: 0 ~ 348\n",
      "2025-06-08 15:12:22,419 - INFO -   - 엣지 인덱스 최대값: 348\n",
      "2025-06-08 15:12:22,432 - INFO - NEW_VISIT_AREA_ID가 있는 방문지: 38865개\n",
      "2025-06-08 15:12:22,438 - INFO - ID 매핑에 있는 방문지: 12326개\n",
      "2025-06-08 15:12:22,438 - INFO - 중복된 방문지 ID 집계 중...\n",
      "2025-06-08 15:12:22,442 - INFO - 집계 후 유니크 방문지: 349개\n",
      "2025-06-08 15:12:22,447 - INFO - 최종 정렬된 방문지: 349개\n",
      "2025-06-08 15:12:22,452 - INFO - 방문지 특성 처리 완료: (349, 36)\n",
      "2025-06-08 15:12:22,456 - INFO - 학습 타겟 생성 시작...\n",
      "2025-06-08 15:12:22,464 - INFO - 중복된 방문지 타겟 정보 집계 중...\n",
      "2025-06-08 15:12:22,469 - INFO - 집계된 유니크 방문지: 9319개\n",
      "2025-06-08 15:12:22,472 - INFO - 방문 빈도 계산 완료: 1301개 방문지, 최대 방문 8126회\n",
      "2025-06-08 15:12:22,484 - INFO - 타겟 생성 완료: 349개\n",
      "2025-06-08 15:12:22,484 - INFO - 타겟 점수 범위: 0.000 ~ 0.649\n",
      "2025-06-08 15:12:22,487 - INFO - 여행 컨텍스트 처리 시작...\n",
      "2025-06-08 15:12:22,491 - INFO - ✅ 데이터 준비 완료:\n",
      "2025-06-08 15:12:22,492 - INFO -   - 노드 수: 349\n",
      "2025-06-08 15:12:22,492 - INFO -   - 특성 수: 36\n",
      "2025-06-08 15:12:22,492 - INFO -   - 엣지 수: 2413\n",
      "2025-06-08 15:12:22,492 - INFO -   - 여행 컨텍스트: (1777, 21)\n",
      "2025-06-08 15:12:22,493 - INFO -   - ID 매핑 수: 349\n",
      "2025-06-08 15:12:22,493 - INFO -   - 타겟 수: 349\n",
      "2025-06-08 15:12:22,493 - INFO -   - 엣지 인덱스 범위: 0 ~ 348\n",
      "2025-06-08 15:12:22,493 - INFO - 🎯 모델 학습 시작...\n",
      "2025-06-08 15:12:22,493 - INFO - 모델 학습 시작...\n",
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]2025-06-08 15:12:22,542 - INFO - Epoch 000, Loss: 0.0628, Best: 0.0628, LR: 0.001000\n",
      "Training:  10%|█         | 20/200 [00:00<00:05, 33.01it/s]2025-06-08 15:12:23,148 - INFO - Epoch 020, Loss: 0.0094, Best: 0.0094, LR: 0.001000\n",
      "Training:  19%|█▉        | 38/200 [00:01<00:04, 39.75it/s]2025-06-08 15:12:23,630 - INFO - Epoch 040, Loss: 0.0080, Best: 0.0080, LR: 0.001000\n",
      "Training:  28%|██▊       | 57/200 [00:01<00:03, 39.72it/s]2025-06-08 15:12:24,152 - INFO - Epoch 060, Loss: 0.0066, Best: 0.0062, LR: 0.001000\n",
      "Training:  40%|███▉      | 79/200 [00:02<00:03, 39.67it/s]2025-06-08 15:12:24,654 - INFO - Epoch 080, Loss: 0.0049, Best: 0.0049, LR: 0.001000\n",
      "Training:  49%|████▉     | 98/200 [00:02<00:02, 40.85it/s]2025-06-08 15:12:25,129 - INFO - Epoch 100, Loss: 0.0057, Best: 0.0046, LR: 0.001000\n",
      "Training:  59%|█████▉    | 118/200 [00:03<00:01, 42.47it/s]2025-06-08 15:12:25,592 - INFO - Epoch 120, Loss: 0.0050, Best: 0.0044, LR: 0.001000\n",
      "Training:  69%|██████▉   | 138/200 [00:03<00:01, 41.75it/s]2025-06-08 15:12:26,078 - INFO - Epoch 140, Loss: 0.0043, Best: 0.0036, LR: 0.001000\n",
      "Training:  79%|███████▉  | 158/200 [00:03<00:00, 42.82it/s]2025-06-08 15:12:26,545 - INFO - Epoch 160, Loss: 0.0035, Best: 0.0032, LR: 0.001000\n",
      "Training:  84%|████████▍ | 168/200 [00:04<00:00, 42.51it/s]2025-06-08 15:12:26,741 - INFO - Epoch 168: Learning rate reduced from 0.001000 to 0.000500\n",
      "Training:  89%|████████▉ | 178/200 [00:04<00:00, 42.78it/s]2025-06-08 15:12:27,024 - INFO - Epoch 180, Loss: 0.0038, Best: 0.0031, LR: 0.000500\n",
      "Training: 100%|██████████| 200/200 [00:04<00:00, 40.28it/s]\n",
      "2025-06-08 15:12:27,466 - INFO - 학습 완료! 최종 손실: 0.0030\n",
      "2025-06-08 15:12:27,467 - INFO - 💾 데이터 저장 중...\n",
      "2025-06-08 15:12:27,467 - INFO - 처리된 데이터 저장 시작...\n",
      "2025-06-08 15:12:27,488 - INFO - 데이터 저장 완료: ./pickle/제주도 및 도서지역/\n",
      "2025-06-08 15:12:27,489 - INFO - 저장된 데이터 구조:\n",
      "2025-06-08 15:12:27,489 - INFO -   - visit_area_df: (38865, 25)\n",
      "2025-06-08 15:12:27,489 - INFO -   - graph_data: 노드 349개, 엣지 2413개\n",
      "2025-06-08 15:12:27,490 - INFO -   - visit_scaler: RobustScaler\n",
      "2025-06-08 15:12:27,490 - INFO -   - travel_scaler: StandardScaler\n",
      "2025-06-08 15:12:27,490 - INFO -   - id_to_index: 349개 매핑\n",
      "2025-06-08 15:12:27,490 - INFO -   - device: cpu\n",
      "2025-06-08 15:12:27,491 - INFO - \n",
      "============================================================\n",
      "2025-06-08 15:12:27,491 - INFO - ✅ 지역 '제주도 및 도서지역' 학습 완료!\n",
      "2025-06-08 15:12:27,491 - INFO - 📁 모델 저장: ./models/제주도 및 도서지역/\n",
      "2025-06-08 15:12:27,492 - INFO - 📁 데이터 저장: ./pickle/제주도 및 도서지역/\n",
      "2025-06-08 15:12:27,492 - INFO - 📊 최종 손실: 0.0030\n",
      "2025-06-08 15:12:27,492 - INFO - 📊 총 에포크: 200\n",
      "2025-06-08 15:12:27,492 - INFO - 🎯 노드 수: 349, 엣지 수: 2413\n",
      "2025-06-08 15:12:27,492 - INFO - ============================================================\n",
      "2025-06-08 15:12:27,497 - INFO - ✅ 지역 '제주도 및 도서지역' 처리 완료!\n",
      "2025-06-08 15:12:27,497 - INFO - \n",
      "🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-08 15:12:27,498 - INFO - 📍 [4/4] 지역 '수도권' 처리 시작!\n",
      "2025-06-08 15:12:27,498 - INFO - 🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-08 15:12:27,498 - INFO - \n",
      "============================================================\n",
      "2025-06-08 15:12:27,498 - INFO - 🚀 지역 '수도권' GNN 모델 학습 시작!\n",
      "2025-06-08 15:12:27,498 - INFO - ============================================================\n",
      "2025-06-08 15:12:27,499 - INFO - 📂 데이터 로드 중...\n",
      "2025-06-08 15:12:27,612 - INFO - 🔍 데이터 검증 중...\n",
      "2025-06-08 15:12:27,619 - INFO - ✅ 여행정보 데이터 로드: 3298개 레코드\n",
      "2025-06-08 15:12:27,619 - INFO - ✅ 데이터 로드 및 검증 완료:\n",
      "2025-06-08 15:12:27,620 - INFO -   - 이동내역: 28300개 레코드 (유효 이동: 28050개)\n",
      "2025-06-08 15:12:27,620 - INFO -   - 방문지: 28300개 레코드 (유효 방문지: 28300개)\n",
      "2025-06-08 15:12:27,620 - INFO - 🔧 데이터 전처리 중...\n",
      "2025-06-08 15:12:27,620 - INFO - 데이터 준비 시작...\n",
      "2025-06-08 15:12:27,621 - INFO - 엣지 생성 시작...\n",
      "2025-06-08 15:12:27,622 - INFO - 방문지 데이터에서 사용 가능한 ID: 12428개\n",
      "2025-06-08 15:12:27,629 - INFO - 최종 사용할 방문지 ID: 1057개\n",
      "2025-06-08 15:12:27,629 - INFO - 방문지 ID 매핑 생성: 1057개\n",
      "Processing travel groups: 100%|██████████| 3298/3298 [00:01<00:00, 2142.59it/s]\n",
      "2025-06-08 15:12:29,189 - INFO - 엣지 생성 통계:\n",
      "2025-06-08 15:12:29,190 - INFO -   - 총 이동 시도: 25002개\n",
      "2025-06-08 15:12:29,190 - INFO -   - 유효한 엣지: 22930개\n",
      "2025-06-08 15:12:29,190 - INFO -   - TRIP_ID 누락: 331개\n",
      "2025-06-08 15:12:29,190 - INFO -   - 유효하지 않은 FROM_ID: 155개\n",
      "2025-06-08 15:12:29,191 - INFO -   - 유효하지 않은 TO_ID: 101개\n",
      "2025-06-08 15:12:29,216 - INFO - ✅ 엣지 생성 완료:\n",
      "2025-06-08 15:12:29,217 - INFO -   - 최종 엣지 수: 22930개\n",
      "2025-06-08 15:12:29,217 - INFO -   - 노드 인덱스 범위: 0 ~ 1056\n",
      "2025-06-08 15:12:29,218 - INFO -   - 엣지 인덱스 최대값: 1056\n",
      "2025-06-08 15:12:29,229 - INFO - NEW_VISIT_AREA_ID가 있는 방문지: 28300개\n",
      "2025-06-08 15:12:29,234 - INFO - ID 매핑에 있는 방문지: 11272개\n",
      "2025-06-08 15:12:29,234 - INFO - 중복된 방문지 ID 집계 중...\n",
      "2025-06-08 15:12:29,237 - INFO - 집계 후 유니크 방문지: 1057개\n",
      "2025-06-08 15:12:29,253 - INFO - 최종 정렬된 방문지: 1057개\n",
      "2025-06-08 15:12:29,261 - INFO - 방문지 특성 처리 완료: (1057, 36)\n",
      "2025-06-08 15:12:29,267 - INFO - 학습 타겟 생성 시작...\n",
      "2025-06-08 15:12:29,272 - INFO - 중복된 방문지 타겟 정보 집계 중...\n",
      "2025-06-08 15:12:29,279 - INFO - 집계된 유니크 방문지: 12428개\n",
      "2025-06-08 15:12:29,282 - INFO - 방문 빈도 계산 완료: 1118개 방문지, 최대 방문 6591회\n",
      "2025-06-08 15:12:29,300 - INFO - 타겟 생성 완료: 1057개\n",
      "2025-06-08 15:12:29,300 - INFO - 타겟 점수 범위: 0.000 ~ 0.644\n",
      "2025-06-08 15:12:29,303 - INFO - 여행 컨텍스트 처리 시작...\n",
      "2025-06-08 15:12:29,308 - INFO - ✅ 데이터 준비 완료:\n",
      "2025-06-08 15:12:29,308 - INFO -   - 노드 수: 1057\n",
      "2025-06-08 15:12:29,308 - INFO -   - 특성 수: 36\n",
      "2025-06-08 15:12:29,309 - INFO -   - 엣지 수: 22930\n",
      "2025-06-08 15:12:29,309 - INFO -   - 여행 컨텍스트: (3298, 21)\n",
      "2025-06-08 15:12:29,309 - INFO -   - ID 매핑 수: 1057\n",
      "2025-06-08 15:12:29,309 - INFO -   - 타겟 수: 1057\n",
      "2025-06-08 15:12:29,310 - INFO -   - 엣지 인덱스 범위: 0 ~ 1056\n",
      "2025-06-08 15:12:29,310 - INFO - 🎯 모델 학습 시작...\n",
      "2025-06-08 15:12:29,310 - INFO - 모델 학습 시작...\n",
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]2025-06-08 15:12:29,495 - INFO - Epoch 000, Loss: 0.0692, Best: 0.0692, LR: 0.001000\n",
      "Training:  10%|▉         | 19/200 [00:02<00:18,  9.59it/s]2025-06-08 15:12:31,548 - INFO - Epoch 020, Loss: 0.0186, Best: 0.0169, LR: 0.001000\n",
      "Training:  20%|██        | 40/200 [00:04<00:15, 10.24it/s]2025-06-08 15:12:33,563 - INFO - Epoch 040, Loss: 0.0156, Best: 0.0156, LR: 0.001000\n",
      "Training:  30%|███       | 60/200 [00:06<00:13, 10.09it/s]2025-06-08 15:12:35,579 - INFO - Epoch 060, Loss: 0.0153, Best: 0.0146, LR: 0.001000\n",
      "Training:  40%|███▉      | 79/200 [00:08<00:11, 10.19it/s]2025-06-08 15:12:37,603 - INFO - Epoch 080, Loss: 0.0135, Best: 0.0135, LR: 0.001000\n",
      "Training:  50%|█████     | 100/200 [00:10<00:11,  8.88it/s]2025-06-08 15:12:39,708 - INFO - Epoch 100, Loss: 0.0129, Best: 0.0127, LR: 0.001000\n",
      "Training:  60%|██████    | 120/200 [00:12<00:08,  9.26it/s]2025-06-08 15:12:41,871 - INFO - Epoch 120, Loss: 0.0122, Best: 0.0113, LR: 0.001000\n",
      "Training:  68%|██████▊   | 135/200 [00:14<00:06,  9.68it/s]2025-06-08 15:12:43,466 - INFO - Epoch 135: Learning rate reduced from 0.001000 to 0.000500\n",
      "Training:  70%|███████   | 140/200 [00:14<00:06,  9.77it/s]2025-06-08 15:12:43,990 - INFO - Epoch 140, Loss: 0.0119, Best: 0.0110, LR: 0.000500\n",
      "Training:  80%|████████  | 160/200 [00:16<00:04,  9.62it/s]2025-06-08 15:12:46,069 - INFO - Epoch 160, Loss: 0.0113, Best: 0.0108, LR: 0.000500\n",
      "Training:  90%|█████████ | 180/200 [00:18<00:02,  8.67it/s]2025-06-08 15:12:48,185 - INFO - Epoch 180, Loss: 0.0109, Best: 0.0105, LR: 0.000500\n",
      "Training: 100%|██████████| 200/200 [00:20<00:00,  9.61it/s]\n",
      "2025-06-08 15:12:50,123 - INFO - 학습 완료! 최종 손실: 0.0101\n",
      "2025-06-08 15:12:50,124 - INFO - 💾 데이터 저장 중...\n",
      "2025-06-08 15:12:50,124 - INFO - 처리된 데이터 저장 시작...\n",
      "2025-06-08 15:12:50,149 - INFO - 데이터 저장 완료: ./pickle/수도권/\n",
      "2025-06-08 15:12:50,149 - INFO - 저장된 데이터 구조:\n",
      "2025-06-08 15:12:50,149 - INFO -   - visit_area_df: (28300, 25)\n",
      "2025-06-08 15:12:50,150 - INFO -   - graph_data: 노드 1057개, 엣지 22930개\n",
      "2025-06-08 15:12:50,150 - INFO -   - visit_scaler: RobustScaler\n",
      "2025-06-08 15:12:50,150 - INFO -   - travel_scaler: StandardScaler\n",
      "2025-06-08 15:12:50,150 - INFO -   - id_to_index: 1057개 매핑\n",
      "2025-06-08 15:12:50,150 - INFO -   - device: cpu\n",
      "2025-06-08 15:12:50,150 - INFO - \n",
      "============================================================\n",
      "2025-06-08 15:12:50,151 - INFO - ✅ 지역 '수도권' 학습 완료!\n",
      "2025-06-08 15:12:50,151 - INFO - 📁 모델 저장: ./models/수도권/\n",
      "2025-06-08 15:12:50,151 - INFO - 📁 데이터 저장: ./pickle/수도권/\n",
      "2025-06-08 15:12:50,151 - INFO - 📊 최종 손실: 0.0101\n",
      "2025-06-08 15:12:50,151 - INFO - 📊 총 에포크: 200\n",
      "2025-06-08 15:12:50,152 - INFO - 🎯 노드 수: 1057, 엣지 수: 22930\n",
      "2025-06-08 15:12:50,152 - INFO - ============================================================\n",
      "2025-06-08 15:12:50,158 - INFO - ✅ 지역 '수도권' 처리 완료!\n",
      "2025-06-08 15:12:50,158 - INFO - \n",
      "🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁\n",
      "2025-06-08 15:12:50,158 - INFO - 🏁 전체 지역 처리 완료!\n",
      "2025-06-08 15:12:50,158 - INFO - 🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁\n",
      "2025-06-08 15:12:50,159 - INFO - ✅ 성공한 지역 (4개): ['서부권', '동부권', '제주도 및 도서지역', '수도권']\n",
      "2025-06-08 15:12:50,159 - INFO - \n",
      "📂 저장된 파일 구조:\n",
      "2025-06-08 15:12:50,159 - INFO -   📁 서부권/\n",
      "2025-06-08 15:12:50,159 - INFO -     🤖 models/서부권/improved_travel_recommendation_model.pt\n",
      "2025-06-08 15:12:50,159 - INFO -     💾 pickle/서부권/improved_travel_data.pkl\n",
      "2025-06-08 15:12:50,160 - INFO -   📁 동부권/\n",
      "2025-06-08 15:12:50,160 - INFO -     🤖 models/동부권/improved_travel_recommendation_model.pt\n",
      "2025-06-08 15:12:50,160 - INFO -     💾 pickle/동부권/improved_travel_data.pkl\n",
      "2025-06-08 15:12:50,160 - INFO -   📁 제주도 및 도서지역/\n",
      "2025-06-08 15:12:50,160 - INFO -     🤖 models/제주도 및 도서지역/improved_travel_recommendation_model.pt\n",
      "2025-06-08 15:12:50,161 - INFO -     💾 pickle/제주도 및 도서지역/improved_travel_data.pkl\n",
      "2025-06-08 15:12:50,161 - INFO -   📁 수도권/\n",
      "2025-06-08 15:12:50,161 - INFO -     🤖 models/수도권/improved_travel_recommendation_model.pt\n",
      "2025-06-08 15:12:50,161 - INFO -     💾 pickle/수도권/improved_travel_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import HeteroData\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnhancedDataProcessor:\n",
    "    \"\"\"데이터 전처리를 위한 클래스\"\"\"\n",
    "    def __init__(self):\n",
    "        self.visit_scaler = RobustScaler()\n",
    "        self.travel_scaler = StandardScaler()\n",
    "        # 제외할 키워드 목록\n",
    "        self.exclude_keywords = {\n",
    "            '역', '터미널', '공항', '휴게소', '정류장', '톨게이트', '교차로', '출구', '입구',\n",
    "            'IC', 'JC', '나들목', '분기점', '요금소', '주차장', '주유소', '충전소',\n",
    "            '아파트', '원룸', '오피스텔', '빌라', '주택', '빌딩', '상가', '모텔', '집', \n",
    "            '교직원', '하나로마트', '마트'\n",
    "        }\n",
    "        \n",
    "    def should_exclude_location(self, name):\n",
    "        \"\"\"위치를 제외해야 하는지 확인\"\"\"\n",
    "        if pd.isna(name):\n",
    "            return False\n",
    "        name_str = str(name).lower()\n",
    "        \n",
    "        for keyword in self.exclude_keywords:\n",
    "            if keyword.lower() in name_str:\n",
    "                # 예외 처리: 관광지로서의 역할이 있는 경우\n",
    "                tourist_keywords = {'관광', '테마', '파크', '랜드', '월드', '호텔',\n",
    "                                  '맛집', '식당', '카페', '박물관', '전시', '갤러리', '문화'}\n",
    "                if any(tk in name_str for tk in tourist_keywords) and keyword != '아파트':\n",
    "                    continue\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def process_visit_area_features(self, visit_area_df, id_to_index=None):\n",
    "        \"\"\"방문지 특성 처리 - 중복 ID 처리 포함\"\"\"\n",
    "        visit_area_df = visit_area_df.copy()\n",
    "        \n",
    "        # 1. 유효한 NEW_VISIT_AREA_ID만 필터링\n",
    "        if 'NEW_VISIT_AREA_ID' in visit_area_df.columns:\n",
    "            valid_rows = visit_area_df['NEW_VISIT_AREA_ID'].notna()\n",
    "            visit_area_df = visit_area_df[valid_rows].copy()\n",
    "            logger.info(f\"NEW_VISIT_AREA_ID가 있는 방문지: {len(visit_area_df)}개\")\n",
    "        else:\n",
    "            logger.error(\"NEW_VISIT_AREA_ID 컬럼이 존재하지 않습니다!\")\n",
    "            raise ValueError(\"NEW_VISIT_AREA_ID 컬럼이 필요합니다.\")\n",
    "        \n",
    "        # 2. ID 매핑이 제공된 경우, 해당 ID만 필터링하고 중복 처리\n",
    "        if id_to_index is not None:\n",
    "            visit_area_df['NEW_VISIT_AREA_ID'] = visit_area_df['NEW_VISIT_AREA_ID'].astype(int)\n",
    "            \n",
    "            # id_to_index에 있는 ID만 필터링\n",
    "            valid_ids = set(id_to_index.keys())\n",
    "            mask = visit_area_df['NEW_VISIT_AREA_ID'].isin(valid_ids)\n",
    "            visit_area_df = visit_area_df[mask].copy()\n",
    "            \n",
    "            logger.info(f\"ID 매핑에 있는 방문지: {len(visit_area_df)}개\")\n",
    "            \n",
    "            # 🔧 중복 처리: 각 NEW_VISIT_AREA_ID별로 집계\n",
    "            logger.info(\"중복된 방문지 ID 집계 중...\")\n",
    "            \n",
    "            # 숫자형 컬럼들의 평균 계산\n",
    "            numeric_cols = ['X_COORD', 'Y_COORD', 'DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']\n",
    "            agg_dict = {}\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                if col in visit_area_df.columns:\n",
    "                    agg_dict[col] = 'mean'  # 평균값 사용\n",
    "            \n",
    "            # 범주형 컬럼들의 최빈값 또는 첫 번째 값 사용\n",
    "            categorical_cols = ['VISIT_AREA_NM', 'VISIT_AREA_TYPE_CD', 'VISIT_CHC_REASON_CD']\n",
    "            for col in categorical_cols:\n",
    "                if col in visit_area_df.columns:\n",
    "                    agg_dict[col] = 'first'  # 첫 번째 값 사용\n",
    "            \n",
    "            # 집계 수행\n",
    "            aggregated_df = visit_area_df.groupby('NEW_VISIT_AREA_ID').agg(agg_dict).reset_index()\n",
    "            \n",
    "            logger.info(f\"집계 후 유니크 방문지: {len(aggregated_df)}개\")\n",
    "            \n",
    "            # id_to_index의 순서대로 정렬\n",
    "            ordered_ids = sorted(id_to_index.keys())\n",
    "            existing_ids = [id for id in ordered_ids if id in aggregated_df['NEW_VISIT_AREA_ID'].values]\n",
    "            \n",
    "            # 정렬된 순서로 재배열\n",
    "            id_to_idx_map = {id: i for i, id in enumerate(aggregated_df['NEW_VISIT_AREA_ID'])}\n",
    "            aggregated_df['sort_order'] = aggregated_df['NEW_VISIT_AREA_ID'].map(\n",
    "                lambda x: existing_ids.index(x) if x in existing_ids else 999999\n",
    "            )\n",
    "            aggregated_df = aggregated_df.sort_values('sort_order').drop('sort_order', axis=1).reset_index(drop=True)\n",
    "            \n",
    "            visit_area_df = aggregated_df\n",
    "            logger.info(f\"최종 정렬된 방문지: {len(visit_area_df)}개\")\n",
    "        \n",
    "        # 3. 기본 컬럼 존재 여부 확인 및 결측치 처리\n",
    "        required_cols = ['X_COORD', 'Y_COORD', 'VISIT_AREA_NM']\n",
    "        for col in required_cols:\n",
    "            if col not in visit_area_df.columns:\n",
    "                logger.warning(f\"필수 컬럼 {col}이 없습니다. 기본값으로 대체합니다.\")\n",
    "                if col in ['X_COORD', 'Y_COORD']:\n",
    "                    visit_area_df[col] = 0.0\n",
    "                else:\n",
    "                    visit_area_df[col] = '알 수 없음'\n",
    "        \n",
    "        # 좌표 결측치 처리\n",
    "        visit_area_df['X_COORD'] = pd.to_numeric(visit_area_df['X_COORD'], errors='coerce')\n",
    "        visit_area_df['Y_COORD'] = pd.to_numeric(visit_area_df['Y_COORD'], errors='coerce')\n",
    "        visit_area_df['X_COORD'] = visit_area_df['X_COORD'].fillna(visit_area_df['X_COORD'].mean())\n",
    "        visit_area_df['Y_COORD'] = visit_area_df['Y_COORD'].fillna(visit_area_df['Y_COORD'].mean())\n",
    "        \n",
    "        # VISIT_CHC_REASON_CD 처리\n",
    "        if 'VISIT_CHC_REASON_CD' in visit_area_df.columns:\n",
    "            visit_area_df['VISIT_CHC_REASON_CD'] = pd.to_numeric(visit_area_df['VISIT_CHC_REASON_CD'], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            visit_area_df['VISIT_CHC_REASON_CD'] = 0\n",
    "        \n",
    "        features = visit_area_df[['X_COORD', 'Y_COORD']].copy()\n",
    "        \n",
    "        # One-hot encoding (안전하게 처리)\n",
    "        if 'VISIT_AREA_TYPE_CD' in visit_area_df.columns:\n",
    "            type_onehot = pd.get_dummies(visit_area_df['VISIT_AREA_TYPE_CD'], prefix='type')\n",
    "        else:\n",
    "            type_onehot = pd.DataFrame({'type_unknown': [1] * len(visit_area_df)})\n",
    "            \n",
    "        reason_onehot = pd.get_dummies(visit_area_df['VISIT_CHC_REASON_CD'], prefix='reason')\n",
    "        \n",
    "        # 만족도 점수 처리 (안전하게)\n",
    "        satisfaction_cols = ['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']\n",
    "        for col in satisfaction_cols:\n",
    "            if col in visit_area_df.columns:\n",
    "                visit_area_df[col] = pd.to_numeric(visit_area_df[col], errors='coerce').fillna(3)\n",
    "            else:\n",
    "                visit_area_df[col] = 3  # 기본값\n",
    "            visit_area_df[f'{col}_norm'] = (visit_area_df[col] - 1) / 4.0\n",
    "        \n",
    "        # 인기도 점수\n",
    "        visit_area_df['popularity_score'] = (\n",
    "            visit_area_df['DGSTFN_norm'] * 0.4 + \n",
    "            visit_area_df['REVISIT_INTENTION_norm'] * 0.3 + \n",
    "            visit_area_df['RCMDTN_INTENTION_norm'] * 0.3\n",
    "        )\n",
    "        \n",
    "        # 제외할 장소에 대한 페널티 추가\n",
    "        exclude_penalty = visit_area_df['VISIT_AREA_NM'].apply(self.should_exclude_location).astype(float) * -0.5\n",
    "        \n",
    "        # 모든 특성 결합\n",
    "        features = pd.concat([\n",
    "            features, type_onehot, reason_onehot,\n",
    "            visit_area_df[['DGSTFN_norm', 'REVISIT_INTENTION_norm', 'RCMDTN_INTENTION_norm', 'popularity_score']],\n",
    "            pd.DataFrame({'exclude_penalty': exclude_penalty})\n",
    "        ], axis=1)\n",
    "        \n",
    "        logger.info(f\"방문지 특성 처리 완료: {features.shape}\")\n",
    "        return self.visit_scaler.fit_transform(features.values.astype(np.float32))\n",
    "    \n",
    "    def create_enhanced_edges(self, move_df, visit_area_df):\n",
    "        \"\"\"향상된 엣지 생성 - 방문지 데이터에 존재하는 ID만 사용\"\"\"\n",
    "        edges = []\n",
    "        edge_weights = []\n",
    "        \n",
    "        logger.info(\"엣지 생성 시작...\")\n",
    "        \n",
    "        # 1. 방문지 데이터에서 실제 존재하는 유효한 NEW_VISIT_AREA_ID만 수집\n",
    "        available_visit_ids = set()\n",
    "        if 'NEW_VISIT_AREA_ID' in visit_area_df.columns:\n",
    "            available_from_visit = visit_area_df['NEW_VISIT_AREA_ID'].dropna().astype(int)\n",
    "            available_visit_ids.update(available_from_visit)\n",
    "        \n",
    "        logger.info(f\"방문지 데이터에서 사용 가능한 ID: {len(available_visit_ids)}개\")\n",
    "        \n",
    "        # 2. 이동 데이터에서 유효한 ID 수집 (단, 방문지 데이터에 존재하는 것만)\n",
    "        valid_visit_ids = set()\n",
    "        \n",
    "        # START_VISIT_AREA_ID 확인\n",
    "        if 'NEW_START_VISIT_AREA_ID' in move_df.columns:\n",
    "            start_ids = move_df['NEW_START_VISIT_AREA_ID'].dropna().astype(int)\n",
    "            # 방문지 데이터에 존재하는 것만 추가\n",
    "            valid_start_ids = start_ids[start_ids.isin(available_visit_ids)]\n",
    "            valid_visit_ids.update(valid_start_ids)\n",
    "        \n",
    "        # END_VISIT_AREA_ID 확인\n",
    "        if 'NEW_END_VISIT_AREA_ID' in move_df.columns:\n",
    "            end_ids = move_df['NEW_END_VISIT_AREA_ID'].dropna().astype(int)\n",
    "            # 방문지 데이터에 존재하는 것만 추가\n",
    "            valid_end_ids = end_ids[end_ids.isin(available_visit_ids)]\n",
    "            valid_visit_ids.update(valid_end_ids)\n",
    "        \n",
    "        # 3. 최종 유효 ID 목록 (방문지 데이터와 이동 데이터 모두에 존재하는 것)\n",
    "        final_valid_ids = available_visit_ids.intersection(valid_visit_ids)\n",
    "        final_valid_ids = sorted(list(final_valid_ids))\n",
    "        id_to_index = {visit_id: idx for idx, visit_id in enumerate(final_valid_ids)}\n",
    "        \n",
    "        logger.info(f\"최종 사용할 방문지 ID: {len(final_valid_ids)}개\")\n",
    "        logger.info(f\"방문지 ID 매핑 생성: {len(id_to_index)}개\")\n",
    "        \n",
    "        # 4. 통계 추적 변수\n",
    "        total_moves = 0\n",
    "        valid_edges = 0\n",
    "        invalid_from_id = 0\n",
    "        invalid_to_id = 0\n",
    "        missing_trip_id = 0\n",
    "        \n",
    "        # 5. 엣지 생성\n",
    "        for travel_id, group in tqdm(move_df.groupby(\"TRAVEL_ID\"), desc=\"Processing travel groups\"):\n",
    "            group = group.sort_values(\"TRIP_ID\").reset_index(drop=True)\n",
    "            \n",
    "            for i in range(1, len(group)):\n",
    "                total_moves += 1\n",
    "                \n",
    "                # 이전 행과 현재 행에서 방문지 ID 추출\n",
    "                prev_row = group.loc[i-1]\n",
    "                curr_row = group.loc[i]\n",
    "                \n",
    "                # 다양한 방법으로 FROM/TO ID 추출 시도\n",
    "                from_id = None\n",
    "                to_id = None\n",
    "                \n",
    "                # 방법 1: NEW_END_VISIT_AREA_ID 사용 (가장 우선)\n",
    "                if pd.notna(prev_row.get('NEW_END_VISIT_AREA_ID')):\n",
    "                    from_id = int(prev_row['NEW_END_VISIT_AREA_ID'])\n",
    "                elif pd.notna(prev_row.get('NEW_START_VISIT_AREA_ID')):\n",
    "                    from_id = int(prev_row['NEW_START_VISIT_AREA_ID'])\n",
    "                \n",
    "                if pd.notna(curr_row.get('NEW_END_VISIT_AREA_ID')):\n",
    "                    to_id = int(curr_row['NEW_END_VISIT_AREA_ID'])\n",
    "                elif pd.notna(curr_row.get('NEW_START_VISIT_AREA_ID')):\n",
    "                    to_id = int(curr_row['NEW_START_VISIT_AREA_ID'])\n",
    "                \n",
    "                # ID 유효성 검사 및 인덱스 변환\n",
    "                if from_id is None or to_id is None:\n",
    "                    missing_trip_id += 1\n",
    "                    continue\n",
    "                \n",
    "                if from_id not in id_to_index:\n",
    "                    invalid_from_id += 1\n",
    "                    continue\n",
    "                    \n",
    "                if to_id not in id_to_index:\n",
    "                    invalid_to_id += 1\n",
    "                    continue\n",
    "                \n",
    "                # 유효한 엣지인 경우 추가\n",
    "                from_idx = id_to_index[from_id]\n",
    "                to_idx = id_to_index[to_id]\n",
    "                \n",
    "                # 같은 노드로의 self-loop 제외\n",
    "                if from_idx == to_idx:\n",
    "                    continue\n",
    "                \n",
    "                duration = curr_row.get(\"DURATION_MINUTES\", 0) if \"DURATION_MINUTES\" in curr_row else 0\n",
    "                transport = curr_row.get(\"MVMN_CD_1\", 0) if \"MVMN_CD_1\" in curr_row else 0\n",
    "                \n",
    "                edges.append([from_idx, to_idx, duration, transport])\n",
    "                edge_weights.append(1.0)\n",
    "                valid_edges += 1\n",
    "        \n",
    "        # 6. 통계 출력\n",
    "        logger.info(f\"엣지 생성 통계:\")\n",
    "        logger.info(f\"  - 총 이동 시도: {total_moves}개\")\n",
    "        logger.info(f\"  - 유효한 엣지: {valid_edges}개\")\n",
    "        logger.info(f\"  - TRIP_ID 누락: {missing_trip_id}개\")\n",
    "        logger.info(f\"  - 유효하지 않은 FROM_ID: {invalid_from_id}개\")\n",
    "        logger.info(f\"  - 유효하지 않은 TO_ID: {invalid_to_id}개\")\n",
    "        \n",
    "        if not edges:\n",
    "            logger.warning(\"❌ 유효한 엣지가 생성되지 않았습니다!\")\n",
    "            return None, None, None\n",
    "            \n",
    "        edges_df = pd.DataFrame(edges, columns=[\"FROM_ID\", \"TO_ID\", \"DURATION_MINUTES\", \"MVMN_CD_1\"])\n",
    "        \n",
    "        # 교통수단 분류\n",
    "        edges_df[\"MVMN_TYPE\"] = edges_df[\"MVMN_CD_1\"].apply(\n",
    "            lambda code: \"drive\" if code in [1,2,3] else \"public\" if code in [4,5,6,7,8,9,10,11,12,13,50] else \"other\"\n",
    "        )\n",
    "        edges_df[\"is_drive\"] = (edges_df[\"MVMN_TYPE\"] == \"drive\").astype(int)\n",
    "        edges_df[\"is_public\"] = (edges_df[\"MVMN_TYPE\"] == \"public\").astype(int)\n",
    "        edges_df[\"is_other\"] = (edges_df[\"MVMN_TYPE\"] == \"other\").astype(int)\n",
    "        \n",
    "        # 엣지 인덱스와 속성 생성\n",
    "        edge_index = torch.tensor(edges_df[[\"FROM_ID\", \"TO_ID\"]].values.T, dtype=torch.long)\n",
    "        edge_attr = torch.tensor(np.column_stack([\n",
    "            edges_df[[\"DURATION_MINUTES\"]].fillna(0).values,\n",
    "            edges_df[[\"is_drive\", \"is_public\", \"is_other\"]].values,\n",
    "            np.array(edge_weights).reshape(-1, 1)\n",
    "        ]), dtype=torch.float32)\n",
    "        \n",
    "        logger.info(f\"✅ 엣지 생성 완료:\")\n",
    "        logger.info(f\"  - 최종 엣지 수: {len(edges)}개\")\n",
    "        logger.info(f\"  - 노드 인덱스 범위: 0 ~ {len(id_to_index)-1}\")\n",
    "        logger.info(f\"  - 엣지 인덱스 최대값: {edge_index.max().item()}\")\n",
    "        \n",
    "        return edge_index, edge_attr, id_to_index\n",
    "\n",
    "    def process_travel_context(self, travel_df):\n",
    "        \"\"\"여행 컨텍스트 처리\"\"\"\n",
    "        logger.info(\"여행 컨텍스트 처리 시작...\")\n",
    "        \n",
    "        # 필요한 컬럼들\n",
    "        travel_feature_cols = [\n",
    "            'TOTAL_COST_BINNED_ENCODED', 'WITH_PET', 'MONTH', 'DURATION',\n",
    "            'MVMN_기타', 'MVMN_대중교통', 'MVMN_자가용',\n",
    "            'TRAVEL_PURPOSE_1', 'TRAVEL_PURPOSE_2', 'TRAVEL_PURPOSE_3',\n",
    "            'TRAVEL_PURPOSE_4', 'TRAVEL_PURPOSE_5', 'TRAVEL_PURPOSE_6',\n",
    "            'TRAVEL_PURPOSE_7', 'TRAVEL_PURPOSE_8', 'TRAVEL_PURPOSE_9',\n",
    "            'WHOWITH_2인여행', 'WHOWITH_가족여행', 'WHOWITH_기타',\n",
    "            'WHOWITH_단독여행', 'WHOWITH_친구/지인 여행'\n",
    "        ]\n",
    "        \n",
    "        # 누락된 컬럼들을 0으로 초기화\n",
    "        for col in travel_feature_cols:\n",
    "            if col not in travel_df.columns:\n",
    "                travel_df[col] = 0\n",
    "        \n",
    "        travel_features = travel_df[travel_feature_cols].fillna(0)\n",
    "        return self.travel_scaler.fit_transform(travel_features.values.astype(np.float32))\n",
    "\n",
    "\n",
    "class ImprovedTravelGNN(nn.Module):\n",
    "    \"\"\"향상된 여행 추천 GNN 모델\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, travel_context_dim, \n",
    "                 num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # GAT 레이어들\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True, edge_dim=5)\n",
    "        self.gat2 = GATConv(hidden_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True, edge_dim=5)\n",
    "        self.gat3 = GATConv(hidden_channels, out_channels, \n",
    "                           heads=1, dropout=dropout, concat=False, edge_dim=5)\n",
    "        \n",
    "        # Batch Normalization\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # 여행 컨텍스트 인코더\n",
    "        self.travel_encoder = nn.Sequential(\n",
    "            nn.Linear(travel_context_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # 거리 기반 attention\n",
    "        self.distance_attention = nn.Sequential(\n",
    "            nn.Linear(2, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 융합 네트워크\n",
    "        self.fusion_net = nn.Sequential(\n",
    "            nn.Linear(out_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # 선호도 예측 헤드\n",
    "        self.preference_head = nn.Sequential(\n",
    "            nn.Linear(out_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, data, travel_context, return_attention=False):\n",
    "        x = data['visit_area'].x\n",
    "        edge_index = data['visit_area', 'moved_to', 'visit_area'].edge_index\n",
    "        edge_attr = data['visit_area', 'moved_to', 'visit_area'].edge_attr\n",
    "        \n",
    "        # 좌표 정보 추출\n",
    "        coords = x[:, :2]\n",
    "        \n",
    "        # GAT 레이어들\n",
    "        x1 = self.gat1(x, edge_index, edge_attr)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        x2 = self.gat2(x1, edge_index, edge_attr)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2 + x1)  # Residual connection\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        graph_embedding = self.gat3(x2, edge_index, edge_attr)\n",
    "        graph_embedding = self.bn3(graph_embedding)\n",
    "        \n",
    "        # 거리 기반 attention 적용\n",
    "        distance_weights = self.distance_attention(coords)\n",
    "        graph_embedding = graph_embedding * distance_weights\n",
    "        \n",
    "        # 여행 컨텍스트 처리\n",
    "        travel_embedding = self.travel_encoder(travel_context)\n",
    "        travel_embedding_expanded = travel_embedding.expand(graph_embedding.size(0), -1)\n",
    "        \n",
    "        # 특성 융합\n",
    "        fused_features = torch.cat([graph_embedding, travel_embedding_expanded], dim=1)\n",
    "        final_embedding = self.fusion_net(fused_features)\n",
    "        \n",
    "        # 선호도 점수 예측\n",
    "        preference_scores = self.preference_head(final_embedding)\n",
    "        \n",
    "        if return_attention:\n",
    "            return final_embedding, preference_scores, distance_weights\n",
    "        \n",
    "        return final_embedding, preference_scores\n",
    "\n",
    "\n",
    "class TravelDatasetLoader:\n",
    "    \"\"\"여행 데이터셋 로더\"\"\"\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        \n",
    "    def create_training_targets(self, visit_area_df, move_df, id_to_index):\n",
    "        \"\"\"학습 타겟 생성 - 중복 ID 고려한 집계\"\"\"\n",
    "        logger.info(\"학습 타겟 생성 시작...\")\n",
    "        \n",
    "        # 1. 유효한 방문지만 필터링\n",
    "        valid_visit_df = visit_area_df[visit_area_df['NEW_VISIT_AREA_ID'].notna()].copy()\n",
    "        valid_visit_df['NEW_VISIT_AREA_ID'] = valid_visit_df['NEW_VISIT_AREA_ID'].astype(int)\n",
    "        \n",
    "        # 2. 중복된 방문지 정보 집계\n",
    "        logger.info(\"중복된 방문지 타겟 정보 집계 중...\")\n",
    "        \n",
    "        # 만족도 관련 컬럼들의 평균 계산\n",
    "        satisfaction_cols = ['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']\n",
    "        agg_dict = {'VISIT_AREA_NM': 'first'}  # 방문지명은 첫 번째 값\n",
    "        \n",
    "        for col in satisfaction_cols:\n",
    "            if col in valid_visit_df.columns:\n",
    "                agg_dict[col] = 'mean'  # 만족도는 평균값\n",
    "            else:\n",
    "                valid_visit_df[col] = 3  # 기본값\n",
    "                agg_dict[col] = 'mean'\n",
    "        \n",
    "        # 각 NEW_VISIT_AREA_ID별로 집계\n",
    "        aggregated_visit_df = valid_visit_df.groupby('NEW_VISIT_AREA_ID').agg(agg_dict).reset_index()\n",
    "        \n",
    "        logger.info(f\"집계된 유니크 방문지: {len(aggregated_visit_df)}개\")\n",
    "        \n",
    "        # 3. 방문 빈도 계산 (실제 ID 기준, 유효한 것만)\n",
    "        valid_move_df = move_df.copy()\n",
    "        visit_counts = {}\n",
    "        \n",
    "        # END_VISIT_AREA_ID에서 방문 빈도 계산\n",
    "        if 'NEW_END_VISIT_AREA_ID' in valid_move_df.columns:\n",
    "            end_counts = valid_move_df['NEW_END_VISIT_AREA_ID'].dropna().astype(int).value_counts()\n",
    "            visit_counts.update(end_counts.to_dict())\n",
    "        \n",
    "        # START_VISIT_AREA_ID에서도 방문 빈도 계산 (있다면)\n",
    "        if 'NEW_START_VISIT_AREA_ID' in valid_move_df.columns:\n",
    "            start_counts = valid_move_df['NEW_START_VISIT_AREA_ID'].dropna().astype(int).value_counts()\n",
    "            for id, count in start_counts.items():\n",
    "                visit_counts[id] = visit_counts.get(id, 0) + count\n",
    "        \n",
    "        max_visit_count = max(visit_counts.values()) if visit_counts else 1\n",
    "        logger.info(f\"방문 빈도 계산 완료: {len(visit_counts)}개 방문지, 최대 방문 {max_visit_count}회\")\n",
    "        \n",
    "        # 4. 각 방문지에 대한 타겟 점수 계산 (id_to_index 순서로)\n",
    "        targets = []\n",
    "        missing_ids = []\n",
    "        \n",
    "        # aggregated_visit_df를 딕셔너리로 변환하여 빠른 접근\n",
    "        visit_dict = aggregated_visit_df.set_index('NEW_VISIT_AREA_ID').to_dict('index')\n",
    "        \n",
    "        # id_to_index의 순서대로 타겟 생성\n",
    "        for visit_id in sorted(id_to_index.keys()):\n",
    "            if visit_id in visit_dict:\n",
    "                row_data = visit_dict[visit_id]\n",
    "                \n",
    "                # 방문 빈도 (정규화)\n",
    "                visit_freq = visit_counts.get(visit_id, 0)\n",
    "                freq_score = min(visit_freq / max_visit_count, 1.0)\n",
    "                \n",
    "                # 만족도 점수 (안전하게 처리)\n",
    "                dgstfn = row_data.get('DGSTFN', 3)\n",
    "                revisit = row_data.get('REVISIT_INTENTION', 3)\n",
    "                recommend = row_data.get('RCMDTN_INTENTION', 3)\n",
    "                \n",
    "                # None이나 NaN인 경우 기본값 사용\n",
    "                dgstfn = dgstfn if pd.notna(dgstfn) else 3\n",
    "                revisit = revisit if pd.notna(revisit) else 3\n",
    "                recommend = recommend if pd.notna(recommend) else 3\n",
    "                \n",
    "                satisfaction = (dgstfn * 0.4 + revisit * 0.3 + recommend * 0.3) / 5.0\n",
    "                \n",
    "                # 제외 장소에 대한 페널티\n",
    "                area_name = row_data.get('VISIT_AREA_NM', '')\n",
    "                exclude_penalty = 0.3 if self.processor.should_exclude_location(area_name) else 0\n",
    "                \n",
    "                # 최종 타겟 점수\n",
    "                final_score = (freq_score * 0.6 + satisfaction * 0.4) - exclude_penalty\n",
    "                final_score = max(0, min(final_score, 1))  # 0-1 범위로 클램핑\n",
    "                \n",
    "                targets.append(final_score)\n",
    "            else:\n",
    "                # 해당 ID가 visit_area_df에 없는 경우\n",
    "                missing_ids.append(visit_id)\n",
    "                # 기본 점수: 방문 빈도만 고려\n",
    "                visit_freq = visit_counts.get(visit_id, 0)\n",
    "                freq_score = min(visit_freq / max_visit_count, 1.0)\n",
    "                targets.append(freq_score * 0.5)  # 낮은 기본 점수\n",
    "        \n",
    "        if missing_ids:\n",
    "            logger.warning(f\"방문지 정보가 없는 ID {len(missing_ids)}개: {missing_ids[:10]}...\")\n",
    "        \n",
    "        logger.info(f\"타겟 생성 완료: {len(targets)}개\")\n",
    "        logger.info(f\"타겟 점수 범위: {min(targets):.3f} ~ {max(targets):.3f}\")\n",
    "        \n",
    "        return torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class TravelRecommendationTrainer:\n",
    "    \"\"\"여행 추천 모델 학습기\"\"\"\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.processor = EnhancedDataProcessor()\n",
    "        self.dataset_loader = TravelDatasetLoader(self.processor)\n",
    "        \n",
    "    def prepare_data(self, visit_area_df, move_df, travel_df=None):\n",
    "        \"\"\"데이터 준비 - 매핑 실패 및 예외 상황 고려\"\"\"\n",
    "        logger.info(\"데이터 준비 시작...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. 먼저 엣지 생성하여 ID 매핑 얻기\n",
    "            edge_index, edge_attr, id_to_index = self.processor.create_enhanced_edges(move_df, visit_area_df)\n",
    "            \n",
    "            if edge_index is None or id_to_index is None or len(id_to_index) == 0:\n",
    "                logger.error(\"엣지 생성에 실패했거나 유효한 ID 매핑이 없습니다!\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # 2. ID 매핑을 사용하여 방문지 특성 처리\n",
    "            visit_features = self.processor.process_visit_area_features(visit_area_df, id_to_index)\n",
    "            \n",
    "            if visit_features is None or len(visit_features) == 0:\n",
    "                logger.error(\"방문지 특성 처리에 실패했습니다!\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # 3. 노드 수와 ID 매핑 수가 일치하는지 확인\n",
    "            expected_nodes = len(id_to_index)\n",
    "            actual_nodes = visit_features.shape[0]\n",
    "            \n",
    "            if expected_nodes != actual_nodes:\n",
    "                logger.error(f\"노드 수 불일치: 예상 {expected_nodes}, 실제 {actual_nodes}\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # 4. 헤테로 그래프 데이터 생성\n",
    "            data = HeteroData()\n",
    "            data['visit_area'].x = torch.tensor(visit_features, dtype=torch.float32)\n",
    "            data['visit_area', 'moved_to', 'visit_area'].edge_index = edge_index\n",
    "            data['visit_area', 'moved_to', 'visit_area'].edge_attr = edge_attr\n",
    "            \n",
    "            # 5. 학습 타겟 생성 (ID 매핑 전달)\n",
    "            targets = self.dataset_loader.create_training_targets(visit_area_df, move_df, id_to_index)\n",
    "            \n",
    "            if targets is None or len(targets) == 0:\n",
    "                logger.error(\"학습 타겟 생성에 실패했습니다!\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # 6. 타겟 수와 노드 수 일치 확인\n",
    "            if len(targets) != expected_nodes:\n",
    "                logger.error(f\"타겟 수 불일치: 예상 {expected_nodes}, 실제 {len(targets)}\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # 7. 여행 컨텍스트 처리 (있는 경우)\n",
    "            travel_contexts = None\n",
    "            if travel_df is not None and not travel_df.empty:\n",
    "                try:\n",
    "                    travel_contexts = self.processor.process_travel_context(travel_df)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"여행 컨텍스트 처리 실패: {e}, 기본값 사용\")\n",
    "                    travel_contexts = None\n",
    "            \n",
    "            if travel_contexts is None:\n",
    "                # 기본 여행 컨텍스트 생성\n",
    "                logger.info(\"기본 여행 컨텍스트 생성...\")\n",
    "                travel_contexts = np.zeros((1, 21), dtype=np.float32)  # 21개 특성\n",
    "                travel_contexts[0, 2] = 6  # MONTH = 6 (6월)\n",
    "                travel_contexts[0, 3] = 2  # DURATION = 2일\n",
    "                travel_contexts[0, 5] = 1  # MVMN_대중교통 = 1\n",
    "                travel_contexts[0, 16] = 1  # WHOWITH_2인여행 = 1\n",
    "            \n",
    "            # 8. 최종 검증\n",
    "            num_nodes = data['visit_area'].x.shape[0]\n",
    "            num_edges = data['visit_area', 'moved_to', 'visit_area'].edge_index.shape[1]\n",
    "            num_features = data['visit_area'].x.shape[1]\n",
    "            \n",
    "            # 엣지 인덱스가 노드 범위를 벗어나지 않는지 확인\n",
    "            max_edge_idx = edge_index.max().item()\n",
    "            if max_edge_idx >= num_nodes:\n",
    "                logger.error(f\"엣지 인덱스 오류: 최대 인덱스 {max_edge_idx} >= 노드 수 {num_nodes}\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            logger.info(f\"✅ 데이터 준비 완료:\")\n",
    "            logger.info(f\"  - 노드 수: {num_nodes}\")\n",
    "            logger.info(f\"  - 특성 수: {num_features}\")\n",
    "            logger.info(f\"  - 엣지 수: {num_edges}\")\n",
    "            logger.info(f\"  - 여행 컨텍스트: {travel_contexts.shape}\")\n",
    "            logger.info(f\"  - ID 매핑 수: {len(id_to_index)}\")\n",
    "            logger.info(f\"  - 타겟 수: {len(targets)}\")\n",
    "            logger.info(f\"  - 엣지 인덱스 범위: 0 ~ {max_edge_idx}\")\n",
    "            \n",
    "            return data, targets, travel_contexts, id_to_index\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"데이터 준비 중 오류 발생: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None, None, None, None\n",
    "    \n",
    "    def train_model(self, data, targets, travel_contexts, epochs=200, lr=0.001, \n",
    "                   weight_decay=1e-4, save_path=\"./models/\"):\n",
    "        \"\"\"모델 학습\"\"\"\n",
    "        logger.info(\"모델 학습 시작...\")\n",
    "        \n",
    "        # 디렉토리 생성\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        data = data.to(self.device)\n",
    "        targets = targets.to(self.device)\n",
    "        travel_contexts = torch.tensor(travel_contexts, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # 모델 초기화\n",
    "        in_channels = data['visit_area'].x.shape[1]\n",
    "        hidden_channels = 256\n",
    "        out_channels = 128\n",
    "        travel_context_dim = travel_contexts.shape[1]\n",
    "        \n",
    "        model = ImprovedTravelGNN(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            travel_context_dim=travel_context_dim,\n",
    "            num_heads=4,\n",
    "            dropout=0.2\n",
    "        ).to(self.device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        # 수정된 부분: verbose 매개변수 제거\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=20\n",
    "        )\n",
    "        \n",
    "        # 현재 학습률 추적을 위한 변수\n",
    "        current_lr = lr\n",
    "        \n",
    "        # 학습 루프\n",
    "        model.train()\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        max_patience = 50\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 첫 번째 여행 컨텍스트 사용 (배치 처리 시뮬레이션)\n",
    "            travel_context = travel_contexts[0:1]\n",
    "            \n",
    "            # Forward pass\n",
    "            embeddings, preference_scores = model(data, travel_context)\n",
    "            \n",
    "            # 손실 계산 (MSE)\n",
    "            loss = F.mse_loss(preference_scores.squeeze(), targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # 학습률 스케줄링 (수정된 부분: 학습률 변화 수동 추적)\n",
    "            old_lr = current_lr\n",
    "            scheduler.step(loss)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # 학습률이 변경된 경우 로그 출력\n",
    "            if current_lr != old_lr:\n",
    "                logger.info(f\"Epoch {epoch}: Learning rate reduced from {old_lr:.6f} to {current_lr:.6f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                patience_counter = 0\n",
    "                \n",
    "                # 최고 모델 저장\n",
    "                model_config = {\n",
    "                    'in_channels': in_channels,\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'out_channels': out_channels,\n",
    "                    'travel_context_dim': travel_context_dim,\n",
    "                    'num_heads': 4,\n",
    "                    'dropout': 0.2\n",
    "                }\n",
    "                \n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'model_config': model_config,\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'loss': best_loss,\n",
    "                    'losses': losses\n",
    "                }, os.path.join(save_path, 'improved_travel_recommendation_model.pt'))\n",
    "                \n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= max_patience:\n",
    "                logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "            # 로그 출력\n",
    "            if epoch % 20 == 0:\n",
    "                logger.info(f'Epoch {epoch:03d}, Loss: {loss.item():.4f}, Best: {best_loss:.4f}, LR: {current_lr:.6f}')\n",
    "        \n",
    "        logger.info(f'학습 완료! 최종 손실: {best_loss:.4f}')\n",
    "        \n",
    "        return model, losses\n",
    "    \n",
    "    def save_processed_data(self, visit_area_df, data, id_to_index, save_path=\"./pickle/\"):\n",
    "        \"\"\"처리된 데이터 저장 - 기존 포맷 호환\"\"\"\n",
    "        logger.info(\"처리된 데이터 저장 시작...\")\n",
    "        \n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # 기존 포맷에 맞춘 데이터 저장\n",
    "        save_data = {\n",
    "            'visit_area_df': visit_area_df,\n",
    "            'graph_data': data.cpu(),  # CPU로 이동하여 저장\n",
    "            'visit_scaler': self.processor.visit_scaler,\n",
    "            'travel_scaler': self.processor.travel_scaler,\n",
    "            'device': str(self.device),\n",
    "            # 추가 정보 (기존 코드 호환성 유지하면서 새 기능 제공)\n",
    "            'id_to_index': id_to_index,  # ID 매핑 정보\n",
    "            'region_info': {\n",
    "                'num_nodes': data['visit_area'].x.shape[0],\n",
    "                'num_edges': data['visit_area', 'moved_to', 'visit_area'].edge_index.shape[1],\n",
    "                'num_features': data['visit_area'].x.shape[1]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_path, 'improved_travel_data.pkl'), 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "        \n",
    "        logger.info(f\"데이터 저장 완료: {save_path}\")\n",
    "        logger.info(f\"저장된 데이터 구조:\")\n",
    "        logger.info(f\"  - visit_area_df: {visit_area_df.shape}\")\n",
    "        logger.info(f\"  - graph_data: 노드 {save_data['region_info']['num_nodes']}개, 엣지 {save_data['region_info']['num_edges']}개\")\n",
    "        logger.info(f\"  - visit_scaler: {type(self.processor.visit_scaler).__name__}\")\n",
    "        logger.info(f\"  - travel_scaler: {type(self.processor.travel_scaler).__name__}\")\n",
    "        logger.info(f\"  - id_to_index: {len(id_to_index)}개 매핑\")\n",
    "        logger.info(f\"  - device: {self.device}\")\n",
    "\n",
    "\n",
    "def main_training_pipeline(visit_area_df, move_df, travel_df=None, \n",
    "                          model_save_path=\"./models/\", data_save_path=\"./pickle/\"):\n",
    "    \"\"\"전체 학습 파이프라인 - 매핑 실패 고려\"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"🚀 GNN 여행 추천 시스템 학습 시작!\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # 1. 트레이너 초기화\n",
    "        trainer = TravelRecommendationTrainer()\n",
    "        \n",
    "        # 2. 데이터 준비\n",
    "        data, targets, travel_contexts, id_to_index = trainer.prepare_data(visit_area_df, move_df, travel_df)\n",
    "        \n",
    "        if data is None or targets is None:\n",
    "            logger.error(\"❌ 데이터 준비에 실패했습니다!\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # 3. 모델 학습\n",
    "        model, losses = trainer.train_model(\n",
    "            data, targets, travel_contexts,\n",
    "            epochs=200,\n",
    "            lr=0.001,\n",
    "            save_path=model_save_path\n",
    "        )\n",
    "        \n",
    "        # 4. 처리된 데이터 저장\n",
    "        trainer.save_processed_data(visit_area_df, data, id_to_index, save_path=data_save_path)\n",
    "        \n",
    "        # 5. 학습 결과 요약\n",
    "        logger.info(\"\\n\" + \"=\" * 60)\n",
    "        logger.info(\"✅ 학습 완료!\")\n",
    "        logger.info(f\"📁 모델 저장 경로: {model_save_path}\")\n",
    "        logger.info(f\"📁 데이터 저장 경로: {data_save_path}\")\n",
    "        logger.info(f\"📊 최종 손실: {min(losses):.4f}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        return model, losses, data, targets\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 학습 중 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "def main_region_training(move_path, travel_path, visit_area_path, region_name):\n",
    "    \"\"\"지역별 모델 학습 및 저장 - 매핑 실패 및 데이터 검증 강화\"\"\"\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"🚀 지역 '{region_name}' GNN 모델 학습 시작!\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. 데이터 로드\n",
    "        logger.info(\"📂 데이터 로드 중...\")\n",
    "        \n",
    "        # 파일 존재 확인\n",
    "        if not os.path.exists(move_path):\n",
    "            logger.error(f\"❌ 이동내역 파일을 찾을 수 없습니다: {move_path}\")\n",
    "            return False\n",
    "        if not os.path.exists(visit_area_path):\n",
    "            logger.error(f\"❌ 방문지 파일을 찾을 수 없습니다: {visit_area_path}\")\n",
    "            return False\n",
    "            \n",
    "        # CSV 파일 로드\n",
    "        move_df = pd.read_csv(move_path)\n",
    "        visit_area_df = pd.read_csv(visit_area_path)\n",
    "        \n",
    "        # 2. 기본 데이터 검증\n",
    "        logger.info(\"🔍 데이터 검증 중...\")\n",
    "        \n",
    "        # 이동내역 검증\n",
    "        if len(move_df) == 0:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 이동내역 데이터가 비어있습니다.\")\n",
    "            return False\n",
    "            \n",
    "        # 방문지 검증\n",
    "        if len(visit_area_df) == 0:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 방문지 데이터가 비어있습니다.\")\n",
    "            return False\n",
    "        \n",
    "        # NEW_VISIT_AREA_ID 검증\n",
    "        if 'NEW_VISIT_AREA_ID' not in visit_area_df.columns:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 방문지 데이터에 NEW_VISIT_AREA_ID 컬럼이 없습니다.\")\n",
    "            return False\n",
    "            \n",
    "        valid_visit_areas = visit_area_df['NEW_VISIT_AREA_ID'].notna().sum()\n",
    "        if valid_visit_areas == 0:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 유효한 NEW_VISIT_AREA_ID가 없습니다.\")\n",
    "            return False\n",
    "            \n",
    "        # 이동내역의 NEW_END_VISIT_AREA_ID 검증\n",
    "        has_end_id = 'NEW_END_VISIT_AREA_ID' in move_df.columns\n",
    "        has_start_id = 'NEW_START_VISIT_AREA_ID' in move_df.columns\n",
    "        \n",
    "        if not has_end_id and not has_start_id:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 이동내역에 NEW_END_VISIT_AREA_ID 또는 NEW_START_VISIT_AREA_ID가 없습니다.\")\n",
    "            return False\n",
    "        \n",
    "        valid_moves = 0\n",
    "        if has_end_id:\n",
    "            valid_moves += move_df['NEW_END_VISIT_AREA_ID'].notna().sum()\n",
    "        if has_start_id:\n",
    "            valid_moves += move_df['NEW_START_VISIT_AREA_ID'].notna().sum()\n",
    "            \n",
    "        if valid_moves == 0:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 유효한 이동 기록이 없습니다.\")\n",
    "            return False\n",
    "        \n",
    "        # 여행 정보 파일 로드 (있는 경우)\n",
    "        travel_df = None\n",
    "        if os.path.exists(travel_path):\n",
    "            travel_df = pd.read_csv(travel_path)\n",
    "            logger.info(f\"✅ 여행정보 데이터 로드: {len(travel_df)}개 레코드\")\n",
    "        else:\n",
    "            logger.warning(f\"⚠️ 여행정보 파일이 없습니다: {travel_path}\")\n",
    "        \n",
    "        logger.info(f\"✅ 데이터 로드 및 검증 완료:\")\n",
    "        logger.info(f\"  - 이동내역: {len(move_df)}개 레코드 (유효 이동: {valid_moves}개)\")\n",
    "        logger.info(f\"  - 방문지: {len(visit_area_df)}개 레코드 (유효 방문지: {valid_visit_areas}개)\")\n",
    "        \n",
    "        # 3. 저장 경로 설정\n",
    "        model_save_path = f\"./models/{region_name}/\"\n",
    "        data_save_path = f\"./pickle/{region_name}/\"\n",
    "        \n",
    "        # 4. 트레이너 초기화\n",
    "        trainer = TravelRecommendationTrainer()\n",
    "        \n",
    "        # 5. 데이터 준비\n",
    "        logger.info(\"🔧 데이터 전처리 중...\")\n",
    "        data, targets, travel_contexts, id_to_index = trainer.prepare_data(visit_area_df, move_df, travel_df)\n",
    "        \n",
    "        # 6. 최종 데이터 검증\n",
    "        if data is None or targets is None:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 데이터 전처리에 실패했습니다.\")\n",
    "            return False\n",
    "        \n",
    "        min_nodes = 10  # 최소 노드 수\n",
    "        min_edges = 5   # 최소 엣지 수\n",
    "        \n",
    "        num_nodes = data['visit_area'].x.shape[0]\n",
    "        num_edges = data['visit_area', 'moved_to', 'visit_area'].edge_index.shape[1]\n",
    "        \n",
    "        if num_nodes < min_nodes:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 노드 수가 너무 적습니다 ({num_nodes} < {min_nodes})\")\n",
    "            return False\n",
    "            \n",
    "        if num_edges < min_edges:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 엣지 수가 너무 적습니다 ({num_edges} < {min_edges})\")\n",
    "            return False\n",
    "        \n",
    "        # 7. 모델 학습\n",
    "        logger.info(\"🎯 모델 학습 시작...\")\n",
    "        model, losses = trainer.train_model(\n",
    "            data, targets, travel_contexts,\n",
    "            epochs=200,\n",
    "            lr=0.001,\n",
    "            save_path=model_save_path\n",
    "        )\n",
    "        \n",
    "        # 8. 처리된 데이터 저장\n",
    "        logger.info(\"💾 데이터 저장 중...\")\n",
    "        trainer.save_processed_data(visit_area_df, data, id_to_index, save_path=data_save_path)\n",
    "        \n",
    "        # 9. 학습 결과 요약\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"✅ 지역 '{region_name}' 학습 완료!\")\n",
    "        logger.info(f\"📁 모델 저장: {model_save_path}\")\n",
    "        logger.info(f\"📁 데이터 저장: {data_save_path}\")\n",
    "        logger.info(f\"📊 최종 손실: {min(losses):.4f}\")\n",
    "        logger.info(f\"📊 총 에포크: {len(losses)}\")\n",
    "        logger.info(f\"🎯 노드 수: {num_nodes}, 엣지 수: {num_edges}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 지역 '{region_name}' 학습 중 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"지역별 모델 학습 메인 함수\"\"\"\n",
    "    logger.info(\"🌍 전체 지역 GNN 모델 학습 시작!\")\n",
    "    \n",
    "    base_dir = './merged_csv/merged_csv_region/'\n",
    "    \n",
    "    # 지역 목록 가져오기\n",
    "    if not os.path.exists(base_dir):\n",
    "        logger.error(f\"❌ 기본 디렉토리를 찾을 수 없습니다: {base_dir}\")\n",
    "        return\n",
    "    \n",
    "    region_list = [r for r in os.listdir(base_dir) \n",
    "                   if not r.startswith('.') and os.path.isdir(os.path.join(base_dir, r))]\n",
    "    \n",
    "    if not region_list:\n",
    "        logger.error(f\"❌ 처리할 지역이 없습니다: {base_dir}\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"📍 총 {len(region_list)}개 지역 발견: {region_list}\")\n",
    "    \n",
    "    # 성공/실패 추적\n",
    "    success_regions = []\n",
    "    failed_regions = []\n",
    "    \n",
    "    # 각 지역별로 처리\n",
    "    for i, region_name in enumerate(region_list, 1):\n",
    "        logger.info(f\"\\n{'🌟' * 20}\")\n",
    "        logger.info(f\"📍 [{i}/{len(region_list)}] 지역 '{region_name}' 처리 시작!\")\n",
    "        logger.info(f\"{'🌟' * 20}\")\n",
    "        \n",
    "        region_path = os.path.join(base_dir, region_name)\n",
    "        \n",
    "        # 파일 경로 정의\n",
    "        move_path = os.path.join(region_path, 'fin', '이동내역_fin.csv')\n",
    "        travel_path = os.path.join(region_path, 'fin', '여행_fin.csv')\n",
    "        visit_area_path = os.path.join(region_path, 'fin', '방문지_fin.csv')\n",
    "        \n",
    "        # 지역별 학습 실행\n",
    "        success = main_region_training(\n",
    "            move_path=move_path,\n",
    "            travel_path=travel_path,\n",
    "            visit_area_path=visit_area_path,\n",
    "            region_name=region_name\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            success_regions.append(region_name)\n",
    "            logger.info(f\"✅ 지역 '{region_name}' 처리 완료!\")\n",
    "        else:\n",
    "            failed_regions.append(region_name)\n",
    "            logger.error(f\"❌ 지역 '{region_name}' 처리 실패!\")\n",
    "    \n",
    "    # 최종 결과 요약\n",
    "    logger.info(f\"\\n{'🏁' * 30}\")\n",
    "    logger.info(\"🏁 전체 지역 처리 완료!\")\n",
    "    logger.info(f\"{'🏁' * 30}\")\n",
    "    logger.info(f\"✅ 성공한 지역 ({len(success_regions)}개): {success_regions}\")\n",
    "    if failed_regions:\n",
    "        logger.info(f\"❌ 실패한 지역 ({len(failed_regions)}개): {failed_regions}\")\n",
    "    \n",
    "    # 모델 파일 구조 출력\n",
    "    logger.info(f\"\\n📂 저장된 파일 구조:\")\n",
    "    for region in success_regions:\n",
    "        logger.info(f\"  📁 {region}/\")\n",
    "        logger.info(f\"    🤖 models/{region}/improved_travel_recommendation_model.pt\")\n",
    "        logger.info(f\"    💾 pickle/{region}/improved_travel_data.pkl\")\n",
    "\n",
    "\n",
    "# 개별 지역 테스트를 위한 함수\n",
    "def test_single_region(region_name):\n",
    "    \"\"\"단일 지역 테스트용 함수\"\"\"\n",
    "    base_dir = './merged_csv/merged_csv_region/'\n",
    "    region_path = os.path.join(base_dir, region_name)\n",
    "    \n",
    "    move_path = os.path.join(region_path, 'fin', '이동내역_fin.csv')\n",
    "    travel_path = os.path.join(region_path, 'fin', '여행_fin.csv')\n",
    "    visit_area_path = os.path.join(region_path, 'fin', '방문지_fin.csv')\n",
    "    \n",
    "    return main_region_training(\n",
    "        move_path=move_path,\n",
    "        travel_path=travel_path,\n",
    "        visit_area_path=visit_area_path,\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "\n",
    "# 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    # 전체 지역 학습 실행\n",
    "    main()\n",
    "    \n",
    "    # 또는 특정 지역만 테스트하고 싶다면:\n",
    "    # test_single_region(\"서울특별시\")\n",
    "    \n",
    "    # 주의사항 정리\n",
    "    # 1. 데이터 매핑 실패나 누락이 있어도 안전하게 처리됩니다\n",
    "    # 2. 각 지역별로 최소 노드 수(10개)와 엣지 수(5개) 검증을 합니다\n",
    "    # 3. 상세한 로그를 통해 문제점을 파악할 수 있습니다\n",
    "    # 4. ID 매핑과 엣지 인덱스가 항상 일치하도록 보장됩니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
