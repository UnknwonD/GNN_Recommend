{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-05 01:52:31,722 - INFO - 🌍 전체 지역 GNN 모델 학습 시작!\n",
      "2025-06-05 01:52:31,723 - INFO - 📍 총 4개 지역 발견: ['서부권', '동부권', '제주도 및 도서지역', '수도권']\n",
      "2025-06-05 01:52:31,723 - INFO - \n",
      "🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-05 01:52:31,723 - INFO - 📍 [1/4] 지역 '서부권' 처리 시작!\n",
      "2025-06-05 01:52:31,724 - INFO - 🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-05 01:52:31,724 - INFO - \n",
      "============================================================\n",
      "2025-06-05 01:52:31,724 - INFO - 🚀 지역 '서부권' GNN 모델 학습 시작!\n",
      "2025-06-05 01:52:31,725 - INFO - ============================================================\n",
      "2025-06-05 01:52:31,725 - INFO - 📂 데이터 로드 중...\n",
      "2025-06-05 01:52:31,856 - INFO - 🔍 데이터 검증 중...\n",
      "2025-06-05 01:52:31,863 - INFO - ✅ 여행정보 데이터 로드: 2880개 레코드\n",
      "2025-06-05 01:52:31,863 - INFO - ✅ 데이터 로드 및 검증 완료:\n",
      "2025-06-05 01:52:31,864 - INFO -   - 이동내역: 31875개 레코드 (유효 이동: 31875개)\n",
      "2025-06-05 01:52:31,864 - INFO -   - 방문지: 31875개 레코드 (유효 방문지: 31875개)\n",
      "2025-06-05 01:52:31,864 - INFO - 🔧 데이터 전처리 중...\n",
      "2025-06-05 01:52:31,864 - INFO - 데이터 준비 시작...\n",
      "2025-06-05 01:52:31,864 - INFO - 엣지 생성 시작...\n",
      "2025-06-05 01:52:31,866 - INFO - 방문지 데이터에서 사용 가능한 ID: 11761개\n",
      "2025-06-05 01:52:31,872 - INFO - 최종 사용할 방문지 ID: 170개\n",
      "2025-06-05 01:52:31,873 - INFO - 방문지 ID 매핑 생성: 170개\n",
      "Processing travel groups: 100%|██████████| 2880/2880 [00:01<00:00, 1633.60it/s]\n",
      "2025-06-05 01:52:33,701 - INFO - 엣지 생성 통계:\n",
      "2025-06-05 01:52:33,701 - INFO -   - 총 이동 시도: 28995개\n",
      "2025-06-05 01:52:33,702 - INFO -   - 유효한 엣지: 1180개\n",
      "2025-06-05 01:52:33,702 - INFO -   - TRIP_ID 누락: 0개\n",
      "2025-06-05 01:52:33,702 - INFO -   - 유효하지 않은 FROM_ID: 22256개\n",
      "2025-06-05 01:52:33,702 - INFO -   - 유효하지 않은 TO_ID: 4707개\n",
      "2025-06-05 01:52:33,709 - INFO - ✅ 엣지 생성 완료:\n",
      "2025-06-05 01:52:33,709 - INFO -   - 최종 엣지 수: 1180개\n",
      "2025-06-05 01:52:33,709 - INFO -   - 노드 인덱스 범위: 0 ~ 169\n",
      "2025-06-05 01:52:33,711 - INFO -   - 엣지 인덱스 최대값: 169\n",
      "2025-06-05 01:52:33,723 - INFO - NEW_VISIT_AREA_ID가 있는 방문지: 31875개\n",
      "2025-06-05 01:52:33,729 - INFO - ID 매핑에 있는 방문지: 8080개\n",
      "2025-06-05 01:52:33,729 - INFO - 중복된 방문지 ID 집계 중...\n",
      "2025-06-05 01:52:33,735 - INFO - 집계 후 유니크 방문지: 170개\n",
      "2025-06-05 01:52:33,740 - INFO - 최종 정렬된 방문지: 170개\n",
      "2025-06-05 01:52:33,746 - INFO - 방문지 특성 처리 완료: (170, 34)\n",
      "2025-06-05 01:52:33,751 - INFO - 학습 타겟 생성 시작...\n",
      "2025-06-05 01:52:33,758 - INFO - 중복된 방문지 타겟 정보 집계 중...\n",
      "2025-06-05 01:52:33,765 - INFO - 집계된 유니크 방문지: 11761개\n",
      "2025-06-05 01:52:33,769 - INFO - 방문 빈도 계산 완료: 1443개 방문지, 최대 방문 5382회\n",
      "2025-06-05 01:52:33,785 - INFO - 타겟 생성 완료: 170개\n",
      "2025-06-05 01:52:33,785 - INFO - 타겟 점수 범위: 0.000 ~ 0.629\n",
      "2025-06-05 01:52:33,787 - INFO - 여행 컨텍스트 처리 시작...\n",
      "2025-06-05 01:52:33,793 - INFO - ✅ 데이터 준비 완료:\n",
      "2025-06-05 01:52:33,793 - INFO -   - 노드 수: 170\n",
      "2025-06-05 01:52:33,794 - INFO -   - 특성 수: 34\n",
      "2025-06-05 01:52:33,794 - INFO -   - 엣지 수: 1180\n",
      "2025-06-05 01:52:33,794 - INFO -   - 여행 컨텍스트: (2880, 21)\n",
      "2025-06-05 01:52:33,794 - INFO -   - ID 매핑 수: 170\n",
      "2025-06-05 01:52:33,795 - INFO -   - 타겟 수: 170\n",
      "2025-06-05 01:52:33,795 - INFO -   - 엣지 인덱스 범위: 0 ~ 169\n",
      "2025-06-05 01:52:33,795 - INFO - 🎯 모델 학습 시작...\n",
      "2025-06-05 01:52:33,795 - INFO - 모델 학습 시작...\n",
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]2025-06-05 01:52:33,963 - INFO - Epoch 000, Loss: 0.1100, Best: 0.1100, LR: 0.001000\n",
      "Training:  10%|█         | 20/200 [00:00<00:03, 53.76it/s]2025-06-05 01:52:34,307 - INFO - Epoch 020, Loss: 0.0161, Best: 0.0161, LR: 0.001000\n",
      "Training:  20%|█▉        | 39/200 [00:00<00:02, 57.10it/s]2025-06-05 01:52:34,648 - INFO - Epoch 040, Loss: 0.0125, Best: 0.0101, LR: 0.001000\n",
      "Training:  26%|██▋       | 53/200 [00:00<00:02, 60.94it/s]2025-06-05 01:52:34,937 - INFO - Epoch 060, Loss: 0.0086, Best: 0.0075, LR: 0.001000\n",
      "Training:  34%|███▍      | 69/200 [00:01<00:01, 68.45it/s]2025-06-05 01:52:35,109 - INFO - Epoch 73: Learning rate reduced from 0.001000 to 0.000500\n",
      "Training:  38%|███▊      | 76/200 [00:01<00:01, 68.72it/s]2025-06-05 01:52:35,216 - INFO - Epoch 080, Loss: 0.0079, Best: 0.0064, LR: 0.000500\n",
      "Training:  49%|████▉     | 98/200 [00:01<00:01, 71.93it/s]2025-06-05 01:52:35,486 - INFO - Epoch 100, Loss: 0.0086, Best: 0.0062, LR: 0.000500\n",
      "2025-06-05 01:52:35,544 - INFO - Epoch 104: Learning rate reduced from 0.000500 to 0.000250\n",
      "Training:  57%|█████▋    | 114/200 [00:01<00:01, 72.87it/s]2025-06-05 01:52:35,767 - INFO - Epoch 120, Loss: 0.0051, Best: 0.0051, LR: 0.000250\n",
      "Training:  69%|██████▉   | 138/200 [00:02<00:00, 73.15it/s]2025-06-05 01:52:36,034 - INFO - Epoch 140, Loss: 0.0062, Best: 0.0051, LR: 0.000250\n",
      "2025-06-05 01:52:36,048 - INFO - Epoch 141: Learning rate reduced from 0.000250 to 0.000125\n",
      "Training:  77%|███████▋  | 154/200 [00:02<00:00, 74.95it/s]2025-06-05 01:52:36,295 - INFO - Epoch 160, Loss: 0.0060, Best: 0.0051, LR: 0.000125\n",
      "Training:  81%|████████  | 162/200 [00:02<00:00, 75.82it/s]2025-06-05 01:52:36,322 - INFO - Epoch 162: Learning rate reduced from 0.000125 to 0.000063\n",
      "Training:  85%|████████▌ | 170/200 [00:02<00:00, 74.57it/s]2025-06-05 01:52:36,434 - INFO - Early stopping at epoch 170\n",
      "Training:  85%|████████▌ | 170/200 [00:02<00:00, 65.80it/s]\n",
      "2025-06-05 01:52:36,435 - INFO - 학습 완료! 최종 손실: 0.0051\n",
      "2025-06-05 01:52:36,436 - INFO - 💾 데이터 저장 중...\n",
      "2025-06-05 01:52:36,436 - INFO - 처리된 데이터 저장 시작...\n",
      "2025-06-05 01:52:36,455 - INFO - 데이터 저장 완료: ./pickle/서부권/\n",
      "2025-06-05 01:52:36,456 - INFO - 저장된 데이터 구조:\n",
      "2025-06-05 01:52:36,456 - INFO -   - visit_area_df: (31875, 26)\n",
      "2025-06-05 01:52:36,456 - INFO -   - graph_data: 노드 170개, 엣지 1180개\n",
      "2025-06-05 01:52:36,456 - INFO -   - visit_scaler: RobustScaler\n",
      "2025-06-05 01:52:36,457 - INFO -   - travel_scaler: StandardScaler\n",
      "2025-06-05 01:52:36,457 - INFO -   - id_to_index: 170개 매핑\n",
      "2025-06-05 01:52:36,457 - INFO -   - device: cpu\n",
      "2025-06-05 01:52:36,457 - INFO - \n",
      "============================================================\n",
      "2025-06-05 01:52:36,457 - INFO - ✅ 지역 '서부권' 학습 완료!\n",
      "2025-06-05 01:52:36,458 - INFO - 📁 모델 저장: ./models/서부권/\n",
      "2025-06-05 01:52:36,458 - INFO - 📁 데이터 저장: ./pickle/서부권/\n",
      "2025-06-05 01:52:36,458 - INFO - 📊 최종 손실: 0.0051\n",
      "2025-06-05 01:52:36,458 - INFO - 📊 총 에포크: 171\n",
      "2025-06-05 01:52:36,458 - INFO - 🎯 노드 수: 170, 엣지 수: 1180\n",
      "2025-06-05 01:52:36,459 - INFO - ============================================================\n",
      "2025-06-05 01:52:36,464 - INFO - ✅ 지역 '서부권' 처리 완료!\n",
      "2025-06-05 01:52:36,465 - INFO - \n",
      "🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-05 01:52:36,465 - INFO - 📍 [2/4] 지역 '동부권' 처리 시작!\n",
      "2025-06-05 01:52:36,465 - INFO - 🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-05 01:52:36,465 - INFO - \n",
      "============================================================\n",
      "2025-06-05 01:52:36,466 - INFO - 🚀 지역 '동부권' GNN 모델 학습 시작!\n",
      "2025-06-05 01:52:36,466 - INFO - ============================================================\n",
      "2025-06-05 01:52:36,466 - INFO - 📂 데이터 로드 중...\n",
      "2025-06-05 01:52:36,599 - INFO - 🔍 데이터 검증 중...\n",
      "2025-06-05 01:52:36,606 - INFO - ✅ 여행정보 데이터 로드: 2880개 레코드\n",
      "2025-06-05 01:52:36,606 - INFO - ✅ 데이터 로드 및 검증 완료:\n",
      "2025-06-05 01:52:36,607 - INFO -   - 이동내역: 32930개 레코드 (유효 이동: 32930개)\n",
      "2025-06-05 01:52:36,607 - INFO -   - 방문지: 32930개 레코드 (유효 방문지: 32930개)\n",
      "2025-06-05 01:52:36,607 - INFO - 🔧 데이터 전처리 중...\n",
      "2025-06-05 01:52:36,607 - INFO - 데이터 준비 시작...\n",
      "2025-06-05 01:52:36,607 - INFO - 엣지 생성 시작...\n",
      "2025-06-05 01:52:36,610 - INFO - 방문지 데이터에서 사용 가능한 ID: 12744개\n",
      "2025-06-05 01:52:36,615 - INFO - 최종 사용할 방문지 ID: 224개\n",
      "2025-06-05 01:52:36,615 - INFO - 방문지 ID 매핑 생성: 224개\n",
      "Processing travel groups: 100%|██████████| 2880/2880 [00:02<00:00, 1262.77it/s]\n",
      "2025-06-05 01:52:38,918 - INFO - 엣지 생성 통계:\n",
      "2025-06-05 01:52:38,918 - INFO -   - 총 이동 시도: 30050개\n",
      "2025-06-05 01:52:38,918 - INFO -   - 유효한 엣지: 786개\n",
      "2025-06-05 01:52:38,918 - INFO -   - TRIP_ID 누락: 0개\n",
      "2025-06-05 01:52:38,919 - INFO -   - 유효하지 않은 FROM_ID: 23948개\n",
      "2025-06-05 01:52:38,919 - INFO -   - 유효하지 않은 TO_ID: 4675개\n",
      "2025-06-05 01:52:38,922 - INFO - ✅ 엣지 생성 완료:\n",
      "2025-06-05 01:52:38,923 - INFO -   - 최종 엣지 수: 786개\n",
      "2025-06-05 01:52:38,923 - INFO -   - 노드 인덱스 범위: 0 ~ 223\n",
      "2025-06-05 01:52:38,923 - INFO -   - 엣지 인덱스 최대값: 223\n",
      "2025-06-05 01:52:38,943 - INFO - NEW_VISIT_AREA_ID가 있는 방문지: 32930개\n",
      "2025-06-05 01:52:38,949 - INFO - ID 매핑에 있는 방문지: 9282개\n",
      "2025-06-05 01:52:38,949 - INFO - 중복된 방문지 ID 집계 중...\n",
      "2025-06-05 01:52:38,952 - INFO - 집계 후 유니크 방문지: 224개\n",
      "2025-06-05 01:52:38,955 - INFO - 최종 정렬된 방문지: 224개\n",
      "2025-06-05 01:52:38,965 - INFO - 방문지 특성 처리 완료: (224, 34)\n",
      "2025-06-05 01:52:38,970 - INFO - 학습 타겟 생성 시작...\n",
      "2025-06-05 01:52:38,977 - INFO - 중복된 방문지 타겟 정보 집계 중...\n",
      "2025-06-05 01:52:38,985 - INFO - 집계된 유니크 방문지: 12744개\n",
      "2025-06-05 01:52:38,988 - INFO - 방문 빈도 계산 완료: 1423개 방문지, 최대 방문 5121회\n",
      "2025-06-05 01:52:39,004 - INFO - 타겟 생성 완료: 224개\n",
      "2025-06-05 01:52:39,004 - INFO - 타겟 점수 범위: 0.000 ~ 0.656\n",
      "2025-06-05 01:52:39,006 - INFO - 여행 컨텍스트 처리 시작...\n",
      "2025-06-05 01:52:39,061 - INFO - ✅ 데이터 준비 완료:\n",
      "2025-06-05 01:52:39,062 - INFO -   - 노드 수: 224\n",
      "2025-06-05 01:52:39,062 - INFO -   - 특성 수: 34\n",
      "2025-06-05 01:52:39,063 - INFO -   - 엣지 수: 786\n",
      "2025-06-05 01:52:39,063 - INFO -   - 여행 컨텍스트: (2880, 21)\n",
      "2025-06-05 01:52:39,064 - INFO -   - ID 매핑 수: 224\n",
      "2025-06-05 01:52:39,064 - INFO -   - 타겟 수: 224\n",
      "2025-06-05 01:52:39,065 - INFO -   - 엣지 인덱스 범위: 0 ~ 223\n",
      "2025-06-05 01:52:39,065 - INFO - 🎯 모델 학습 시작...\n",
      "2025-06-05 01:52:39,065 - INFO - 모델 학습 시작...\n",
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]2025-06-05 01:52:39,129 - INFO - Epoch 000, Loss: 0.1052, Best: 0.1052, LR: 0.001000\n",
      "Training:  10%|█         | 20/200 [00:00<00:03, 45.05it/s]2025-06-05 01:52:39,624 - INFO - Epoch 020, Loss: 0.0176, Best: 0.0176, LR: 0.001000\n",
      "Training:  18%|█▊        | 36/200 [00:00<00:03, 46.29it/s]2025-06-05 01:52:40,036 - INFO - Epoch 040, Loss: 0.0125, Best: 0.0125, LR: 0.001000\n",
      "Training:  30%|███       | 60/200 [00:01<00:03, 45.64it/s]2025-06-05 01:52:40,443 - INFO - Epoch 060, Loss: 0.0113, Best: 0.0098, LR: 0.001000\n",
      "Training:  40%|████      | 80/200 [00:01<00:02, 56.08it/s]2025-06-05 01:52:40,765 - INFO - Epoch 080, Loss: 0.0099, Best: 0.0087, LR: 0.001000\n",
      "Training:  50%|████▉     | 99/200 [00:02<00:01, 54.09it/s]2025-06-05 01:52:41,146 - INFO - Epoch 100, Loss: 0.0104, Best: 0.0087, LR: 0.001000\n",
      "Training:  60%|█████▉    | 119/200 [00:02<00:01, 57.24it/s]2025-06-05 01:52:41,509 - INFO - Epoch 120, Loss: 0.0083, Best: 0.0081, LR: 0.001000\n",
      "Training:  69%|██████▉   | 138/200 [00:02<00:01, 53.50it/s]2025-06-05 01:52:41,896 - INFO - Epoch 140, Loss: 0.0076, Best: 0.0067, LR: 0.001000\n",
      "Training:  72%|███████▏  | 144/200 [00:02<00:01, 54.81it/s]2025-06-05 01:52:41,971 - INFO - Epoch 144: Learning rate reduced from 0.001000 to 0.000500\n",
      "Training:  79%|███████▉  | 158/200 [00:03<00:00, 60.28it/s]2025-06-05 01:52:42,200 - INFO - Epoch 160, Loss: 0.0070, Best: 0.0062, LR: 0.000500\n",
      "Training:  86%|████████▋ | 173/200 [00:03<00:00, 63.97it/s]2025-06-05 01:52:42,480 - INFO - Epoch 180, Loss: 0.0071, Best: 0.0061, LR: 0.000500\n",
      "Training:  90%|█████████ | 181/200 [00:03<00:00, 68.38it/s]2025-06-05 01:52:42,533 - INFO - Epoch 184: Learning rate reduced from 0.000500 to 0.000250\n",
      "Training: 100%|██████████| 200/200 [00:03<00:00, 54.96it/s]\n",
      "2025-06-05 01:52:42,723 - INFO - 학습 완료! 최종 손실: 0.0061\n",
      "2025-06-05 01:52:42,724 - INFO - 💾 데이터 저장 중...\n",
      "2025-06-05 01:52:42,724 - INFO - 처리된 데이터 저장 시작...\n",
      "2025-06-05 01:52:42,791 - INFO - 데이터 저장 완료: ./pickle/동부권/\n",
      "2025-06-05 01:52:42,791 - INFO - 저장된 데이터 구조:\n",
      "2025-06-05 01:52:42,792 - INFO -   - visit_area_df: (32930, 26)\n",
      "2025-06-05 01:52:42,792 - INFO -   - graph_data: 노드 224개, 엣지 786개\n",
      "2025-06-05 01:52:42,793 - INFO -   - visit_scaler: RobustScaler\n",
      "2025-06-05 01:52:42,793 - INFO -   - travel_scaler: StandardScaler\n",
      "2025-06-05 01:52:42,793 - INFO -   - id_to_index: 224개 매핑\n",
      "2025-06-05 01:52:42,793 - INFO -   - device: cpu\n",
      "2025-06-05 01:52:42,793 - INFO - \n",
      "============================================================\n",
      "2025-06-05 01:52:42,794 - INFO - ✅ 지역 '동부권' 학습 완료!\n",
      "2025-06-05 01:52:42,794 - INFO - 📁 모델 저장: ./models/동부권/\n",
      "2025-06-05 01:52:42,794 - INFO - 📁 데이터 저장: ./pickle/동부권/\n",
      "2025-06-05 01:52:42,794 - INFO - 📊 최종 손실: 0.0061\n",
      "2025-06-05 01:52:42,794 - INFO - 📊 총 에포크: 200\n",
      "2025-06-05 01:52:42,795 - INFO - 🎯 노드 수: 224, 엣지 수: 786\n",
      "2025-06-05 01:52:42,795 - INFO - ============================================================\n",
      "2025-06-05 01:52:42,799 - INFO - ✅ 지역 '동부권' 처리 완료!\n",
      "2025-06-05 01:52:42,799 - INFO - \n",
      "🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-05 01:52:42,799 - INFO - 📍 [3/4] 지역 '제주도 및 도서지역' 처리 시작!\n",
      "2025-06-05 01:52:42,800 - INFO - 🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-05 01:52:42,800 - INFO - \n",
      "============================================================\n",
      "2025-06-05 01:52:42,800 - INFO - 🚀 지역 '제주도 및 도서지역' GNN 모델 학습 시작!\n",
      "2025-06-05 01:52:42,800 - INFO - ============================================================\n",
      "2025-06-05 01:52:42,801 - INFO - 📂 데이터 로드 중...\n",
      "2025-06-05 01:52:43,018 - INFO - 🔍 데이터 검증 중...\n",
      "2025-06-05 01:52:43,026 - INFO - ✅ 여행정보 데이터 로드: 2880개 레코드\n",
      "2025-06-05 01:52:43,026 - INFO - ✅ 데이터 로드 및 검증 완료:\n",
      "2025-06-05 01:52:43,026 - INFO -   - 이동내역: 51596개 레코드 (유효 이동: 51596개)\n",
      "2025-06-05 01:52:43,026 - INFO -   - 방문지: 51596개 레코드 (유효 방문지: 51596개)\n",
      "2025-06-05 01:52:43,027 - INFO - 🔧 데이터 전처리 중...\n",
      "2025-06-05 01:52:43,027 - INFO - 데이터 준비 시작...\n",
      "2025-06-05 01:52:43,027 - INFO - 엣지 생성 시작...\n",
      "2025-06-05 01:52:43,030 - INFO - 방문지 데이터에서 사용 가능한 ID: 15713개\n",
      "2025-06-05 01:52:43,037 - INFO - 최종 사용할 방문지 ID: 523개\n",
      "2025-06-05 01:52:43,037 - INFO - 방문지 ID 매핑 생성: 523개\n",
      "Processing travel groups: 100%|██████████| 2880/2880 [00:02<00:00, 1080.08it/s]\n",
      "2025-06-05 01:52:45,728 - INFO - 엣지 생성 통계:\n",
      "2025-06-05 01:52:45,728 - INFO -   - 총 이동 시도: 48716개\n",
      "2025-06-05 01:52:45,728 - INFO -   - 유효한 엣지: 5398개\n",
      "2025-06-05 01:52:45,729 - INFO -   - TRIP_ID 누락: 0개\n",
      "2025-06-05 01:52:45,729 - INFO -   - 유효하지 않은 FROM_ID: 32209개\n",
      "2025-06-05 01:52:45,729 - INFO -   - 유효하지 않은 TO_ID: 9858개\n",
      "2025-06-05 01:52:45,736 - INFO - ✅ 엣지 생성 완료:\n",
      "2025-06-05 01:52:45,736 - INFO -   - 최종 엣지 수: 5398개\n",
      "2025-06-05 01:52:45,736 - INFO -   - 노드 인덱스 범위: 0 ~ 522\n",
      "2025-06-05 01:52:45,737 - INFO -   - 엣지 인덱스 최대값: 522\n",
      "2025-06-05 01:52:45,756 - INFO - NEW_VISIT_AREA_ID가 있는 방문지: 51596개\n",
      "2025-06-05 01:52:45,764 - INFO - ID 매핑에 있는 방문지: 15227개\n",
      "2025-06-05 01:52:45,764 - INFO - 중복된 방문지 ID 집계 중...\n",
      "2025-06-05 01:52:45,768 - INFO - 집계 후 유니크 방문지: 523개\n",
      "2025-06-05 01:52:45,774 - INFO - 최종 정렬된 방문지: 523개\n",
      "2025-06-05 01:52:45,781 - INFO - 방문지 특성 처리 완료: (523, 36)\n",
      "2025-06-05 01:52:45,786 - INFO - 학습 타겟 생성 시작...\n",
      "2025-06-05 01:52:45,798 - INFO - 중복된 방문지 타겟 정보 집계 중...\n",
      "2025-06-05 01:52:45,805 - INFO - 집계된 유니크 방문지: 15713개\n",
      "2025-06-05 01:52:45,809 - INFO - 방문 빈도 계산 완료: 1443개 방문지, 최대 방문 8422회\n",
      "2025-06-05 01:52:45,830 - INFO - 타겟 생성 완료: 523개\n",
      "2025-06-05 01:52:45,831 - INFO - 타겟 점수 범위: 0.000 ~ 0.647\n",
      "2025-06-05 01:52:45,834 - INFO - 여행 컨텍스트 처리 시작...\n",
      "2025-06-05 01:52:45,839 - INFO - ✅ 데이터 준비 완료:\n",
      "2025-06-05 01:52:45,839 - INFO -   - 노드 수: 523\n",
      "2025-06-05 01:52:45,839 - INFO -   - 특성 수: 36\n",
      "2025-06-05 01:52:45,839 - INFO -   - 엣지 수: 5398\n",
      "2025-06-05 01:52:45,839 - INFO -   - 여행 컨텍스트: (2880, 21)\n",
      "2025-06-05 01:52:45,840 - INFO -   - ID 매핑 수: 523\n",
      "2025-06-05 01:52:45,840 - INFO -   - 타겟 수: 523\n",
      "2025-06-05 01:52:45,840 - INFO -   - 엣지 인덱스 범위: 0 ~ 522\n",
      "2025-06-05 01:52:45,840 - INFO - 🎯 모델 학습 시작...\n",
      "2025-06-05 01:52:45,840 - INFO - 모델 학습 시작...\n",
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]2025-06-05 01:52:45,901 - INFO - Epoch 000, Loss: 0.0492, Best: 0.0492, LR: 0.001000\n",
      "Training:   9%|▉         | 18/200 [00:00<00:06, 27.27it/s]2025-06-05 01:52:46,662 - INFO - Epoch 020, Loss: 0.0109, Best: 0.0108, LR: 0.001000\n",
      "Training:  20%|█▉        | 39/200 [00:01<00:06, 24.67it/s]2025-06-05 01:52:47,493 - INFO - Epoch 040, Loss: 0.0088, Best: 0.0082, LR: 0.001000\n",
      "Training:  28%|██▊       | 57/200 [00:02<00:05, 26.44it/s]2025-06-05 01:52:48,242 - INFO - Epoch 060, Loss: 0.0069, Best: 0.0069, LR: 0.001000\n",
      "Training:  38%|███▊      | 77/200 [00:02<00:04, 28.54it/s]2025-06-05 01:52:48,931 - INFO - Epoch 080, Loss: 0.0066, Best: 0.0056, LR: 0.001000\n",
      "Training:  50%|█████     | 100/200 [00:03<00:03, 29.05it/s]2025-06-05 01:52:49,622 - INFO - Epoch 100, Loss: 0.0051, Best: 0.0051, LR: 0.001000\n",
      "Training:  60%|█████▉    | 119/200 [00:04<00:03, 26.83it/s]2025-06-05 01:52:50,398 - INFO - Epoch 120, Loss: 0.0051, Best: 0.0045, LR: 0.001000\n",
      "Training:  68%|██████▊   | 137/200 [00:05<00:02, 27.36it/s]2025-06-05 01:52:51,087 - INFO - Epoch 139: Learning rate reduced from 0.001000 to 0.000500\n",
      "Training:  70%|███████   | 140/200 [00:05<00:02, 27.38it/s]2025-06-05 01:52:51,127 - INFO - Epoch 140, Loss: 0.0048, Best: 0.0045, LR: 0.000500\n",
      "Training:  79%|███████▉  | 158/200 [00:05<00:01, 26.93it/s]2025-06-05 01:52:51,877 - INFO - Epoch 160, Loss: 0.0047, Best: 0.0041, LR: 0.000500\n",
      "Training:  90%|████████▉ | 179/200 [00:06<00:00, 24.14it/s]2025-06-05 01:52:52,711 - INFO - Epoch 180, Loss: 0.0045, Best: 0.0039, LR: 0.000500\n",
      "Training:  91%|█████████ | 182/200 [00:06<00:00, 25.10it/s]2025-06-05 01:52:52,864 - INFO - Epoch 184: Learning rate reduced from 0.000500 to 0.000250\n",
      "Training: 100%|██████████| 200/200 [00:07<00:00, 26.30it/s]\n",
      "2025-06-05 01:52:53,454 - INFO - 학습 완료! 최종 손실: 0.0039\n",
      "2025-06-05 01:52:53,455 - INFO - 💾 데이터 저장 중...\n",
      "2025-06-05 01:52:53,455 - INFO - 처리된 데이터 저장 시작...\n",
      "2025-06-05 01:52:53,524 - INFO - 데이터 저장 완료: ./pickle/제주도 및 도서지역/\n",
      "2025-06-05 01:52:53,524 - INFO - 저장된 데이터 구조:\n",
      "2025-06-05 01:52:53,525 - INFO -   - visit_area_df: (51596, 26)\n",
      "2025-06-05 01:52:53,525 - INFO -   - graph_data: 노드 523개, 엣지 5398개\n",
      "2025-06-05 01:52:53,525 - INFO -   - visit_scaler: RobustScaler\n",
      "2025-06-05 01:52:53,526 - INFO -   - travel_scaler: StandardScaler\n",
      "2025-06-05 01:52:53,526 - INFO -   - id_to_index: 523개 매핑\n",
      "2025-06-05 01:52:53,526 - INFO -   - device: cpu\n",
      "2025-06-05 01:52:53,527 - INFO - \n",
      "============================================================\n",
      "2025-06-05 01:52:53,527 - INFO - ✅ 지역 '제주도 및 도서지역' 학습 완료!\n",
      "2025-06-05 01:52:53,527 - INFO - 📁 모델 저장: ./models/제주도 및 도서지역/\n",
      "2025-06-05 01:52:53,527 - INFO - 📁 데이터 저장: ./pickle/제주도 및 도서지역/\n",
      "2025-06-05 01:52:53,528 - INFO - 📊 최종 손실: 0.0039\n",
      "2025-06-05 01:52:53,528 - INFO - 📊 총 에포크: 200\n",
      "2025-06-05 01:52:53,528 - INFO - 🎯 노드 수: 523, 엣지 수: 5398\n",
      "2025-06-05 01:52:53,528 - INFO - ============================================================\n",
      "2025-06-05 01:52:53,537 - INFO - ✅ 지역 '제주도 및 도서지역' 처리 완료!\n",
      "2025-06-05 01:52:53,537 - INFO - \n",
      "🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-05 01:52:53,538 - INFO - 📍 [4/4] 지역 '수도권' 처리 시작!\n",
      "2025-06-05 01:52:53,538 - INFO - 🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟\n",
      "2025-06-05 01:52:53,538 - INFO - \n",
      "============================================================\n",
      "2025-06-05 01:52:53,538 - INFO - 🚀 지역 '수도권' GNN 모델 학습 시작!\n",
      "2025-06-05 01:52:53,539 - INFO - ============================================================\n",
      "2025-06-05 01:52:53,540 - INFO - 📂 데이터 로드 중...\n",
      "2025-06-05 01:52:53,642 - INFO - 🔍 데이터 검증 중...\n",
      "2025-06-05 01:52:53,649 - INFO - ✅ 여행정보 데이터 로드: 2880개 레코드\n",
      "2025-06-05 01:52:53,649 - INFO - ✅ 데이터 로드 및 검증 완료:\n",
      "2025-06-05 01:52:53,649 - INFO -   - 이동내역: 24154개 레코드 (유효 이동: 24154개)\n",
      "2025-06-05 01:52:53,650 - INFO -   - 방문지: 24154개 레코드 (유효 방문지: 24154개)\n",
      "2025-06-05 01:52:53,650 - INFO - 🔧 데이터 전처리 중...\n",
      "2025-06-05 01:52:53,650 - INFO - 데이터 준비 시작...\n",
      "2025-06-05 01:52:53,650 - INFO - 엣지 생성 시작...\n",
      "2025-06-05 01:52:53,652 - INFO - 방문지 데이터에서 사용 가능한 ID: 10863개\n",
      "2025-06-05 01:52:53,657 - INFO - 최종 사용할 방문지 ID: 916개\n",
      "2025-06-05 01:52:53,657 - INFO - 방문지 ID 매핑 생성: 916개\n",
      "Processing travel groups: 100%|██████████| 2880/2880 [00:01<00:00, 1798.51it/s]\n",
      "2025-06-05 01:52:55,322 - INFO - 엣지 생성 통계:\n",
      "2025-06-05 01:52:55,323 - INFO -   - 총 이동 시도: 21274개\n",
      "2025-06-05 01:52:55,323 - INFO -   - 유효한 엣지: 10049개\n",
      "2025-06-05 01:52:55,323 - INFO -   - TRIP_ID 누락: 0개\n",
      "2025-06-05 01:52:55,323 - INFO -   - 유효하지 않은 FROM_ID: 6633개\n",
      "2025-06-05 01:52:55,324 - INFO -   - 유효하지 않은 TO_ID: 4016개\n",
      "2025-06-05 01:52:55,339 - INFO - ✅ 엣지 생성 완료:\n",
      "2025-06-05 01:52:55,339 - INFO -   - 최종 엣지 수: 10049개\n",
      "2025-06-05 01:52:55,340 - INFO -   - 노드 인덱스 범위: 0 ~ 915\n",
      "2025-06-05 01:52:55,340 - INFO -   - 엣지 인덱스 최대값: 915\n",
      "2025-06-05 01:52:55,350 - INFO - NEW_VISIT_AREA_ID가 있는 방문지: 24154개\n",
      "2025-06-05 01:52:55,355 - INFO - ID 매핑에 있는 방문지: 9641개\n",
      "2025-06-05 01:52:55,355 - INFO - 중복된 방문지 ID 집계 중...\n",
      "2025-06-05 01:52:55,360 - INFO - 집계 후 유니크 방문지: 916개\n",
      "2025-06-05 01:52:55,373 - INFO - 최종 정렬된 방문지: 916개\n",
      "2025-06-05 01:52:55,381 - INFO - 방문지 특성 처리 완료: (916, 36)\n",
      "2025-06-05 01:52:55,387 - INFO - 학습 타겟 생성 시작...\n",
      "2025-06-05 01:52:55,393 - INFO - 중복된 방문지 타겟 정보 집계 중...\n",
      "2025-06-05 01:52:55,399 - INFO - 집계된 유니크 방문지: 10863개\n",
      "2025-06-05 01:52:55,402 - INFO - 방문 빈도 계산 완료: 1142개 방문지, 최대 방문 3809회\n",
      "2025-06-05 01:52:55,419 - INFO - 타겟 생성 완료: 916개\n",
      "2025-06-05 01:52:55,420 - INFO - 타겟 점수 범위: 0.000 ~ 0.644\n",
      "2025-06-05 01:52:55,422 - INFO - 여행 컨텍스트 처리 시작...\n",
      "2025-06-05 01:52:55,428 - INFO - ✅ 데이터 준비 완료:\n",
      "2025-06-05 01:52:55,428 - INFO -   - 노드 수: 916\n",
      "2025-06-05 01:52:55,429 - INFO -   - 특성 수: 36\n",
      "2025-06-05 01:52:55,429 - INFO -   - 엣지 수: 10049\n",
      "2025-06-05 01:52:55,429 - INFO -   - 여행 컨텍스트: (2880, 21)\n",
      "2025-06-05 01:52:55,429 - INFO -   - ID 매핑 수: 916\n",
      "2025-06-05 01:52:55,430 - INFO -   - 타겟 수: 916\n",
      "2025-06-05 01:52:55,430 - INFO -   - 엣지 인덱스 범위: 0 ~ 915\n",
      "2025-06-05 01:52:55,430 - INFO - 🎯 모델 학습 시작...\n",
      "2025-06-05 01:52:55,430 - INFO - 모델 학습 시작...\n",
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]2025-06-05 01:52:55,533 - INFO - Epoch 000, Loss: 0.0602, Best: 0.0602, LR: 0.001000\n",
      "Training:  10%|█         | 20/200 [00:01<00:14, 12.62it/s]2025-06-05 01:52:57,124 - INFO - Epoch 020, Loss: 0.0166, Best: 0.0162, LR: 0.001000\n",
      "Training:  20%|██        | 40/200 [00:03<00:12, 13.05it/s]2025-06-05 01:52:58,511 - INFO - Epoch 040, Loss: 0.0138, Best: 0.0138, LR: 0.001000\n",
      "Training:  30%|███       | 60/200 [00:04<00:09, 14.57it/s]2025-06-05 01:52:59,969 - INFO - Epoch 060, Loss: 0.0121, Best: 0.0121, LR: 0.001000\n",
      "Training:  40%|████      | 80/200 [00:05<00:06, 17.15it/s]2025-06-05 01:53:01,153 - INFO - Epoch 080, Loss: 0.0113, Best: 0.0106, LR: 0.001000\n",
      "Training:  50%|█████     | 100/200 [00:06<00:05, 18.32it/s]2025-06-05 01:53:02,244 - INFO - Epoch 100, Loss: 0.0108, Best: 0.0100, LR: 0.001000\n",
      "Training:  60%|██████    | 120/200 [00:07<00:04, 18.44it/s]2025-06-05 01:53:03,336 - INFO - Epoch 120, Loss: 0.0104, Best: 0.0095, LR: 0.001000\n",
      "Training:  70%|███████   | 140/200 [00:09<00:03, 17.20it/s]2025-06-05 01:53:04,558 - INFO - Epoch 140, Loss: 0.0097, Best: 0.0087, LR: 0.001000\n",
      "Training:  80%|████████  | 160/200 [00:10<00:02, 14.84it/s]2025-06-05 01:53:05,915 - INFO - Epoch 160, Loss: 0.0089, Best: 0.0082, LR: 0.001000\n",
      "Training:  90%|█████████ | 180/200 [00:11<00:01, 14.26it/s]2025-06-05 01:53:07,308 - INFO - Epoch 180, Loss: 0.0088, Best: 0.0081, LR: 0.001000\n",
      "Training: 100%|██████████| 200/200 [00:13<00:00, 15.06it/s]\n",
      "2025-06-05 01:53:08,720 - INFO - 학습 완료! 최종 손실: 0.0073\n",
      "2025-06-05 01:53:08,721 - INFO - 💾 데이터 저장 중...\n",
      "2025-06-05 01:53:08,721 - INFO - 처리된 데이터 저장 시작...\n",
      "2025-06-05 01:53:08,741 - INFO - 데이터 저장 완료: ./pickle/수도권/\n",
      "2025-06-05 01:53:08,741 - INFO - 저장된 데이터 구조:\n",
      "2025-06-05 01:53:08,741 - INFO -   - visit_area_df: (24154, 26)\n",
      "2025-06-05 01:53:08,742 - INFO -   - graph_data: 노드 916개, 엣지 10049개\n",
      "2025-06-05 01:53:08,742 - INFO -   - visit_scaler: RobustScaler\n",
      "2025-06-05 01:53:08,742 - INFO -   - travel_scaler: StandardScaler\n",
      "2025-06-05 01:53:08,742 - INFO -   - id_to_index: 916개 매핑\n",
      "2025-06-05 01:53:08,743 - INFO -   - device: cpu\n",
      "2025-06-05 01:53:08,743 - INFO - \n",
      "============================================================\n",
      "2025-06-05 01:53:08,743 - INFO - ✅ 지역 '수도권' 학습 완료!\n",
      "2025-06-05 01:53:08,744 - INFO - 📁 모델 저장: ./models/수도권/\n",
      "2025-06-05 01:53:08,744 - INFO - 📁 데이터 저장: ./pickle/수도권/\n",
      "2025-06-05 01:53:08,745 - INFO - 📊 최종 손실: 0.0073\n",
      "2025-06-05 01:53:08,745 - INFO - 📊 총 에포크: 200\n",
      "2025-06-05 01:53:08,745 - INFO - 🎯 노드 수: 916, 엣지 수: 10049\n",
      "2025-06-05 01:53:08,745 - INFO - ============================================================\n",
      "2025-06-05 01:53:08,752 - INFO - ✅ 지역 '수도권' 처리 완료!\n",
      "2025-06-05 01:53:08,753 - INFO - \n",
      "🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁\n",
      "2025-06-05 01:53:08,753 - INFO - 🏁 전체 지역 처리 완료!\n",
      "2025-06-05 01:53:08,754 - INFO - 🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁\n",
      "2025-06-05 01:53:08,754 - INFO - ✅ 성공한 지역 (4개): ['서부권', '동부권', '제주도 및 도서지역', '수도권']\n",
      "2025-06-05 01:53:08,754 - INFO - \n",
      "📂 저장된 파일 구조:\n",
      "2025-06-05 01:53:08,755 - INFO -   📁 서부권/\n",
      "2025-06-05 01:53:08,755 - INFO -     🤖 models/서부권/improved_travel_recommendation_model.pt\n",
      "2025-06-05 01:53:08,755 - INFO -     💾 pickle/서부권/improved_travel_data.pkl\n",
      "2025-06-05 01:53:08,755 - INFO -   📁 동부권/\n",
      "2025-06-05 01:53:08,755 - INFO -     🤖 models/동부권/improved_travel_recommendation_model.pt\n",
      "2025-06-05 01:53:08,756 - INFO -     💾 pickle/동부권/improved_travel_data.pkl\n",
      "2025-06-05 01:53:08,756 - INFO -   📁 제주도 및 도서지역/\n",
      "2025-06-05 01:53:08,756 - INFO -     🤖 models/제주도 및 도서지역/improved_travel_recommendation_model.pt\n",
      "2025-06-05 01:53:08,756 - INFO -     💾 pickle/제주도 및 도서지역/improved_travel_data.pkl\n",
      "2025-06-05 01:53:08,756 - INFO -   📁 수도권/\n",
      "2025-06-05 01:53:08,756 - INFO -     🤖 models/수도권/improved_travel_recommendation_model.pt\n",
      "2025-06-05 01:53:08,757 - INFO -     💾 pickle/수도권/improved_travel_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import HeteroData\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnhancedDataProcessor:\n",
    "    \"\"\"데이터 전처리를 위한 클래스\"\"\"\n",
    "    def __init__(self):\n",
    "        self.visit_scaler = RobustScaler()\n",
    "        self.travel_scaler = StandardScaler()\n",
    "        # 제외할 키워드 목록\n",
    "        self.exclude_keywords = {\n",
    "            '역', '터미널', '공항', '휴게소', '정류장', '톨게이트', '교차로', '출구', '입구',\n",
    "            'IC', 'JC', '나들목', '분기점', '요금소', '주차장', '주유소', '충전소',\n",
    "            '아파트', '원룸', '오피스텔', '빌라', '주택', '빌딩', '상가', '모텔', '집', \n",
    "            '교직원', '하나로마트', '마트'\n",
    "        }\n",
    "        \n",
    "    def should_exclude_location(self, name):\n",
    "        \"\"\"위치를 제외해야 하는지 확인\"\"\"\n",
    "        if pd.isna(name):\n",
    "            return False\n",
    "        name_str = str(name).lower()\n",
    "        \n",
    "        for keyword in self.exclude_keywords:\n",
    "            if keyword.lower() in name_str:\n",
    "                # 예외 처리: 관광지로서의 역할이 있는 경우\n",
    "                tourist_keywords = {'관광', '테마', '파크', '랜드', '월드', '호텔',\n",
    "                                  '맛집', '식당', '카페', '박물관', '전시', '갤러리', '문화'}\n",
    "                if any(tk in name_str for tk in tourist_keywords) and keyword != '아파트':\n",
    "                    continue\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def process_visit_area_features(self, visit_area_df, id_to_index=None):\n",
    "        \"\"\"방문지 특성 처리 - 중복 ID 처리 포함\"\"\"\n",
    "        visit_area_df = visit_area_df.copy()\n",
    "        \n",
    "        # 1. 유효한 NEW_VISIT_AREA_ID만 필터링\n",
    "        if 'NEW_VISIT_AREA_ID' in visit_area_df.columns:\n",
    "            valid_rows = visit_area_df['NEW_VISIT_AREA_ID'].notna()\n",
    "            visit_area_df = visit_area_df[valid_rows].copy()\n",
    "            logger.info(f\"NEW_VISIT_AREA_ID가 있는 방문지: {len(visit_area_df)}개\")\n",
    "        else:\n",
    "            logger.error(\"NEW_VISIT_AREA_ID 컬럼이 존재하지 않습니다!\")\n",
    "            raise ValueError(\"NEW_VISIT_AREA_ID 컬럼이 필요합니다.\")\n",
    "        \n",
    "        # 2. ID 매핑이 제공된 경우, 해당 ID만 필터링하고 중복 처리\n",
    "        if id_to_index is not None:\n",
    "            visit_area_df['NEW_VISIT_AREA_ID'] = visit_area_df['NEW_VISIT_AREA_ID'].astype(int)\n",
    "            \n",
    "            # id_to_index에 있는 ID만 필터링\n",
    "            valid_ids = set(id_to_index.keys())\n",
    "            mask = visit_area_df['NEW_VISIT_AREA_ID'].isin(valid_ids)\n",
    "            visit_area_df = visit_area_df[mask].copy()\n",
    "            \n",
    "            logger.info(f\"ID 매핑에 있는 방문지: {len(visit_area_df)}개\")\n",
    "            \n",
    "            # 🔧 중복 처리: 각 NEW_VISIT_AREA_ID별로 집계\n",
    "            logger.info(\"중복된 방문지 ID 집계 중...\")\n",
    "            \n",
    "            # 숫자형 컬럼들의 평균 계산\n",
    "            numeric_cols = ['X_COORD', 'Y_COORD', 'DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']\n",
    "            agg_dict = {}\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                if col in visit_area_df.columns:\n",
    "                    agg_dict[col] = 'mean'  # 평균값 사용\n",
    "            \n",
    "            # 범주형 컬럼들의 최빈값 또는 첫 번째 값 사용\n",
    "            categorical_cols = ['VISIT_AREA_NM', 'VISIT_AREA_TYPE_CD', 'VISIT_CHC_REASON_CD']\n",
    "            for col in categorical_cols:\n",
    "                if col in visit_area_df.columns:\n",
    "                    agg_dict[col] = 'first'  # 첫 번째 값 사용\n",
    "            \n",
    "            # 집계 수행\n",
    "            aggregated_df = visit_area_df.groupby('NEW_VISIT_AREA_ID').agg(agg_dict).reset_index()\n",
    "            \n",
    "            logger.info(f\"집계 후 유니크 방문지: {len(aggregated_df)}개\")\n",
    "            \n",
    "            # id_to_index의 순서대로 정렬\n",
    "            ordered_ids = sorted(id_to_index.keys())\n",
    "            existing_ids = [id for id in ordered_ids if id in aggregated_df['NEW_VISIT_AREA_ID'].values]\n",
    "            \n",
    "            # 정렬된 순서로 재배열\n",
    "            id_to_idx_map = {id: i for i, id in enumerate(aggregated_df['NEW_VISIT_AREA_ID'])}\n",
    "            aggregated_df['sort_order'] = aggregated_df['NEW_VISIT_AREA_ID'].map(\n",
    "                lambda x: existing_ids.index(x) if x in existing_ids else 999999\n",
    "            )\n",
    "            aggregated_df = aggregated_df.sort_values('sort_order').drop('sort_order', axis=1).reset_index(drop=True)\n",
    "            \n",
    "            visit_area_df = aggregated_df\n",
    "            logger.info(f\"최종 정렬된 방문지: {len(visit_area_df)}개\")\n",
    "        \n",
    "        # 3. 기본 컬럼 존재 여부 확인 및 결측치 처리\n",
    "        required_cols = ['X_COORD', 'Y_COORD', 'VISIT_AREA_NM']\n",
    "        for col in required_cols:\n",
    "            if col not in visit_area_df.columns:\n",
    "                logger.warning(f\"필수 컬럼 {col}이 없습니다. 기본값으로 대체합니다.\")\n",
    "                if col in ['X_COORD', 'Y_COORD']:\n",
    "                    visit_area_df[col] = 0.0\n",
    "                else:\n",
    "                    visit_area_df[col] = '알 수 없음'\n",
    "        \n",
    "        # 좌표 결측치 처리\n",
    "        visit_area_df['X_COORD'] = pd.to_numeric(visit_area_df['X_COORD'], errors='coerce')\n",
    "        visit_area_df['Y_COORD'] = pd.to_numeric(visit_area_df['Y_COORD'], errors='coerce')\n",
    "        visit_area_df['X_COORD'] = visit_area_df['X_COORD'].fillna(visit_area_df['X_COORD'].mean())\n",
    "        visit_area_df['Y_COORD'] = visit_area_df['Y_COORD'].fillna(visit_area_df['Y_COORD'].mean())\n",
    "        \n",
    "        # VISIT_CHC_REASON_CD 처리\n",
    "        if 'VISIT_CHC_REASON_CD' in visit_area_df.columns:\n",
    "            visit_area_df['VISIT_CHC_REASON_CD'] = pd.to_numeric(visit_area_df['VISIT_CHC_REASON_CD'], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            visit_area_df['VISIT_CHC_REASON_CD'] = 0\n",
    "        \n",
    "        features = visit_area_df[['X_COORD', 'Y_COORD']].copy()\n",
    "        \n",
    "        # One-hot encoding (안전하게 처리)\n",
    "        if 'VISIT_AREA_TYPE_CD' in visit_area_df.columns:\n",
    "            type_onehot = pd.get_dummies(visit_area_df['VISIT_AREA_TYPE_CD'], prefix='type')\n",
    "        else:\n",
    "            type_onehot = pd.DataFrame({'type_unknown': [1] * len(visit_area_df)})\n",
    "            \n",
    "        reason_onehot = pd.get_dummies(visit_area_df['VISIT_CHC_REASON_CD'], prefix='reason')\n",
    "        \n",
    "        # 만족도 점수 처리 (안전하게)\n",
    "        satisfaction_cols = ['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']\n",
    "        for col in satisfaction_cols:\n",
    "            if col in visit_area_df.columns:\n",
    "                visit_area_df[col] = pd.to_numeric(visit_area_df[col], errors='coerce').fillna(3)\n",
    "            else:\n",
    "                visit_area_df[col] = 3  # 기본값\n",
    "            visit_area_df[f'{col}_norm'] = (visit_area_df[col] - 1) / 4.0\n",
    "        \n",
    "        # 인기도 점수\n",
    "        visit_area_df['popularity_score'] = (\n",
    "            visit_area_df['DGSTFN_norm'] * 0.4 + \n",
    "            visit_area_df['REVISIT_INTENTION_norm'] * 0.3 + \n",
    "            visit_area_df['RCMDTN_INTENTION_norm'] * 0.3\n",
    "        )\n",
    "        \n",
    "        # 제외할 장소에 대한 페널티 추가\n",
    "        exclude_penalty = visit_area_df['VISIT_AREA_NM'].apply(self.should_exclude_location).astype(float) * -0.5\n",
    "        \n",
    "        # 모든 특성 결합\n",
    "        features = pd.concat([\n",
    "            features, type_onehot, reason_onehot,\n",
    "            visit_area_df[['DGSTFN_norm', 'REVISIT_INTENTION_norm', 'RCMDTN_INTENTION_norm', 'popularity_score']],\n",
    "            pd.DataFrame({'exclude_penalty': exclude_penalty})\n",
    "        ], axis=1)\n",
    "        \n",
    "        logger.info(f\"방문지 특성 처리 완료: {features.shape}\")\n",
    "        return self.visit_scaler.fit_transform(features.values.astype(np.float32))\n",
    "    \n",
    "    def create_enhanced_edges(self, move_df, visit_area_df):\n",
    "        \"\"\"향상된 엣지 생성 - 방문지 데이터에 존재하는 ID만 사용\"\"\"\n",
    "        edges = []\n",
    "        edge_weights = []\n",
    "        \n",
    "        logger.info(\"엣지 생성 시작...\")\n",
    "        \n",
    "        # 1. 방문지 데이터에서 실제 존재하는 유효한 NEW_VISIT_AREA_ID만 수집\n",
    "        available_visit_ids = set()\n",
    "        if 'NEW_VISIT_AREA_ID' in visit_area_df.columns:\n",
    "            available_from_visit = visit_area_df['NEW_VISIT_AREA_ID'].dropna().astype(int)\n",
    "            available_visit_ids.update(available_from_visit)\n",
    "        \n",
    "        logger.info(f\"방문지 데이터에서 사용 가능한 ID: {len(available_visit_ids)}개\")\n",
    "        \n",
    "        # 2. 이동 데이터에서 유효한 ID 수집 (단, 방문지 데이터에 존재하는 것만)\n",
    "        valid_visit_ids = set()\n",
    "        \n",
    "        # START_VISIT_AREA_ID 확인\n",
    "        if 'NEW_START_VISIT_AREA_ID' in move_df.columns:\n",
    "            start_ids = move_df['NEW_START_VISIT_AREA_ID'].dropna().astype(int)\n",
    "            # 방문지 데이터에 존재하는 것만 추가\n",
    "            valid_start_ids = start_ids[start_ids.isin(available_visit_ids)]\n",
    "            valid_visit_ids.update(valid_start_ids)\n",
    "        \n",
    "        # END_VISIT_AREA_ID 확인\n",
    "        if 'NEW_END_VISIT_AREA_ID' in move_df.columns:\n",
    "            end_ids = move_df['NEW_END_VISIT_AREA_ID'].dropna().astype(int)\n",
    "            # 방문지 데이터에 존재하는 것만 추가\n",
    "            valid_end_ids = end_ids[end_ids.isin(available_visit_ids)]\n",
    "            valid_visit_ids.update(valid_end_ids)\n",
    "        \n",
    "        # 3. 최종 유효 ID 목록 (방문지 데이터와 이동 데이터 모두에 존재하는 것)\n",
    "        final_valid_ids = available_visit_ids.intersection(valid_visit_ids)\n",
    "        final_valid_ids = sorted(list(final_valid_ids))\n",
    "        id_to_index = {visit_id: idx for idx, visit_id in enumerate(final_valid_ids)}\n",
    "        \n",
    "        logger.info(f\"최종 사용할 방문지 ID: {len(final_valid_ids)}개\")\n",
    "        logger.info(f\"방문지 ID 매핑 생성: {len(id_to_index)}개\")\n",
    "        \n",
    "        # 4. 통계 추적 변수\n",
    "        total_moves = 0\n",
    "        valid_edges = 0\n",
    "        invalid_from_id = 0\n",
    "        invalid_to_id = 0\n",
    "        missing_trip_id = 0\n",
    "        \n",
    "        # 5. 엣지 생성\n",
    "        for travel_id, group in tqdm(move_df.groupby(\"TRAVEL_ID\"), desc=\"Processing travel groups\"):\n",
    "            group = group.sort_values(\"TRIP_ID\").reset_index(drop=True)\n",
    "            \n",
    "            for i in range(1, len(group)):\n",
    "                total_moves += 1\n",
    "                \n",
    "                # 이전 행과 현재 행에서 방문지 ID 추출\n",
    "                prev_row = group.loc[i-1]\n",
    "                curr_row = group.loc[i]\n",
    "                \n",
    "                # 다양한 방법으로 FROM/TO ID 추출 시도\n",
    "                from_id = None\n",
    "                to_id = None\n",
    "                \n",
    "                # 방법 1: NEW_END_VISIT_AREA_ID 사용 (가장 우선)\n",
    "                if pd.notna(prev_row.get('NEW_END_VISIT_AREA_ID')):\n",
    "                    from_id = int(prev_row['NEW_END_VISIT_AREA_ID'])\n",
    "                elif pd.notna(prev_row.get('NEW_START_VISIT_AREA_ID')):\n",
    "                    from_id = int(prev_row['NEW_START_VISIT_AREA_ID'])\n",
    "                \n",
    "                if pd.notna(curr_row.get('NEW_END_VISIT_AREA_ID')):\n",
    "                    to_id = int(curr_row['NEW_END_VISIT_AREA_ID'])\n",
    "                elif pd.notna(curr_row.get('NEW_START_VISIT_AREA_ID')):\n",
    "                    to_id = int(curr_row['NEW_START_VISIT_AREA_ID'])\n",
    "                \n",
    "                # ID 유효성 검사 및 인덱스 변환\n",
    "                if from_id is None or to_id is None:\n",
    "                    missing_trip_id += 1\n",
    "                    continue\n",
    "                \n",
    "                if from_id not in id_to_index:\n",
    "                    invalid_from_id += 1\n",
    "                    continue\n",
    "                    \n",
    "                if to_id not in id_to_index:\n",
    "                    invalid_to_id += 1\n",
    "                    continue\n",
    "                \n",
    "                # 유효한 엣지인 경우 추가\n",
    "                from_idx = id_to_index[from_id]\n",
    "                to_idx = id_to_index[to_id]\n",
    "                \n",
    "                # 같은 노드로의 self-loop 제외\n",
    "                if from_idx == to_idx:\n",
    "                    continue\n",
    "                \n",
    "                duration = curr_row.get(\"DURATION_MINUTES\", 0) if \"DURATION_MINUTES\" in curr_row else 0\n",
    "                transport = curr_row.get(\"MVMN_CD_1\", 0) if \"MVMN_CD_1\" in curr_row else 0\n",
    "                \n",
    "                edges.append([from_idx, to_idx, duration, transport])\n",
    "                edge_weights.append(1.0)\n",
    "                valid_edges += 1\n",
    "        \n",
    "        # 6. 통계 출력\n",
    "        logger.info(f\"엣지 생성 통계:\")\n",
    "        logger.info(f\"  - 총 이동 시도: {total_moves}개\")\n",
    "        logger.info(f\"  - 유효한 엣지: {valid_edges}개\")\n",
    "        logger.info(f\"  - TRIP_ID 누락: {missing_trip_id}개\")\n",
    "        logger.info(f\"  - 유효하지 않은 FROM_ID: {invalid_from_id}개\")\n",
    "        logger.info(f\"  - 유효하지 않은 TO_ID: {invalid_to_id}개\")\n",
    "        \n",
    "        if not edges:\n",
    "            logger.warning(\"❌ 유효한 엣지가 생성되지 않았습니다!\")\n",
    "            return None, None, None\n",
    "            \n",
    "        edges_df = pd.DataFrame(edges, columns=[\"FROM_ID\", \"TO_ID\", \"DURATION_MINUTES\", \"MVMN_CD_1\"])\n",
    "        \n",
    "        # 교통수단 분류\n",
    "        edges_df[\"MVMN_TYPE\"] = edges_df[\"MVMN_CD_1\"].apply(\n",
    "            lambda code: \"drive\" if code in [1,2,3] else \"public\" if code in [4,5,6,7,8,9,10,11,12,13,50] else \"other\"\n",
    "        )\n",
    "        edges_df[\"is_drive\"] = (edges_df[\"MVMN_TYPE\"] == \"drive\").astype(int)\n",
    "        edges_df[\"is_public\"] = (edges_df[\"MVMN_TYPE\"] == \"public\").astype(int)\n",
    "        edges_df[\"is_other\"] = (edges_df[\"MVMN_TYPE\"] == \"other\").astype(int)\n",
    "        \n",
    "        # 엣지 인덱스와 속성 생성\n",
    "        edge_index = torch.tensor(edges_df[[\"FROM_ID\", \"TO_ID\"]].values.T, dtype=torch.long)\n",
    "        edge_attr = torch.tensor(np.column_stack([\n",
    "            edges_df[[\"DURATION_MINUTES\"]].fillna(0).values,\n",
    "            edges_df[[\"is_drive\", \"is_public\", \"is_other\"]].values,\n",
    "            np.array(edge_weights).reshape(-1, 1)\n",
    "        ]), dtype=torch.float32)\n",
    "        \n",
    "        logger.info(f\"✅ 엣지 생성 완료:\")\n",
    "        logger.info(f\"  - 최종 엣지 수: {len(edges)}개\")\n",
    "        logger.info(f\"  - 노드 인덱스 범위: 0 ~ {len(id_to_index)-1}\")\n",
    "        logger.info(f\"  - 엣지 인덱스 최대값: {edge_index.max().item()}\")\n",
    "        \n",
    "        return edge_index, edge_attr, id_to_index\n",
    "\n",
    "    def process_travel_context(self, travel_df):\n",
    "        \"\"\"여행 컨텍스트 처리\"\"\"\n",
    "        logger.info(\"여행 컨텍스트 처리 시작...\")\n",
    "        \n",
    "        # 필요한 컬럼들\n",
    "        travel_feature_cols = [\n",
    "            'TOTAL_COST_BINNED_ENCODED', 'WITH_PET', 'MONTH', 'DURATION',\n",
    "            'MVMN_기타', 'MVMN_대중교통', 'MVMN_자가용',\n",
    "            'TRAVEL_PURPOSE_1', 'TRAVEL_PURPOSE_2', 'TRAVEL_PURPOSE_3',\n",
    "            'TRAVEL_PURPOSE_4', 'TRAVEL_PURPOSE_5', 'TRAVEL_PURPOSE_6',\n",
    "            'TRAVEL_PURPOSE_7', 'TRAVEL_PURPOSE_8', 'TRAVEL_PURPOSE_9',\n",
    "            'WHOWITH_2인여행', 'WHOWITH_가족여행', 'WHOWITH_기타',\n",
    "            'WHOWITH_단독여행', 'WHOWITH_친구/지인 여행'\n",
    "        ]\n",
    "        \n",
    "        # 누락된 컬럼들을 0으로 초기화\n",
    "        for col in travel_feature_cols:\n",
    "            if col not in travel_df.columns:\n",
    "                travel_df[col] = 0\n",
    "        \n",
    "        travel_features = travel_df[travel_feature_cols].fillna(0)\n",
    "        return self.travel_scaler.fit_transform(travel_features.values.astype(np.float32))\n",
    "\n",
    "\n",
    "class ImprovedTravelGNN(nn.Module):\n",
    "    \"\"\"향상된 여행 추천 GNN 모델\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, travel_context_dim, \n",
    "                 num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # GAT 레이어들\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True, edge_dim=5)\n",
    "        self.gat2 = GATConv(hidden_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True, edge_dim=5)\n",
    "        self.gat3 = GATConv(hidden_channels, out_channels, \n",
    "                           heads=1, dropout=dropout, concat=False, edge_dim=5)\n",
    "        \n",
    "        # Batch Normalization\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # 여행 컨텍스트 인코더\n",
    "        self.travel_encoder = nn.Sequential(\n",
    "            nn.Linear(travel_context_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # 거리 기반 attention\n",
    "        self.distance_attention = nn.Sequential(\n",
    "            nn.Linear(2, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 융합 네트워크\n",
    "        self.fusion_net = nn.Sequential(\n",
    "            nn.Linear(out_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # 선호도 예측 헤드\n",
    "        self.preference_head = nn.Sequential(\n",
    "            nn.Linear(out_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, data, travel_context, return_attention=False):\n",
    "        x = data['visit_area'].x\n",
    "        edge_index = data['visit_area', 'moved_to', 'visit_area'].edge_index\n",
    "        edge_attr = data['visit_area', 'moved_to', 'visit_area'].edge_attr\n",
    "        \n",
    "        # 좌표 정보 추출\n",
    "        coords = x[:, :2]\n",
    "        \n",
    "        # GAT 레이어들\n",
    "        x1 = self.gat1(x, edge_index, edge_attr)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        x2 = self.gat2(x1, edge_index, edge_attr)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2 + x1)  # Residual connection\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        graph_embedding = self.gat3(x2, edge_index, edge_attr)\n",
    "        graph_embedding = self.bn3(graph_embedding)\n",
    "        \n",
    "        # 거리 기반 attention 적용\n",
    "        distance_weights = self.distance_attention(coords)\n",
    "        graph_embedding = graph_embedding * distance_weights\n",
    "        \n",
    "        # 여행 컨텍스트 처리\n",
    "        travel_embedding = self.travel_encoder(travel_context)\n",
    "        travel_embedding_expanded = travel_embedding.expand(graph_embedding.size(0), -1)\n",
    "        \n",
    "        # 특성 융합\n",
    "        fused_features = torch.cat([graph_embedding, travel_embedding_expanded], dim=1)\n",
    "        final_embedding = self.fusion_net(fused_features)\n",
    "        \n",
    "        # 선호도 점수 예측\n",
    "        preference_scores = self.preference_head(final_embedding)\n",
    "        \n",
    "        if return_attention:\n",
    "            return final_embedding, preference_scores, distance_weights\n",
    "        \n",
    "        return final_embedding, preference_scores\n",
    "\n",
    "\n",
    "class TravelDatasetLoader:\n",
    "    \"\"\"여행 데이터셋 로더\"\"\"\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        \n",
    "    def create_training_targets(self, visit_area_df, move_df, id_to_index):\n",
    "        \"\"\"학습 타겟 생성 - 중복 ID 고려한 집계\"\"\"\n",
    "        logger.info(\"학습 타겟 생성 시작...\")\n",
    "        \n",
    "        # 1. 유효한 방문지만 필터링\n",
    "        valid_visit_df = visit_area_df[visit_area_df['NEW_VISIT_AREA_ID'].notna()].copy()\n",
    "        valid_visit_df['NEW_VISIT_AREA_ID'] = valid_visit_df['NEW_VISIT_AREA_ID'].astype(int)\n",
    "        \n",
    "        # 2. 중복된 방문지 정보 집계\n",
    "        logger.info(\"중복된 방문지 타겟 정보 집계 중...\")\n",
    "        \n",
    "        # 만족도 관련 컬럼들의 평균 계산\n",
    "        satisfaction_cols = ['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']\n",
    "        agg_dict = {'VISIT_AREA_NM': 'first'}  # 방문지명은 첫 번째 값\n",
    "        \n",
    "        for col in satisfaction_cols:\n",
    "            if col in valid_visit_df.columns:\n",
    "                agg_dict[col] = 'mean'  # 만족도는 평균값\n",
    "            else:\n",
    "                valid_visit_df[col] = 3  # 기본값\n",
    "                agg_dict[col] = 'mean'\n",
    "        \n",
    "        # 각 NEW_VISIT_AREA_ID별로 집계\n",
    "        aggregated_visit_df = valid_visit_df.groupby('NEW_VISIT_AREA_ID').agg(agg_dict).reset_index()\n",
    "        \n",
    "        logger.info(f\"집계된 유니크 방문지: {len(aggregated_visit_df)}개\")\n",
    "        \n",
    "        # 3. 방문 빈도 계산 (실제 ID 기준, 유효한 것만)\n",
    "        valid_move_df = move_df.copy()\n",
    "        visit_counts = {}\n",
    "        \n",
    "        # END_VISIT_AREA_ID에서 방문 빈도 계산\n",
    "        if 'NEW_END_VISIT_AREA_ID' in valid_move_df.columns:\n",
    "            end_counts = valid_move_df['NEW_END_VISIT_AREA_ID'].dropna().astype(int).value_counts()\n",
    "            visit_counts.update(end_counts.to_dict())\n",
    "        \n",
    "        # START_VISIT_AREA_ID에서도 방문 빈도 계산 (있다면)\n",
    "        if 'NEW_START_VISIT_AREA_ID' in valid_move_df.columns:\n",
    "            start_counts = valid_move_df['NEW_START_VISIT_AREA_ID'].dropna().astype(int).value_counts()\n",
    "            for id, count in start_counts.items():\n",
    "                visit_counts[id] = visit_counts.get(id, 0) + count\n",
    "        \n",
    "        max_visit_count = max(visit_counts.values()) if visit_counts else 1\n",
    "        logger.info(f\"방문 빈도 계산 완료: {len(visit_counts)}개 방문지, 최대 방문 {max_visit_count}회\")\n",
    "        \n",
    "        # 4. 각 방문지에 대한 타겟 점수 계산 (id_to_index 순서로)\n",
    "        targets = []\n",
    "        missing_ids = []\n",
    "        \n",
    "        # aggregated_visit_df를 딕셔너리로 변환하여 빠른 접근\n",
    "        visit_dict = aggregated_visit_df.set_index('NEW_VISIT_AREA_ID').to_dict('index')\n",
    "        \n",
    "        # id_to_index의 순서대로 타겟 생성\n",
    "        for visit_id in sorted(id_to_index.keys()):\n",
    "            if visit_id in visit_dict:\n",
    "                row_data = visit_dict[visit_id]\n",
    "                \n",
    "                # 방문 빈도 (정규화)\n",
    "                visit_freq = visit_counts.get(visit_id, 0)\n",
    "                freq_score = min(visit_freq / max_visit_count, 1.0)\n",
    "                \n",
    "                # 만족도 점수 (안전하게 처리)\n",
    "                dgstfn = row_data.get('DGSTFN', 3)\n",
    "                revisit = row_data.get('REVISIT_INTENTION', 3)\n",
    "                recommend = row_data.get('RCMDTN_INTENTION', 3)\n",
    "                \n",
    "                # None이나 NaN인 경우 기본값 사용\n",
    "                dgstfn = dgstfn if pd.notna(dgstfn) else 3\n",
    "                revisit = revisit if pd.notna(revisit) else 3\n",
    "                recommend = recommend if pd.notna(recommend) else 3\n",
    "                \n",
    "                satisfaction = (dgstfn * 0.4 + revisit * 0.3 + recommend * 0.3) / 5.0\n",
    "                \n",
    "                # 제외 장소에 대한 페널티\n",
    "                area_name = row_data.get('VISIT_AREA_NM', '')\n",
    "                exclude_penalty = 0.3 if self.processor.should_exclude_location(area_name) else 0\n",
    "                \n",
    "                # 최종 타겟 점수\n",
    "                final_score = (freq_score * 0.6 + satisfaction * 0.4) - exclude_penalty\n",
    "                final_score = max(0, min(final_score, 1))  # 0-1 범위로 클램핑\n",
    "                \n",
    "                targets.append(final_score)\n",
    "            else:\n",
    "                # 해당 ID가 visit_area_df에 없는 경우\n",
    "                missing_ids.append(visit_id)\n",
    "                # 기본 점수: 방문 빈도만 고려\n",
    "                visit_freq = visit_counts.get(visit_id, 0)\n",
    "                freq_score = min(visit_freq / max_visit_count, 1.0)\n",
    "                targets.append(freq_score * 0.5)  # 낮은 기본 점수\n",
    "        \n",
    "        if missing_ids:\n",
    "            logger.warning(f\"방문지 정보가 없는 ID {len(missing_ids)}개: {missing_ids[:10]}...\")\n",
    "        \n",
    "        logger.info(f\"타겟 생성 완료: {len(targets)}개\")\n",
    "        logger.info(f\"타겟 점수 범위: {min(targets):.3f} ~ {max(targets):.3f}\")\n",
    "        \n",
    "        return torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class TravelRecommendationTrainer:\n",
    "    \"\"\"여행 추천 모델 학습기\"\"\"\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.processor = EnhancedDataProcessor()\n",
    "        self.dataset_loader = TravelDatasetLoader(self.processor)\n",
    "        \n",
    "    def prepare_data(self, visit_area_df, move_df, travel_df=None):\n",
    "        \"\"\"데이터 준비 - 매핑 실패 및 예외 상황 고려\"\"\"\n",
    "        logger.info(\"데이터 준비 시작...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. 먼저 엣지 생성하여 ID 매핑 얻기\n",
    "            edge_index, edge_attr, id_to_index = self.processor.create_enhanced_edges(move_df, visit_area_df)\n",
    "            \n",
    "            if edge_index is None or id_to_index is None or len(id_to_index) == 0:\n",
    "                logger.error(\"엣지 생성에 실패했거나 유효한 ID 매핑이 없습니다!\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # 2. ID 매핑을 사용하여 방문지 특성 처리\n",
    "            visit_features = self.processor.process_visit_area_features(visit_area_df, id_to_index)\n",
    "            \n",
    "            if visit_features is None or len(visit_features) == 0:\n",
    "                logger.error(\"방문지 특성 처리에 실패했습니다!\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # 3. 노드 수와 ID 매핑 수가 일치하는지 확인\n",
    "            expected_nodes = len(id_to_index)\n",
    "            actual_nodes = visit_features.shape[0]\n",
    "            \n",
    "            if expected_nodes != actual_nodes:\n",
    "                logger.error(f\"노드 수 불일치: 예상 {expected_nodes}, 실제 {actual_nodes}\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # 4. 헤테로 그래프 데이터 생성\n",
    "            data = HeteroData()\n",
    "            data['visit_area'].x = torch.tensor(visit_features, dtype=torch.float32)\n",
    "            data['visit_area', 'moved_to', 'visit_area'].edge_index = edge_index\n",
    "            data['visit_area', 'moved_to', 'visit_area'].edge_attr = edge_attr\n",
    "            \n",
    "            # 5. 학습 타겟 생성 (ID 매핑 전달)\n",
    "            targets = self.dataset_loader.create_training_targets(visit_area_df, move_df, id_to_index)\n",
    "            \n",
    "            if targets is None or len(targets) == 0:\n",
    "                logger.error(\"학습 타겟 생성에 실패했습니다!\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # 6. 타겟 수와 노드 수 일치 확인\n",
    "            if len(targets) != expected_nodes:\n",
    "                logger.error(f\"타겟 수 불일치: 예상 {expected_nodes}, 실제 {len(targets)}\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # 7. 여행 컨텍스트 처리 (있는 경우)\n",
    "            travel_contexts = None\n",
    "            if travel_df is not None and not travel_df.empty:\n",
    "                try:\n",
    "                    travel_contexts = self.processor.process_travel_context(travel_df)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"여행 컨텍스트 처리 실패: {e}, 기본값 사용\")\n",
    "                    travel_contexts = None\n",
    "            \n",
    "            if travel_contexts is None:\n",
    "                # 기본 여행 컨텍스트 생성\n",
    "                logger.info(\"기본 여행 컨텍스트 생성...\")\n",
    "                travel_contexts = np.zeros((1, 21), dtype=np.float32)  # 21개 특성\n",
    "                travel_contexts[0, 2] = 6  # MONTH = 6 (6월)\n",
    "                travel_contexts[0, 3] = 2  # DURATION = 2일\n",
    "                travel_contexts[0, 5] = 1  # MVMN_대중교통 = 1\n",
    "                travel_contexts[0, 16] = 1  # WHOWITH_2인여행 = 1\n",
    "            \n",
    "            # 8. 최종 검증\n",
    "            num_nodes = data['visit_area'].x.shape[0]\n",
    "            num_edges = data['visit_area', 'moved_to', 'visit_area'].edge_index.shape[1]\n",
    "            num_features = data['visit_area'].x.shape[1]\n",
    "            \n",
    "            # 엣지 인덱스가 노드 범위를 벗어나지 않는지 확인\n",
    "            max_edge_idx = edge_index.max().item()\n",
    "            if max_edge_idx >= num_nodes:\n",
    "                logger.error(f\"엣지 인덱스 오류: 최대 인덱스 {max_edge_idx} >= 노드 수 {num_nodes}\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            logger.info(f\"✅ 데이터 준비 완료:\")\n",
    "            logger.info(f\"  - 노드 수: {num_nodes}\")\n",
    "            logger.info(f\"  - 특성 수: {num_features}\")\n",
    "            logger.info(f\"  - 엣지 수: {num_edges}\")\n",
    "            logger.info(f\"  - 여행 컨텍스트: {travel_contexts.shape}\")\n",
    "            logger.info(f\"  - ID 매핑 수: {len(id_to_index)}\")\n",
    "            logger.info(f\"  - 타겟 수: {len(targets)}\")\n",
    "            logger.info(f\"  - 엣지 인덱스 범위: 0 ~ {max_edge_idx}\")\n",
    "            \n",
    "            return data, targets, travel_contexts, id_to_index\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"데이터 준비 중 오류 발생: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None, None, None, None\n",
    "    \n",
    "    def train_model(self, data, targets, travel_contexts, epochs=200, lr=0.001, \n",
    "                   weight_decay=1e-4, save_path=\"./models/\"):\n",
    "        \"\"\"모델 학습\"\"\"\n",
    "        logger.info(\"모델 학습 시작...\")\n",
    "        \n",
    "        # 디렉토리 생성\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        data = data.to(self.device)\n",
    "        targets = targets.to(self.device)\n",
    "        travel_contexts = torch.tensor(travel_contexts, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # 모델 초기화\n",
    "        in_channels = data['visit_area'].x.shape[1]\n",
    "        hidden_channels = 256\n",
    "        out_channels = 128\n",
    "        travel_context_dim = travel_contexts.shape[1]\n",
    "        \n",
    "        model = ImprovedTravelGNN(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            travel_context_dim=travel_context_dim,\n",
    "            num_heads=4,\n",
    "            dropout=0.2\n",
    "        ).to(self.device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        # 수정된 부분: verbose 매개변수 제거\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=20\n",
    "        )\n",
    "        \n",
    "        # 현재 학습률 추적을 위한 변수\n",
    "        current_lr = lr\n",
    "        \n",
    "        # 학습 루프\n",
    "        model.train()\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        max_patience = 50\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 첫 번째 여행 컨텍스트 사용 (배치 처리 시뮬레이션)\n",
    "            travel_context = travel_contexts[0:1]\n",
    "            \n",
    "            # Forward pass\n",
    "            embeddings, preference_scores = model(data, travel_context)\n",
    "            \n",
    "            # 손실 계산 (MSE)\n",
    "            loss = F.mse_loss(preference_scores.squeeze(), targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # 학습률 스케줄링 (수정된 부분: 학습률 변화 수동 추적)\n",
    "            old_lr = current_lr\n",
    "            scheduler.step(loss)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # 학습률이 변경된 경우 로그 출력\n",
    "            if current_lr != old_lr:\n",
    "                logger.info(f\"Epoch {epoch}: Learning rate reduced from {old_lr:.6f} to {current_lr:.6f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                patience_counter = 0\n",
    "                \n",
    "                # 최고 모델 저장\n",
    "                model_config = {\n",
    "                    'in_channels': in_channels,\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'out_channels': out_channels,\n",
    "                    'travel_context_dim': travel_context_dim,\n",
    "                    'num_heads': 4,\n",
    "                    'dropout': 0.2\n",
    "                }\n",
    "                \n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'model_config': model_config,\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'loss': best_loss,\n",
    "                    'losses': losses\n",
    "                }, os.path.join(save_path, 'improved_travel_recommendation_model.pt'))\n",
    "                \n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= max_patience:\n",
    "                logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "            # 로그 출력\n",
    "            if epoch % 20 == 0:\n",
    "                logger.info(f'Epoch {epoch:03d}, Loss: {loss.item():.4f}, Best: {best_loss:.4f}, LR: {current_lr:.6f}')\n",
    "        \n",
    "        logger.info(f'학습 완료! 최종 손실: {best_loss:.4f}')\n",
    "        \n",
    "        return model, losses\n",
    "    \n",
    "    def save_processed_data(self, visit_area_df, data, id_to_index, save_path=\"./pickle/\"):\n",
    "        \"\"\"처리된 데이터 저장 - 기존 포맷 호환\"\"\"\n",
    "        logger.info(\"처리된 데이터 저장 시작...\")\n",
    "        \n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # 기존 포맷에 맞춘 데이터 저장\n",
    "        save_data = {\n",
    "            'visit_area_df': visit_area_df,\n",
    "            'graph_data': data.cpu(),  # CPU로 이동하여 저장\n",
    "            'visit_scaler': self.processor.visit_scaler,\n",
    "            'travel_scaler': self.processor.travel_scaler,\n",
    "            'device': str(self.device),\n",
    "            # 추가 정보 (기존 코드 호환성 유지하면서 새 기능 제공)\n",
    "            'id_to_index': id_to_index,  # ID 매핑 정보\n",
    "            'region_info': {\n",
    "                'num_nodes': data['visit_area'].x.shape[0],\n",
    "                'num_edges': data['visit_area', 'moved_to', 'visit_area'].edge_index.shape[1],\n",
    "                'num_features': data['visit_area'].x.shape[1]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_path, 'improved_travel_data.pkl'), 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "        \n",
    "        logger.info(f\"데이터 저장 완료: {save_path}\")\n",
    "        logger.info(f\"저장된 데이터 구조:\")\n",
    "        logger.info(f\"  - visit_area_df: {visit_area_df.shape}\")\n",
    "        logger.info(f\"  - graph_data: 노드 {save_data['region_info']['num_nodes']}개, 엣지 {save_data['region_info']['num_edges']}개\")\n",
    "        logger.info(f\"  - visit_scaler: {type(self.processor.visit_scaler).__name__}\")\n",
    "        logger.info(f\"  - travel_scaler: {type(self.processor.travel_scaler).__name__}\")\n",
    "        logger.info(f\"  - id_to_index: {len(id_to_index)}개 매핑\")\n",
    "        logger.info(f\"  - device: {self.device}\")\n",
    "\n",
    "\n",
    "def main_training_pipeline(visit_area_df, move_df, travel_df=None, \n",
    "                          model_save_path=\"./models/\", data_save_path=\"./pickle/\"):\n",
    "    \"\"\"전체 학습 파이프라인 - 매핑 실패 고려\"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"🚀 GNN 여행 추천 시스템 학습 시작!\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # 1. 트레이너 초기화\n",
    "        trainer = TravelRecommendationTrainer()\n",
    "        \n",
    "        # 2. 데이터 준비\n",
    "        data, targets, travel_contexts, id_to_index = trainer.prepare_data(visit_area_df, move_df, travel_df)\n",
    "        \n",
    "        if data is None or targets is None:\n",
    "            logger.error(\"❌ 데이터 준비에 실패했습니다!\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # 3. 모델 학습\n",
    "        model, losses = trainer.train_model(\n",
    "            data, targets, travel_contexts,\n",
    "            epochs=200,\n",
    "            lr=0.001,\n",
    "            save_path=model_save_path\n",
    "        )\n",
    "        \n",
    "        # 4. 처리된 데이터 저장\n",
    "        trainer.save_processed_data(visit_area_df, data, id_to_index, save_path=data_save_path)\n",
    "        \n",
    "        # 5. 학습 결과 요약\n",
    "        logger.info(\"\\n\" + \"=\" * 60)\n",
    "        logger.info(\"✅ 학습 완료!\")\n",
    "        logger.info(f\"📁 모델 저장 경로: {model_save_path}\")\n",
    "        logger.info(f\"📁 데이터 저장 경로: {data_save_path}\")\n",
    "        logger.info(f\"📊 최종 손실: {min(losses):.4f}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        return model, losses, data, targets\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 학습 중 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "def main_region_training(move_path, travel_path, visit_area_path, region_name):\n",
    "    \"\"\"지역별 모델 학습 및 저장 - 매핑 실패 및 데이터 검증 강화\"\"\"\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"🚀 지역 '{region_name}' GNN 모델 학습 시작!\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. 데이터 로드\n",
    "        logger.info(\"📂 데이터 로드 중...\")\n",
    "        \n",
    "        # 파일 존재 확인\n",
    "        if not os.path.exists(move_path):\n",
    "            logger.error(f\"❌ 이동내역 파일을 찾을 수 없습니다: {move_path}\")\n",
    "            return False\n",
    "        if not os.path.exists(visit_area_path):\n",
    "            logger.error(f\"❌ 방문지 파일을 찾을 수 없습니다: {visit_area_path}\")\n",
    "            return False\n",
    "            \n",
    "        # CSV 파일 로드\n",
    "        move_df = pd.read_csv(move_path)\n",
    "        visit_area_df = pd.read_csv(visit_area_path)\n",
    "        \n",
    "        # 2. 기본 데이터 검증\n",
    "        logger.info(\"🔍 데이터 검증 중...\")\n",
    "        \n",
    "        # 이동내역 검증\n",
    "        if len(move_df) == 0:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 이동내역 데이터가 비어있습니다.\")\n",
    "            return False\n",
    "            \n",
    "        # 방문지 검증\n",
    "        if len(visit_area_df) == 0:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 방문지 데이터가 비어있습니다.\")\n",
    "            return False\n",
    "        \n",
    "        # NEW_VISIT_AREA_ID 검증\n",
    "        if 'NEW_VISIT_AREA_ID' not in visit_area_df.columns:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 방문지 데이터에 NEW_VISIT_AREA_ID 컬럼이 없습니다.\")\n",
    "            return False\n",
    "            \n",
    "        valid_visit_areas = visit_area_df['NEW_VISIT_AREA_ID'].notna().sum()\n",
    "        if valid_visit_areas == 0:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 유효한 NEW_VISIT_AREA_ID가 없습니다.\")\n",
    "            return False\n",
    "            \n",
    "        # 이동내역의 NEW_END_VISIT_AREA_ID 검증\n",
    "        has_end_id = 'NEW_END_VISIT_AREA_ID' in move_df.columns\n",
    "        has_start_id = 'NEW_START_VISIT_AREA_ID' in move_df.columns\n",
    "        \n",
    "        if not has_end_id and not has_start_id:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 이동내역에 NEW_END_VISIT_AREA_ID 또는 NEW_START_VISIT_AREA_ID가 없습니다.\")\n",
    "            return False\n",
    "        \n",
    "        valid_moves = 0\n",
    "        if has_end_id:\n",
    "            valid_moves += move_df['NEW_END_VISIT_AREA_ID'].notna().sum()\n",
    "        if has_start_id:\n",
    "            valid_moves += move_df['NEW_START_VISIT_AREA_ID'].notna().sum()\n",
    "            \n",
    "        if valid_moves == 0:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 유효한 이동 기록이 없습니다.\")\n",
    "            return False\n",
    "        \n",
    "        # 여행 정보 파일 로드 (있는 경우)\n",
    "        travel_df = None\n",
    "        if os.path.exists(travel_path):\n",
    "            travel_df = pd.read_csv(travel_path)\n",
    "            logger.info(f\"✅ 여행정보 데이터 로드: {len(travel_df)}개 레코드\")\n",
    "        else:\n",
    "            logger.warning(f\"⚠️ 여행정보 파일이 없습니다: {travel_path}\")\n",
    "        \n",
    "        logger.info(f\"✅ 데이터 로드 및 검증 완료:\")\n",
    "        logger.info(f\"  - 이동내역: {len(move_df)}개 레코드 (유효 이동: {valid_moves}개)\")\n",
    "        logger.info(f\"  - 방문지: {len(visit_area_df)}개 레코드 (유효 방문지: {valid_visit_areas}개)\")\n",
    "        \n",
    "        # 3. 저장 경로 설정\n",
    "        model_save_path = f\"./models/{region_name}/\"\n",
    "        data_save_path = f\"./pickle/{region_name}/\"\n",
    "        \n",
    "        # 4. 트레이너 초기화\n",
    "        trainer = TravelRecommendationTrainer()\n",
    "        \n",
    "        # 5. 데이터 준비\n",
    "        logger.info(\"🔧 데이터 전처리 중...\")\n",
    "        data, targets, travel_contexts, id_to_index = trainer.prepare_data(visit_area_df, move_df, travel_df)\n",
    "        \n",
    "        # 6. 최종 데이터 검증\n",
    "        if data is None or targets is None:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 데이터 전처리에 실패했습니다.\")\n",
    "            return False\n",
    "        \n",
    "        min_nodes = 10  # 최소 노드 수\n",
    "        min_edges = 5   # 최소 엣지 수\n",
    "        \n",
    "        num_nodes = data['visit_area'].x.shape[0]\n",
    "        num_edges = data['visit_area', 'moved_to', 'visit_area'].edge_index.shape[1]\n",
    "        \n",
    "        if num_nodes < min_nodes:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 노드 수가 너무 적습니다 ({num_nodes} < {min_nodes})\")\n",
    "            return False\n",
    "            \n",
    "        if num_edges < min_edges:\n",
    "            logger.error(f\"❌ 지역 '{region_name}': 엣지 수가 너무 적습니다 ({num_edges} < {min_edges})\")\n",
    "            return False\n",
    "        \n",
    "        # 7. 모델 학습\n",
    "        logger.info(\"🎯 모델 학습 시작...\")\n",
    "        model, losses = trainer.train_model(\n",
    "            data, targets, travel_contexts,\n",
    "            epochs=200,\n",
    "            lr=0.001,\n",
    "            save_path=model_save_path\n",
    "        )\n",
    "        \n",
    "        # 8. 처리된 데이터 저장\n",
    "        logger.info(\"💾 데이터 저장 중...\")\n",
    "        trainer.save_processed_data(visit_area_df, data, id_to_index, save_path=data_save_path)\n",
    "        \n",
    "        # 9. 학습 결과 요약\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"✅ 지역 '{region_name}' 학습 완료!\")\n",
    "        logger.info(f\"📁 모델 저장: {model_save_path}\")\n",
    "        logger.info(f\"📁 데이터 저장: {data_save_path}\")\n",
    "        logger.info(f\"📊 최종 손실: {min(losses):.4f}\")\n",
    "        logger.info(f\"📊 총 에포크: {len(losses)}\")\n",
    "        logger.info(f\"🎯 노드 수: {num_nodes}, 엣지 수: {num_edges}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ 지역 '{region_name}' 학습 중 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"지역별 모델 학습 메인 함수\"\"\"\n",
    "    logger.info(\"🌍 전체 지역 GNN 모델 학습 시작!\")\n",
    "    \n",
    "    base_dir = './merged_csv/merged_csv_region/'\n",
    "    \n",
    "    # 지역 목록 가져오기\n",
    "    if not os.path.exists(base_dir):\n",
    "        logger.error(f\"❌ 기본 디렉토리를 찾을 수 없습니다: {base_dir}\")\n",
    "        return\n",
    "    \n",
    "    region_list = [r for r in os.listdir(base_dir) \n",
    "                   if not r.startswith('.') and os.path.isdir(os.path.join(base_dir, r))]\n",
    "    \n",
    "    if not region_list:\n",
    "        logger.error(f\"❌ 처리할 지역이 없습니다: {base_dir}\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"📍 총 {len(region_list)}개 지역 발견: {region_list}\")\n",
    "    \n",
    "    # 성공/실패 추적\n",
    "    success_regions = []\n",
    "    failed_regions = []\n",
    "    \n",
    "    # 각 지역별로 처리\n",
    "    for i, region_name in enumerate(region_list, 1):\n",
    "        logger.info(f\"\\n{'🌟' * 20}\")\n",
    "        logger.info(f\"📍 [{i}/{len(region_list)}] 지역 '{region_name}' 처리 시작!\")\n",
    "        logger.info(f\"{'🌟' * 20}\")\n",
    "        \n",
    "        region_path = os.path.join(base_dir, region_name)\n",
    "        \n",
    "        # 파일 경로 정의\n",
    "        move_path = os.path.join(region_path, 'fin', '이동내역_fin.csv')\n",
    "        travel_path = os.path.join(region_path, 'fin', '여행_fin.csv')\n",
    "        visit_area_path = os.path.join(region_path, 'fin', '방문지_fin.csv')\n",
    "        \n",
    "        # 지역별 학습 실행\n",
    "        success = main_region_training(\n",
    "            move_path=move_path,\n",
    "            travel_path=travel_path,\n",
    "            visit_area_path=visit_area_path,\n",
    "            region_name=region_name\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            success_regions.append(region_name)\n",
    "            logger.info(f\"✅ 지역 '{region_name}' 처리 완료!\")\n",
    "        else:\n",
    "            failed_regions.append(region_name)\n",
    "            logger.error(f\"❌ 지역 '{region_name}' 처리 실패!\")\n",
    "    \n",
    "    # 최종 결과 요약\n",
    "    logger.info(f\"\\n{'🏁' * 30}\")\n",
    "    logger.info(\"🏁 전체 지역 처리 완료!\")\n",
    "    logger.info(f\"{'🏁' * 30}\")\n",
    "    logger.info(f\"✅ 성공한 지역 ({len(success_regions)}개): {success_regions}\")\n",
    "    if failed_regions:\n",
    "        logger.info(f\"❌ 실패한 지역 ({len(failed_regions)}개): {failed_regions}\")\n",
    "    \n",
    "    # 모델 파일 구조 출력\n",
    "    logger.info(f\"\\n📂 저장된 파일 구조:\")\n",
    "    for region in success_regions:\n",
    "        logger.info(f\"  📁 {region}/\")\n",
    "        logger.info(f\"    🤖 models/{region}/improved_travel_recommendation_model.pt\")\n",
    "        logger.info(f\"    💾 pickle/{region}/improved_travel_data.pkl\")\n",
    "\n",
    "\n",
    "# 개별 지역 테스트를 위한 함수\n",
    "def test_single_region(region_name):\n",
    "    \"\"\"단일 지역 테스트용 함수\"\"\"\n",
    "    base_dir = './merged_csv/merged_csv_region/'\n",
    "    region_path = os.path.join(base_dir, region_name)\n",
    "    \n",
    "    move_path = os.path.join(region_path, 'fin', '이동내역_fin.csv')\n",
    "    travel_path = os.path.join(region_path, 'fin', '여행_fin.csv')\n",
    "    visit_area_path = os.path.join(region_path, 'fin', '방문지_fin.csv')\n",
    "    \n",
    "    return main_region_training(\n",
    "        move_path=move_path,\n",
    "        travel_path=travel_path,\n",
    "        visit_area_path=visit_area_path,\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "\n",
    "# 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    # 전체 지역 학습 실행\n",
    "    main()\n",
    "    \n",
    "    # 또는 특정 지역만 테스트하고 싶다면:\n",
    "    # test_single_region(\"서울특별시\")\n",
    "    \n",
    "    # 주의사항 정리\n",
    "    # 1. 데이터 매핑 실패나 누락이 있어도 안전하게 처리됩니다\n",
    "    # 2. 각 지역별로 최소 노드 수(10개)와 엣지 수(5개) 검증을 합니다\n",
    "    # 3. 상세한 로그를 통해 문제점을 파악할 수 있습니다\n",
    "    # 4. ID 매핑과 엣지 인덱스가 항상 일치하도록 보장됩니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
