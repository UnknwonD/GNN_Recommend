# GNN 모델 구현

- HAN 기반의 모델을 통해 학습을 돌렸을 때, 경로가 안정적이지 못하고 결과적으로 일반적인 추천시스템보다 못한 결과를 출력함
- 아래의 사항을 고려해서 알고리즘을 강화할 계획을 추가



---
</br>

1. GNN 출력 → 확률 → 샘플링 + 보상 → REINFORCE

2. 보상 기반 weight tuning (Supervised-style)

3. PPO, DDPG 등 Actor-Critic 방식

</br>

---
## 각 알고리즘 분석

| 방식                         | 일반화 방식                      | 한계                    |
| -------------------------- | --------------------------- | --------------------- |
| **Supervised fine-tuning** | 특정 노드의 점수만 조정               | 주변 노드(유사 장소)에 영향을 못 줌 |
| **REINFORCE** (확률 기반)      | 잘못 추천한 장소의 **선택 확률 전체를 줄임** | 자연스레 비슷한 확률 분포에도 반영   |
| **PPO/Actor-Critic**       | 환경 전체 고려 → 장기 정책 학습         | 복잡도 ↑                 |



## 알고리즘 선정 및 사유

| 방식                       | 이유                      |
| ------------------------ | ----------------------- |
| ✅ REINFORCE              | 추천 정책을 학습할 수 있음      |
| PPO / DDPG               | 구현 복잡도와 안정성 이슈    |
| supervised weight tuning | feedback을 일반화하기 어려움 |


---
</br>

> 단순 “score 조정 + fine-tune” 방식은 개별 피드백을 국소적으로 처리할 뿐,
GNN의 전체 추천 정책을 바꾸거나 비슷한 패턴까지 퍼뜨리기 어려울 것이라고 판단

- **GNN 출력 → 확률 변환 → 샘플링 → reward 학습** 구조의 학습 파이프라인 설계 진행
