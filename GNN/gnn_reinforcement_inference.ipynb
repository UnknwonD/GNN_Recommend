{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\n",
      "âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\n",
      "ğŸ—“ï¸ ì¶”ì²œ ì¼ì •:\n",
      "Day 2: ['ê´‘í™”ë¬¸ê´‘ì¥ (ID: 107)', 'ì‚¬ë¬´ì‹¤ (ID: 37)', 'ìˆ˜ì› ì „í†µë¬¸í™”ê´€ ì „í†µì‹ìƒí™œ ì²´í—˜ê´€ (ID: 525)']\n",
      "Day 1: ['ë¶ì„ ì‚°ì—­ (ID: 2607)', 'ë¶€ì‚°ì¢…í•©ë²„ìŠ¤í„°ë¯¸ë„ (ID: 1551)', 'ê¹€í•´ ìœ¨í•˜ì‹œí‹°í”„ë¼ë””ì›€ì•„íŒŒíŠ¸ (ID: 49)', 'ì§„ì£¼ ì‹œì™¸ë²„ìŠ¤í„°ë¯¸ë„ (ID: 25)', 'ìµì‚°ì—­ (ID: 4620)', 'ì²­ì£¼ ì—¬ê°í„°ë¯¸ë„ ë¶ë¶€ì •ë¥˜ì¥ (ID: 3665)', 'ì¹´í˜ ì—¬ì£¼ (ID: 1766)', 'ê·¸ëœë“œ í•˜ì–íŠ¸ ì„œìš¸ (ID: 2399)', 'êµ­ë¦½í˜„ëŒ€ë¯¸ìˆ ê´€ ì„œìš¸ê´€ (ID: 726)', 'ë”˜íƒ€ì´í‘ ê¹€í¬ ë¡¯ë°ëª° ì  (ID: 1672)', 'ì•ˆì¤‘ê·¼ê³µì› (ID: 941)', 'ì£½ë³€ ì‹œì™¸ë²„ìŠ¤ì •ë¥˜ì¥ (ID: 4603)']\n",
      "âœ… í”¼ë“œë°± ì—…ë°ì´íŠ¸: ì¢‹ì•„ìš” 1ê°œ, ì‹«ì–´ìš” 1ê°œ\n",
      "\n",
      "ğŸ”„ í”¼ë“œë°± ë°˜ì˜ í›„:\n",
      "Day 1: ['ë‚¨ì´ì„¬ (ID: 1441)', 'ì²œë¡œì—­ì • ìˆœë¡€ê¸¸ (ID: 6184)', 'ìš©ë¬¸ì‚¬ì€í–‰ë‚˜ë¬´ (ID: 2134)', 'ì„¸ì¢…ëŒ€ì™• ì—­ì‚¬ë¬¸í™”ê´€ (ID: 4285)', 'ì„¤ë´‰ì„œì› (ID: 4116)', 'ì§„ìœ„í–¥êµ (ID: 6068)', 'í™”ì„±í–‰ê¶ (ID: 282)', 'ìˆ˜ì› í™”ì„± ì„œì¥ëŒ€ (ID: 3304)', 'ìˆ˜ì› í™”ì„± ë¶ë™í¬ë£¨ (ID: 4574)', 'ëŒ€í•œë¶ˆêµì¡°ê³„ì¢… ë´‰ì€ì‚¬ (ID: 3674)', 'ë™ëŒ€ë¬¸ ì—­ì‚¬ ë¬¸í™”ê³µì› ë™ëŒ€ë¬¸ìš´ë™ì¥ ê¸°ë…ê´€ (ID: 981)', 'í–‰ì£¼ì‚°ì„± (ID: 1318)', 'ì•ˆì¤‘ê·¼ê³µì› (ID: 941)', 'ìŒë´‰ì‚° (ID: 1337)', 'ì „ì£¼í•œì˜¥ë§ˆì„ (ID: 9871)']\n"
     ]
    }
   ],
   "source": [
    "# web_inference.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import HeteroData\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "\n",
    "class ImprovedTravelGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, travel_context_dim, \n",
    "                 num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gat1 = GATConv(in_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True)\n",
    "        self.gat2 = GATConv(hidden_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True)\n",
    "        self.gat3 = GATConv(hidden_channels, out_channels, \n",
    "                           heads=1, dropout=dropout, concat=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.travel_encoder = nn.Sequential(\n",
    "            nn.Linear(travel_context_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.fusion_net = nn.Sequential(\n",
    "            nn.Linear(out_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.preference_head = nn.Sequential(\n",
    "            nn.Linear(out_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, data, travel_context, return_attention=False):\n",
    "        x = data['visit_area'].x\n",
    "        edge_index = data['visit_area', 'moved_to', 'visit_area'].edge_index\n",
    "        \n",
    "        x1 = self.gat1(x, edge_index)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        x2 = self.gat2(x1, edge_index)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2 + x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        graph_embedding = self.gat3(x2, edge_index)\n",
    "        graph_embedding = self.bn3(graph_embedding)\n",
    "        \n",
    "        travel_embedding = self.travel_encoder(travel_context)\n",
    "        travel_embedding_expanded = travel_embedding.expand(graph_embedding.size(0), -1)\n",
    "        \n",
    "        fused_features = torch.cat([graph_embedding, travel_embedding_expanded], dim=1)\n",
    "        final_embedding = self.fusion_net(fused_features)\n",
    "        \n",
    "        preference_scores = self.preference_head(final_embedding)\n",
    "        \n",
    "        return final_embedding, preference_scores\n",
    "\n",
    "class TravelRecommendationSystem:\n",
    "    def __init__(self, model_path='improved_travel_recommendation_model.pt', data_path='improved_travel_data.pkl'):\n",
    "        \"\"\"ì›¹ìš© ì—¬í–‰ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”\"\"\"\n",
    "        \n",
    "        print(\"ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\")\n",
    "        \n",
    "        # 1. ë°ì´í„° ë¡œë“œ\n",
    "        with open(data_path, 'rb') as f:\n",
    "            self.data_dict = pickle.load(f)\n",
    "        \n",
    "        self.visit_area_df = self.data_dict['visit_area_df']\n",
    "        self.graph_data = self.data_dict['graph_data']\n",
    "        self.visit_scaler = self.data_dict['visit_scaler']\n",
    "        self.travel_scaler = self.data_dict['travel_scaler']\n",
    "        \n",
    "        # 2. ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.graph_data = self.graph_data.to(self.device)\n",
    "        \n",
    "        # 3. ëª¨ë¸ ë¡œë“œ\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        model_config = checkpoint['model_config']\n",
    "        \n",
    "        self.model = ImprovedTravelGNN(**model_config).to(self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 4. í”¼ë“œë°± ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "        self.user_feedback_history = []\n",
    "        self.preference_weights = None\n",
    "        self.excluded_ids = set()\n",
    "        \n",
    "        print(\"âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "    \n",
    "    def process_travel_input(self, travel_info):\n",
    "        \"\"\"ì›¹ì—ì„œ ë°›ì€ ì—¬í–‰ ì •ë³´ ì „ì²˜ë¦¬\"\"\"\n",
    "        \n",
    "        travel_feature_cols = [\n",
    "            'TOTAL_COST_BINNED_ENCODED', 'WITH_PET', 'MONTH', 'DURATION',\n",
    "            'MVMN_ê¸°íƒ€', 'MVMN_ëŒ€ì¤‘êµí†µ', 'MVMN_ìê°€ìš©',\n",
    "            'TRAVEL_PURPOSE_1', 'TRAVEL_PURPOSE_2', 'TRAVEL_PURPOSE_3',\n",
    "            'TRAVEL_PURPOSE_4', 'TRAVEL_PURPOSE_5', 'TRAVEL_PURPOSE_6',\n",
    "            'TRAVEL_PURPOSE_7', 'TRAVEL_PURPOSE_8', 'TRAVEL_PURPOSE_9',\n",
    "            'WHOWITH_2ì¸ì—¬í–‰', 'WHOWITH_ê°€ì¡±ì—¬í–‰', 'WHOWITH_ê¸°íƒ€',\n",
    "            'WHOWITH_ë‹¨ë…ì—¬í–‰', 'WHOWITH_ì¹œêµ¬/ì§€ì¸ ì—¬í–‰'\n",
    "        ]\n",
    "        \n",
    "        processed_info = travel_info.copy()\n",
    "        \n",
    "        # ë°˜ë ¤ë™ë¬¼ ë™ë°˜\n",
    "        mission_list = processed_info['mission_ENC'].strip().split(',')\n",
    "        processed_info['WITH_PET'] = 1 if '0' in mission_list else 0\n",
    "            \n",
    "        # ì—¬í–‰ ëª©ì \n",
    "        for i in range(1, 10):\n",
    "            processed_info[f'TRAVEL_PURPOSE_{i}'] = 1 if str(i) in mission_list else 0\n",
    "            \n",
    "        # ë‚ ì§œ ì²˜ë¦¬\n",
    "        dates = processed_info['date_range'].split(' - ')\n",
    "        start_date = datetime.strptime(dates[0].strip(), \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(dates[1].strip(), \"%Y-%m-%d\")\n",
    "        \n",
    "        processed_info['MONTH'] = end_date.month\n",
    "        processed_info['DURATION'] = max(1, (end_date - start_date).days)\n",
    "        \n",
    "        # êµí†µìˆ˜ë‹¨\n",
    "        for m in ['ìê°€ìš©', 'ëŒ€ì¤‘êµí†µ', 'ê¸°íƒ€']:\n",
    "            processed_info[f\"MVMN_{m}\"] = 0\n",
    "        \n",
    "        if processed_info['MVMN_NM_ENC'] == '1':\n",
    "            processed_info['MVMN_ìê°€ìš©'] = 1\n",
    "        elif processed_info['MVMN_NM_ENC'] == '2':\n",
    "            processed_info['MVMN_ëŒ€ì¤‘êµí†µ'] = 1\n",
    "        else:\n",
    "            processed_info['MVMN_ê¸°íƒ€'] = 1\n",
    "        \n",
    "        # ë™í–‰ì\n",
    "        whowith_onehot = [0] * 5\n",
    "        idx = int(processed_info['whowith_ENC']) - 1\n",
    "        if 0 <= idx < 5:\n",
    "            whowith_onehot[idx] = 1\n",
    "        \n",
    "        processed_info.update({\n",
    "            'WHOWITH_ë‹¨ë…ì—¬í–‰': whowith_onehot[0],\n",
    "            'WHOWITH_2ì¸ì—¬í–‰': whowith_onehot[1],\n",
    "            'WHOWITH_ê°€ì¡±ì—¬í–‰': whowith_onehot[2],\n",
    "            'WHOWITH_ì¹œêµ¬/ì§€ì¸ ì—¬í–‰': whowith_onehot[3],\n",
    "            'WHOWITH_ê¸°íƒ€': whowith_onehot[4],\n",
    "        })\n",
    "        \n",
    "        # ë¹„ìš©\n",
    "        processed_info['TOTAL_COST_BINNED_ENCODED'] = int(processed_info['TOTAL_COST'])\n",
    "        \n",
    "        # ìµœì¢… ë²¡í„° ìƒì„±\n",
    "        travel_vector = [int(processed_info.get(k, 0)) for k in travel_feature_cols]\n",
    "        \n",
    "        return np.array([travel_vector]).astype(np.float32), processed_info['DURATION']\n",
    "    \n",
    "    def get_recommendations(self, travel_info, top_k=20, diversity_weight=0.3):\n",
    "        \"\"\"ì—¬í–‰ ì¶”ì²œ ìƒì„±\"\"\"\n",
    "        \n",
    "        # 1. ì—¬í–‰ ì •ë³´ ì „ì²˜ë¦¬\n",
    "        travel_tensor, duration = self.process_travel_input(travel_info)\n",
    "        travel_context = torch.tensor(travel_tensor, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # 2. ëª¨ë¸ ì¶”ë¡ \n",
    "        with torch.no_grad():\n",
    "            embeddings, preference_scores = self.model(self.graph_data, travel_context)\n",
    "        \n",
    "        scores = preference_scores.squeeze()\n",
    "        \n",
    "        # 3. ì œì™¸ëœ í•­ëª©ë“¤ ë‚®ì€ ì ìˆ˜ë¡œ ì„¤ì •\n",
    "        for exclude_id in self.excluded_ids:\n",
    "            matching_indices = self.visit_area_df[\n",
    "                self.visit_area_df['NEW_VISIT_AREA_ID'] == exclude_id\n",
    "            ].index.tolist()\n",
    "            \n",
    "            for idx in matching_indices:\n",
    "                if idx < len(scores):\n",
    "                    scores[idx] *= 0.2\n",
    "        \n",
    "        # 4. í”¼ë“œë°± ê¸°ë°˜ ì ìˆ˜ ì¡°ì •\n",
    "        if self.preference_weights:\n",
    "            scores = self._apply_preference_weights(scores)\n",
    "        \n",
    "        # 5. MMR ê¸°ë°˜ ë‹¤ì–‘ì„± ì¶”ì²œ\n",
    "        recommendations = self._mmr_selection(scores, embeddings, top_k, diversity_weight)\n",
    "        \n",
    "        # 6. ì¤‘ë³µ ì œê±° ë° ìœ íš¨í•œ ì¶”ì²œë§Œ ì„ íƒ\n",
    "        final_recommendations = self._filter_unique_recommendations(recommendations, max_items=15)\n",
    "        \n",
    "        # 7. ìµœì  ê²½ë¡œ ìƒì„±\n",
    "        optimized_routes = self._generate_optimized_routes(final_recommendations, duration)\n",
    "        \n",
    "        return {\n",
    "            'routes': optimized_routes,\n",
    "            'recommendations_data': final_recommendations,\n",
    "            'embeddings': embeddings,\n",
    "            'duration': duration\n",
    "        }\n",
    "    \n",
    "    def update_feedback(self, liked_items, disliked_items, embeddings):\n",
    "        \"\"\"ì‚¬ìš©ì í”¼ë“œë°± ì—…ë°ì´íŠ¸\"\"\"\n",
    "        \n",
    "        # 1. ì œì™¸ ëª©ë¡ ì—…ë°ì´íŠ¸\n",
    "        for item_id in disliked_items:\n",
    "            self.excluded_ids.add(item_id)\n",
    "        \n",
    "        # 2. í”¼ë“œë°± íˆìŠ¤í† ë¦¬ ì €ì¥\n",
    "        feedback = {\n",
    "            'liked': liked_items,\n",
    "            'disliked': disliked_items,\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "        self.user_feedback_history.append(feedback)\n",
    "        \n",
    "        # 3. ì„ í˜¸ë„ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
    "        self.preference_weights = self._calculate_preference_weights()\n",
    "        \n",
    "        print(f\"âœ… í”¼ë“œë°± ì—…ë°ì´íŠ¸: ì¢‹ì•„ìš” {len(liked_items)}ê°œ, ì‹«ì–´ìš” {len(disliked_items)}ê°œ\")\n",
    "        \n",
    "        return len(self.excluded_ids)\n",
    "    \n",
    "    def _apply_preference_weights(self, scores):\n",
    "        \"\"\"í”¼ë“œë°± ê¸°ë°˜ ì ìˆ˜ ì¡°ì •\"\"\"\n",
    "        if not self.preference_weights:\n",
    "            return scores\n",
    "            \n",
    "        adjusted_scores = scores.clone()\n",
    "        \n",
    "        for i, row in self.visit_area_df.iterrows():\n",
    "            if i >= len(adjusted_scores):\n",
    "                break\n",
    "                \n",
    "            area_type = row.get('VISIT_AREA_TYPE_CD', 0)\n",
    "            \n",
    "            if area_type in self.preference_weights.get('preferred_types', []):\n",
    "                adjusted_scores[i] *= 1.3\n",
    "            elif area_type in self.preference_weights.get('avoided_types', []):\n",
    "                adjusted_scores[i] *= 0.7\n",
    "        \n",
    "        return adjusted_scores\n",
    "    \n",
    "    def _mmr_selection(self, scores, embeddings, top_k, diversity_weight):\n",
    "        \"\"\"MMR ê¸°ë°˜ ë‹¤ì–‘ì„± ì„ íƒ\"\"\"\n",
    "        recommendations = []\n",
    "        remaining_indices = list(range(len(scores)))\n",
    "        \n",
    "        # ì œì™¸ëœ ì¸ë±ìŠ¤ ì œê±°\n",
    "        for exclude_id in self.excluded_ids:\n",
    "            matching_indices = self.visit_area_df[\n",
    "                self.visit_area_df['NEW_VISIT_AREA_ID'] == exclude_id\n",
    "            ].index.tolist()\n",
    "            for idx in matching_indices:\n",
    "                if idx in remaining_indices:\n",
    "                    remaining_indices.remove(idx)\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ ì¶”ì²œ\n",
    "        if remaining_indices:\n",
    "            valid_scores = [(i, scores[i].item()) for i in remaining_indices if scores[i].item() >= 0]\n",
    "            if valid_scores:\n",
    "                best_idx = max(valid_scores, key=lambda x: x[1])[0]\n",
    "                recommendations.append(best_idx)\n",
    "                remaining_indices.remove(best_idx)\n",
    "        \n",
    "        # ë‚˜ë¨¸ì§€ MMR ì„ íƒ\n",
    "        for _ in range(min(top_k - 1, len(remaining_indices))):\n",
    "            if not remaining_indices:\n",
    "                break\n",
    "            \n",
    "            best_score = -float('inf')\n",
    "            best_idx = None\n",
    "            \n",
    "            for idx in remaining_indices:\n",
    "                if scores[idx].item() < 0:\n",
    "                    continue\n",
    "                \n",
    "                relevance = scores[idx].item()\n",
    "                \n",
    "                # ë‹¤ì–‘ì„± ê³„ì‚°\n",
    "                similarities = []\n",
    "                for rec_idx in recommendations:\n",
    "                    sim = F.cosine_similarity(\n",
    "                        embeddings[idx:idx+1], \n",
    "                        embeddings[rec_idx:rec_idx+1]\n",
    "                    ).item()\n",
    "                    similarities.append(sim)\n",
    "                \n",
    "                diversity = 1 - max(similarities) if similarities else 1\n",
    "                final_score = (1 - diversity_weight) * relevance + diversity_weight * diversity\n",
    "                \n",
    "                if final_score > best_score:\n",
    "                    best_score = final_score\n",
    "                    best_idx = idx\n",
    "            \n",
    "            if best_idx is not None:\n",
    "                recommendations.append(best_idx)\n",
    "                remaining_indices.remove(best_idx)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    \n",
    "    def _filter_unique_recommendations(self, recommendations, max_items=10):\n",
    "        \"\"\"ì¤‘ë³µ ì œê±° ë° ìœ íš¨í•œ ì¶”ì²œ í•„í„°ë§\"\"\"\n",
    "        unique_recommendations = []\n",
    "        seen_ids = set()\n",
    "        \n",
    "        for idx in recommendations:\n",
    "            if idx < len(self.visit_area_df):\n",
    "                row = self.visit_area_df.iloc[idx]\n",
    "                area_id = row['NEW_VISIT_AREA_ID']\n",
    "                \n",
    "                if area_id not in seen_ids and area_id not in self.excluded_ids and area_id != 0:\n",
    "                    unique_recommendations.append({\n",
    "                        'idx': idx,\n",
    "                        'id': area_id,\n",
    "                        'name': row['VISIT_AREA_NM'],\n",
    "                        'coords': [row['X_COORD'], row['Y_COORD']],\n",
    "                        'type': row.get('VISIT_AREA_TYPE_CD', 0),\n",
    "                        'display_name': f\"{row['VISIT_AREA_NM']} (ID: {area_id})\"  # í‘œì‹œìš© ì´ë¦„ ì¶”ê°€\n",
    "                    })\n",
    "                    seen_ids.add(area_id)\n",
    "                \n",
    "                if len(unique_recommendations) >= max_items:\n",
    "                    break\n",
    "        \n",
    "        return unique_recommendations\n",
    "    \n",
    "    def _generate_optimized_routes(self, recommendations, duration):\n",
    "        \"\"\"ìµœì í™”ëœ ì¼ë³„ ê²½ë¡œ ìƒì„±\"\"\"\n",
    "        if not recommendations:\n",
    "            return {}\n",
    "        \n",
    "        coords = np.array([rec['coords'] for rec in recommendations])\n",
    "        coords = np.nan_to_num(coords, nan=0.0)\n",
    "        \n",
    "        # K-means í´ëŸ¬ìŠ¤í„°ë§\n",
    "        n_clusters = min(duration, len(recommendations))\n",
    "        if n_clusters > 1:\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            day_labels = kmeans.fit_predict(coords)\n",
    "        else:\n",
    "            day_labels = np.zeros(len(recommendations))\n",
    "        \n",
    "        # ì¼ë³„ ê·¸ë£¹ ìƒì„±\n",
    "        daily_groups = {}\n",
    "        for i, rec in enumerate(recommendations):\n",
    "            day = int(day_labels[i])\n",
    "            if day not in daily_groups:\n",
    "                daily_groups[day] = []\n",
    "            daily_groups[day].append(rec)\n",
    "        \n",
    "        # ë‹¨ì¼ ì¥ì†Œ ì¼ì • ì¬ë°°ì¹˜\n",
    "        for day, locs in list(daily_groups.items()):\n",
    "            if len(locs) == 1 and len(daily_groups) > 1:\n",
    "                target_day = min(daily_groups.keys())\n",
    "                daily_groups[target_day].extend(locs)\n",
    "                del daily_groups[day]\n",
    "        \n",
    "        # TSP ìµœì í™”\n",
    "        optimized_routes = {}\n",
    "        for day, locations in daily_groups.items():\n",
    "            if len(locations) > 1:\n",
    "                optimized_routes[day] = self._solve_tsp_simple(locations)\n",
    "            else:\n",
    "                optimized_routes[day] = locations\n",
    "        \n",
    "        return optimized_routes\n",
    "    \n",
    "    def _solve_tsp_simple(self, locations):\n",
    "        \"\"\"ê°„ë‹¨í•œ TSP í•´ë²• (Nearest Neighbor)\"\"\"\n",
    "        if len(locations) <= 2:\n",
    "            return locations\n",
    "        \n",
    "        coords = np.array([loc['coords'] for loc in locations])\n",
    "        n = len(coords)\n",
    "        \n",
    "        # ê±°ë¦¬ í–‰ë ¬\n",
    "        dist_matrix = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                dist_matrix[i][j] = np.linalg.norm(coords[i] - coords[j])\n",
    "        \n",
    "        # Nearest Neighbor ì•Œê³ ë¦¬ì¦˜\n",
    "        unvisited = set(range(1, n))\n",
    "        current = 0\n",
    "        route = [0]\n",
    "        \n",
    "        while unvisited:\n",
    "            nearest = min(unvisited, key=lambda x: dist_matrix[current][x])\n",
    "            route.append(nearest)\n",
    "            unvisited.remove(nearest)\n",
    "            current = nearest\n",
    "        \n",
    "        return [locations[i] for i in route]\n",
    "    \n",
    "    def _calculate_preference_weights(self):\n",
    "        \"\"\"í”¼ë“œë°± íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì„ í˜¸ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°\"\"\"\n",
    "        if not self.user_feedback_history:\n",
    "            return None\n",
    "        \n",
    "        liked_types = []\n",
    "        disliked_types = []\n",
    "        \n",
    "        for feedback in self.user_feedback_history:\n",
    "            for area_id in feedback['liked']:\n",
    "                matching_rows = self.visit_area_df[\n",
    "                    self.visit_area_df['NEW_VISIT_AREA_ID'] == area_id\n",
    "                ]\n",
    "                if not matching_rows.empty:\n",
    "                    liked_types.append(matching_rows.iloc[0].get('VISIT_AREA_TYPE_CD', 0))\n",
    "            \n",
    "            for area_id in feedback['disliked']:\n",
    "                matching_rows = self.visit_area_df[\n",
    "                    self.visit_area_df['NEW_VISIT_AREA_ID'] == area_id\n",
    "                ]\n",
    "                if not matching_rows.empty:\n",
    "                    disliked_types.append(matching_rows.iloc[0].get('VISIT_AREA_TYPE_CD', 0))\n",
    "        \n",
    "        return {\n",
    "            'preferred_types': list(set(liked_types)),\n",
    "            'avoided_types': list(set(disliked_types))\n",
    "        }\n",
    "    \n",
    "    def reset_feedback(self):\n",
    "        \"\"\"í”¼ë“œë°± ì´ˆê¸°í™”\"\"\"\n",
    "        self.user_feedback_history = []\n",
    "        self.preference_weights = None\n",
    "        self.excluded_ids = set()\n",
    "        print(\"ğŸ”„ í”¼ë“œë°± ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "    recommender = TravelRecommendationSystem()\n",
    "    \n",
    "    # ì—¬í–‰ ì •ë³´ ì˜ˆì‹œ\n",
    "    travel_example = {\n",
    "        'mission_ENC': '0,1,2',\n",
    "        'date_range': '2025-09-28 - 2025-09-30',\n",
    "        'TOTAL_COST': '2',\n",
    "        'MVMN_NM_ENC': '2',\n",
    "        'whowith_ENC': '2'\n",
    "    }\n",
    "    \n",
    "    # ì´ˆê¸° ì¶”ì²œ\n",
    "    result = recommender.get_recommendations(travel_example)\n",
    "    print(\"ğŸ—“ï¸ ì¶”ì²œ ì¼ì •:\")\n",
    "    for day, route in result['routes'].items():\n",
    "        print(f\"Day {day + 1}: {[loc['display_name'] for loc in route]}\")\n",
    "    \n",
    "    # í”¼ë“œë°± ì²˜ë¦¬ (ì˜ˆì‹œ)\n",
    "    liked_ids = [result['recommendations_data'][0]['id']]\n",
    "    disliked_ids = [result['recommendations_data'][1]['id']]\n",
    "    \n",
    "    recommender.update_feedback(liked_ids, disliked_ids, result['embeddings'])\n",
    "    \n",
    "    # ì¬ì¶”ì²œ\n",
    "    result2 = recommender.get_recommendations(travel_example)\n",
    "    print(\"\\nğŸ”„ í”¼ë“œë°± ë°˜ì˜ í›„:\")\n",
    "    for day, route in result2['routes'].items():\n",
    "        print(f\"Day {day + 1}: {[loc['display_name'] for loc in route]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ìƒˆë¡œìš´ ë²„ì „"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ê°œì„ ëœ í”¼ë“œë°± ê¸°ë°˜ ê²½ë¡œ ëŒ€ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘!\n",
      "============================================================\n",
      "ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: cpu\n",
      "ğŸ“… ì—¬í–‰ ê¸°ê°„: 2025-09-28 - 2025-09-29 (2ì¼)\n",
      "\n",
      "ğŸ—“ï¸ ì´ˆê¸° ì—¬í–‰ ì¼ì • (ìµœì í™” ë° í•„í„°ë§ ì ìš©):\n",
      "\n",
      "ğŸ“… Day 1:\n",
      " 1. [2594] ë¬¼ì•ˆê°œê³µì›\n",
      " 2. [2595] ìŠ¤íƒ€ë²…ìŠ¤ ë” ì–‘í‰ DTR\n",
      " 3. [832] ê³¤ì§€ì•”ë¦¬ì¡°íŠ¸\n",
      " 4. [684] ì„¸ë¯¸ì›\n",
      " 5. [5565] ìœ¡ í†µì´ë„¤ ì—°ìë°¥ ì—°ì ëŒì†¥ë°¥\n",
      "\n",
      "ğŸ“… Day 2:\n",
      " 1. [6701] ì„œë¼ë²Œ ì•„ë“¤\n",
      " 2. [3707] ì–‘ìˆ˜ë¦¬ ì „í†µì‹œì¥\n",
      " 3. [2500] ì–‘ì„œë†í˜‘ í•˜ë‚˜ë¡œë§ˆíŠ¸ ë³¸ì \n",
      " 4. [972] ì¹´í˜ ë¦¬ë…¸\n",
      " 5. [7664] ê·¸ë¦¼ ì •ì›\n",
      "\n",
      "ì´ 10ê°œ ì¥ì†Œ ì¶”ì²œ\n",
      "\n",
      "============================================================\n",
      "ğŸ”„ í”¼ë“œë°± ë¼ìš´ë“œ 1\n",
      "ğŸ‘ ì¢‹ì•„ìš”: ['ê³¤ì§€ì•”ë¦¬ì¡°íŠ¸', 'ê°•ì´ë‹¤']\n",
      "ğŸ‘ ì‹«ì–´ìš”: ['ë¬¼ë ˆê¸¸ ì‹ë‹¹']\n",
      "âœ… í”¼ë“œë°± ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì¢‹ì•„ìš” 2ê°œ, ì‹«ì–´ìš” 1ê°œ\n",
      "\n",
      "ğŸ¯ í”¼ë“œë°± ë°˜ì˜ í›„ ìµœì í™”ëœ ì—¬í–‰ ì¼ì •:\n",
      "\n",
      "ğŸ“… Day 1:\n",
      " 1. [941] ì•ˆì¤‘ê·¼ê³µì›\n",
      " 2. [799] í˜„ëŒ€ë°±í™”ì  ì¤‘ë™ì \n",
      " 3. [5788] CGV ì†Œí’\n",
      " 4. [3786] ì›…ì§„í”Œë ˆì´ë„ì‹œ\n",
      " 5. [955] ìƒë™í˜¸ìˆ˜ ê³µì›\n",
      "\n",
      "ğŸ“… Day 2:\n",
      " 1. [1940] ì„œë¦¬í’€ ì‹ë‹¹\n",
      " 2. [5217] ì—ì‡ ì„¸ì»¨ì¦ˆ ê°•ë‚¨ëŒ€ë¡œì \n",
      " 3. [5215] ì—­ì‚¼ ë¬¸í™”ê³µì›\n",
      " 4. [2399] ê·¸ëœë“œ í•˜ì–íŠ¸ ì„œìš¸\n",
      " 5. [8589] ìŠ¤íƒ€ë²…ìŠ¤ ë™ëŒ€ë¬¸ë””ìì¸í”Œë¼ìì \n",
      "\n",
      "ì´ 10ê°œ ì¥ì†Œ ì¶”ì²œ\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, GATConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cdist\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ImprovedTravelGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, travel_context_dim, \n",
    "                 num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # GNN layers with edge features\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True, \n",
    "                           edge_dim=5)  # edge features ê³ ë ¤\n",
    "        self.gat2 = GATConv(hidden_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True,\n",
    "                           edge_dim=5)\n",
    "        self.gat3 = GATConv(hidden_channels, out_channels, \n",
    "                           heads=1, dropout=dropout, concat=False,\n",
    "                           edge_dim=5)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Travel context encoder\n",
    "        self.travel_encoder = nn.Sequential(\n",
    "            nn.Linear(travel_context_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # Distance-aware attention module\n",
    "        self.distance_attention = nn.Sequential(\n",
    "            nn.Linear(2, hidden_channels // 2),  # x, y coordinates\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Fusion network\n",
    "        self.fusion_net = nn.Sequential(\n",
    "            nn.Linear(out_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # Preference head\n",
    "        self.preference_head = nn.Sequential(\n",
    "            nn.Linear(out_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, data, travel_context, return_attention=False):\n",
    "        x = data['visit_area'].x\n",
    "        edge_index = data['visit_area', 'moved_to', 'visit_area'].edge_index\n",
    "        edge_attr = data['visit_area', 'moved_to', 'visit_area'].edge_attr\n",
    "        \n",
    "        # Extract coordinates for distance attention\n",
    "        coords = x[:, :2]  # Assuming first two features are X_COORD, Y_COORD\n",
    "        \n",
    "        # GNN layers with edge features\n",
    "        x1 = self.gat1(x, edge_index, edge_attr)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        x2 = self.gat2(x1, edge_index, edge_attr)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2 + x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        graph_embedding = self.gat3(x2, edge_index, edge_attr)\n",
    "        graph_embedding = self.bn3(graph_embedding)\n",
    "        \n",
    "        # Apply distance-based attention\n",
    "        distance_weights = self.distance_attention(coords)\n",
    "        graph_embedding = graph_embedding * distance_weights\n",
    "        \n",
    "        # Travel context encoding\n",
    "        travel_embedding = self.travel_encoder(travel_context)\n",
    "        travel_embedding_expanded = travel_embedding.expand(graph_embedding.size(0), -1)\n",
    "        \n",
    "        # Fusion\n",
    "        fused_features = torch.cat([graph_embedding, travel_embedding_expanded], dim=1)\n",
    "        final_embedding = self.fusion_net(fused_features)\n",
    "        \n",
    "        # Preference scores\n",
    "        preference_scores = self.preference_head(final_embedding)\n",
    "        \n",
    "        return final_embedding, preference_scores\n",
    "\n",
    "class EnhancedDataProcessor:\n",
    "    def __init__(self):\n",
    "        self.visit_scaler = RobustScaler()\n",
    "        self.travel_scaler = StandardScaler()\n",
    "        # ì œì™¸í•  í‚¤ì›Œë“œ ëª©ë¡\n",
    "        self.exclude_keywords = [\n",
    "            'ì—­', 'í„°ë¯¸ë„', 'ê³µí•­', 'íœ´ê²Œì†Œ', 'ì •ë¥˜ì¥', 'í†¨ê²Œì´íŠ¸', 'êµì°¨ë¡œ', 'ì¶œêµ¬', 'ì…êµ¬',\n",
    "            'IC', 'JC', 'ë‚˜ë“¤ëª©', 'ë¶„ê¸°ì ', 'ìš”ê¸ˆì†Œ', 'ì£¼ì°¨ì¥', 'ì£¼ìœ ì†Œ', 'ì¶©ì „ì†Œ',\n",
    "            'ì•„íŒŒíŠ¸', 'ì›ë£¸', 'ì˜¤í”¼ìŠ¤í…”', 'ë¹Œë¼', 'ì£¼íƒ', 'ë¹Œë”©', 'ìƒê°€', 'ëª¨í…”'\n",
    "        ]\n",
    "        \n",
    "    def should_exclude_location(self, name):\n",
    "        \"\"\"ìœ„ì¹˜ë¥¼ ì œì™¸í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸\"\"\"\n",
    "        if pd.isna(name):\n",
    "            return False\n",
    "        name_str = str(name).lower()\n",
    "        \n",
    "        # íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ë˜ê³ , ë‹¤ë¥¸ ìœ ìš©í•œ ë‹¨ì–´ê°€ ì—†ëŠ” ê²½ìš° ì œì™¸\n",
    "        for keyword in self.exclude_keywords:\n",
    "            if keyword.lower() in name_str:\n",
    "                # ì˜ˆì™¸ ì²˜ë¦¬: ê´€ê´‘ì§€ë¡œì„œì˜ ì—­í• ì´ ìˆëŠ” ê²½ìš°\n",
    "                tourist_keywords = ['ê´€ê´‘', 'í…Œë§ˆ', 'íŒŒí¬', 'ëœë“œ', 'ì›”ë“œ', 'ë¦¬ì¡°íŠ¸', 'í˜¸í…”', \n",
    "                                   'ë§›ì§‘', 'ì‹ë‹¹', 'ì¹´í˜', 'ë°•ë¬¼ê´€', 'ì „ì‹œ', 'ê°¤ëŸ¬ë¦¬', 'ë¬¸í™”']\n",
    "                if any(tk in name_str for tk in tourist_keywords):\n",
    "                    continue\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def process_visit_area_features(self, visit_area_df):\n",
    "        visit_area_df = visit_area_df.copy()\n",
    "        \n",
    "        # ì¢Œí‘œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "        visit_area_df['X_COORD'] = visit_area_df['X_COORD'].fillna(visit_area_df['X_COORD'].mean())\n",
    "        visit_area_df['Y_COORD'] = visit_area_df['Y_COORD'].fillna(visit_area_df['Y_COORD'].mean())\n",
    "        visit_area_df['VISIT_CHC_REASON_CD'] = visit_area_df['VISIT_CHC_REASON_CD'].fillna(0)\n",
    "        \n",
    "        features = visit_area_df[['X_COORD', 'Y_COORD']].copy()\n",
    "        \n",
    "        # One-hot encoding\n",
    "        type_onehot = pd.get_dummies(visit_area_df['VISIT_AREA_TYPE_CD'], prefix='type')\n",
    "        reason_onehot = pd.get_dummies(visit_area_df['VISIT_CHC_REASON_CD'].fillna(0), prefix='reason')\n",
    "        \n",
    "        # ì •ê·œí™”ëœ ë§Œì¡±ë„ ì ìˆ˜\n",
    "        for col in ['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']:\n",
    "            visit_area_df[col] = visit_area_df[col].fillna(3)\n",
    "            visit_area_df[f'{col}_norm'] = (visit_area_df[col] - 1) / 4.0\n",
    "        \n",
    "        # ì¸ê¸°ë„ ì ìˆ˜\n",
    "        visit_area_df['popularity_score'] = (\n",
    "            visit_area_df['DGSTFN_norm'] * 0.4 + \n",
    "            visit_area_df['REVISIT_INTENTION_norm'] * 0.3 + \n",
    "            visit_area_df['RCMDTN_INTENTION_norm'] * 0.3\n",
    "        )\n",
    "        \n",
    "        # ì œì™¸í•  ì¥ì†Œì— ëŒ€í•œ í˜ë„í‹° ì¶”ê°€\n",
    "        exclude_penalty = visit_area_df['VISIT_AREA_NM'].apply(self.should_exclude_location).astype(float) * -0.5\n",
    "        \n",
    "        # ëª¨ë“  íŠ¹ì„± ê²°í•©\n",
    "        features = pd.concat([\n",
    "            features, type_onehot, reason_onehot,\n",
    "            visit_area_df[['DGSTFN_norm', 'REVISIT_INTENTION_norm', 'RCMDTN_INTENTION_norm', 'popularity_score']],\n",
    "            pd.DataFrame({'exclude_penalty': exclude_penalty})\n",
    "        ], axis=1)\n",
    "        \n",
    "        return self.visit_scaler.fit_transform(features.values.astype(np.float32))\n",
    "    \n",
    "    def create_enhanced_edges(self, move_df, visit_area_df):\n",
    "        edges = []\n",
    "        edge_weights = []\n",
    "        \n",
    "        for travel_id, group in move_df.groupby(\"TRAVEL_ID\"):\n",
    "            group = group.sort_values(\"TRIP_ID\").reset_index(drop=True)\n",
    "            \n",
    "            for i in range(1, len(group)):\n",
    "                from_id = group.loc[i-1, \"END_NEW_ID\"]\n",
    "                to_id = group.loc[i, \"END_NEW_ID\"]\n",
    "                \n",
    "                if pd.notna(from_id) and pd.notna(to_id):\n",
    "                    duration = group.loc[i, \"DURATION_MINUTES\"] if \"DURATION_MINUTES\" in group.columns else 0\n",
    "                    transport = group.loc[i, \"MVMN_CD_1\"] if \"MVMN_CD_1\" in group.columns else 0\n",
    "                    \n",
    "                    edges.append([int(from_id), int(to_id), duration, transport])\n",
    "                    edge_weights.append(1.0)\n",
    "        \n",
    "        edges_df = pd.DataFrame(edges, columns=[\"FROM_ID\", \"TO_ID\", \"DURATION_MINUTES\", \"MVMN_CD_1\"])\n",
    "        \n",
    "        # êµí†µìˆ˜ë‹¨ ì›í•« ì¸ì½”ë”©\n",
    "        edges_df[\"MVMN_TYPE\"] = edges_df[\"MVMN_CD_1\"].apply(\n",
    "            lambda code: \"drive\" if code in [1,2,3] else \"public\" if code in [4,5,6,7,8,9,10,11,12,13,50] else \"other\"\n",
    "        )\n",
    "        edges_df[\"is_drive\"] = (edges_df[\"MVMN_TYPE\"] == \"drive\").astype(int)\n",
    "        edges_df[\"is_public\"] = (edges_df[\"MVMN_TYPE\"] == \"public\").astype(int)\n",
    "        edges_df[\"is_other\"] = (edges_df[\"MVMN_TYPE\"] == \"other\").astype(int)\n",
    "        \n",
    "        edge_index = torch.tensor(edges_df[[\"FROM_ID\", \"TO_ID\"]].values.T, dtype=torch.long)\n",
    "        edge_attr = torch.tensor(np.column_stack([\n",
    "            edges_df[[\"DURATION_MINUTES\"]].fillna(0).values,\n",
    "            edges_df[[\"is_drive\", \"is_public\", \"is_other\"]].values,\n",
    "            np.array(edge_weights).reshape(-1, 1)\n",
    "        ]), dtype=torch.float32)\n",
    "        \n",
    "        return edge_index, edge_attr\n",
    "\n",
    "class SmartRecommendationEngine:\n",
    "    def __init__(self, device, model_path='improved_travel_recommendation_model.pt', data_path='improved_travel_data.pkl'):\n",
    "        # 1. ë°ì´í„° ë¡œë“œ\n",
    "        with open(data_path, 'rb') as f:\n",
    "            self.data_dict = pickle.load(f)\n",
    "        \n",
    "        self.visit_area_df = self.data_dict['visit_area_df']\n",
    "        self.graph_data = self.data_dict['graph_data']\n",
    "        self.visit_scaler = self.data_dict['visit_scaler']\n",
    "        self.travel_scaler = self.data_dict['travel_scaler']\n",
    "        \n",
    "        # 2. ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.graph_data = self.graph_data.to(self.device)\n",
    "        \n",
    "        # 3. ëª¨ë¸ ë¡œë“œ\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        model_config = checkpoint['model_config']\n",
    "        \n",
    "        self.model = ImprovedTravelGNN(**model_config).to(self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 4. í”¼ë“œë°± ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "        self.excluded_ids = set()\n",
    "        self.device = device\n",
    "        \n",
    "        self.user_feedback_history = []\n",
    "        self.preference_weights = None\n",
    "        self.processor = EnhancedDataProcessor()\n",
    "        self.route_generator = OptimizedRouteGenerator(distance_threshold_km=50)  # ê±°ë¦¬ ì„ê³„ê°’ ì¤„ì„\n",
    "    \n",
    "    def feedback_model(self, feedback, travel_context_tensor, travel_duration, unique_recommendations, embeddings):\n",
    "        liked_indices = [unique_recommendations[i] for i in feedback[\"liked\"] if i < len(unique_recommendations)]\n",
    "        disliked_indices = [unique_recommendations[i] for i in feedback[\"disliked\"] if i < len(unique_recommendations)]\n",
    "        \n",
    "        if liked_indices:\n",
    "            print(f\"ğŸ‘ ì¢‹ì•„ìš”: {[self.visit_area_df.iloc[idx]['VISIT_AREA_NM'] for idx in liked_indices]}\")\n",
    "        if disliked_indices:\n",
    "            print(f\"ğŸ‘ ì‹«ì–´ìš”: {[self.visit_area_df.iloc[idx]['VISIT_AREA_NM'] for idx in disliked_indices]}\")\n",
    "        \n",
    "        self.update_with_feedback(liked_indices, disliked_indices, embeddings)\n",
    "        \n",
    "        # ì œì™¸ëœ í•­ëª© ë°˜ì˜\n",
    "        excluded_ids = {self.visit_area_df.iloc[idx]['NEW_VISIT_AREA_ID'] for idx in disliked_indices}\n",
    "        \n",
    "        recommendations, embeddings, _ = self.get_recommendations(\n",
    "            travel_context_tensor, top_k=30, diversity_weight=0.3, \n",
    "            excluded_ids=excluded_ids, filter_useless=True, consider_distance=True\n",
    "        )\n",
    "        \n",
    "        unique_recommendations, seen_ids = [], set()\n",
    "        for idx in recommendations:\n",
    "            if idx < len(self.visit_area_df):\n",
    "                row = self.visit_area_df.iloc[idx]\n",
    "                area_id = row['NEW_VISIT_AREA_ID']\n",
    "                name = row['VISIT_AREA_NM']\n",
    "                \n",
    "                if (area_id not in seen_ids and \n",
    "                    area_id not in excluded_ids and \n",
    "                    area_id != 0 and\n",
    "                    not self.processor.should_exclude_location(name)):\n",
    "                    unique_recommendations.append(idx)\n",
    "                    seen_ids.add(area_id)\n",
    "                \n",
    "                if len(unique_recommendations) >= 15:\n",
    "                    break\n",
    "        \n",
    "        optimized_routes = self.route_generator.generate_daily_routes(\n",
    "            unique_recommendations, self.visit_area_df, travel_duration\n",
    "        )\n",
    "        \n",
    "        return optimized_routes\n",
    "    \n",
    "    # ì¤‘ë³µ ë°©ë¬¸ì§€ ì œê±° ë° ìœ íš¨ì„± ê²€ì‚¬\n",
    "    def optimize_routes(self, recommendations, travel_tensor):    \n",
    "        unique_recommendations, seen_ids = [], set()\n",
    "        for idx in recommendations:\n",
    "            if idx < len(self.visit_area_df):\n",
    "                row = self.visit_area_df.iloc[idx]\n",
    "                area_id = row['NEW_VISIT_AREA_ID']\n",
    "                name = row['VISIT_AREA_NM']\n",
    "                \n",
    "                # ì¤‘ë³µ ì²´í¬ ë° ì“¸ëª¨ì—†ëŠ” ì¥ì†Œ ì¬í™•ì¸\n",
    "                if (area_id not in seen_ids and \n",
    "                    area_id != 0 and \n",
    "                    not self.processor.should_exclude_location(name)):\n",
    "                    unique_recommendations.append(idx)\n",
    "                    seen_ids.add(area_id)\n",
    "                \n",
    "                if len(unique_recommendations) >= 15:  # ì—¬ìœ ìˆê²Œ ì„ íƒ\n",
    "                    break\n",
    "        \n",
    "        # ìµœì í™” ê²½ë¡œ ìƒì„±\n",
    "        travel_duration = int(travel_tensor[0, 3])\n",
    "        optimized_routes = self.route_generator.generate_daily_routes(\n",
    "            unique_recommendations, self.visit_area_df, travel_duration\n",
    "        )\n",
    "        return optimized_routes, unique_recommendations\n",
    "    \n",
    "    # SmartRecommendationEngine í´ë˜ìŠ¤ ë‚´ get_recommendations ë©”ì†Œë“œ\n",
    "    def get_recommendations(self, travel_context, top_k=10, diversity_weight=0.3, \n",
    "                            excluded_ids=None, filter_useless=True, consider_distance=True):\n",
    "\n",
    "        self.model.eval()\n",
    "        data = self.graph_data\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings, preference_scores = self.model(data, travel_context)\n",
    "        \n",
    "        scores = preference_scores.squeeze()\n",
    "\n",
    "        if filter_useless:\n",
    "            for idx in range(len(scores)):\n",
    "                name = self.visit_area_df.iloc[idx]['VISIT_AREA_NM']\n",
    "                if self.processor.should_exclude_location(name):\n",
    "                    scores[idx] *= 0.1  # ë„ˆë¬´ ë‚®ì§€ ì•Šë„ë¡ ìˆ˜ì • ê°€ëŠ¥ (ì˜ˆ: 0.3)\n",
    "\n",
    "        if excluded_ids:\n",
    "            for exclude_id in excluded_ids:\n",
    "                matching_indices = self.visit_area_df[\n",
    "                    self.visit_area_df['NEW_VISIT_AREA_ID'] == exclude_id\n",
    "                ].index.tolist()\n",
    "                for idx in matching_indices:\n",
    "                    scores[idx] *= 0.1\n",
    "        \n",
    "        # ì¶”ì²œ ì ìˆ˜ ìµœì†Œ ì„ê³„ê°’ ì„¤ì •\n",
    "        min_allowed_score = 0.01\n",
    "        scores[scores < min_allowed_score] = min_allowed_score\n",
    "\n",
    "        recommendations = self._distance_aware_recommendation(\n",
    "            scores, embeddings, top_k, diversity_weight\n",
    "        )\n",
    "        \n",
    "        return recommendations, embeddings, preference_scores\n",
    "\n",
    "    \n",
    "    def _distance_aware_recommendation(self, scores, embeddings, top_k, diversity_weight):\n",
    "        \"\"\"ê±°ë¦¬ë¥¼ ê³ ë ¤í•œ ìˆœì°¨ì  ì¶”ì²œ\"\"\"\n",
    "        recommendations = []\n",
    "        remaining_indices = [i for i in range(len(scores)) if scores[i] > 0]\n",
    "        \n",
    "        if not remaining_indices:\n",
    "            return recommendations\n",
    "        \n",
    "        # ì ìˆ˜ê°€ ë†’ì€ ìƒìœ„ í›„ë³´ ì¤‘ì—ì„œ ì‹œì‘ì  ì„ íƒ\n",
    "        valid_scores = [(i, scores[i].item()) for i in remaining_indices]\n",
    "        valid_scores = sorted(valid_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # ìƒìœ„ 10ê°œ ì¤‘ì—ì„œ ëœë¤í•˜ê²Œ ì‹œì‘\n",
    "        top_candidates = valid_scores[:10]\n",
    "        if top_candidates:\n",
    "            start_idx = random.choice(top_candidates)[0]\n",
    "            recommendations.append(start_idx)\n",
    "            remaining_indices.remove(start_idx)\n",
    "        \n",
    "        # ë‚˜ë¨¸ì§€ ì¶”ì²œ: ê±°ë¦¬ì™€ ì ìˆ˜ë¥¼ ë™ì‹œì— ê³ ë ¤\n",
    "        while len(recommendations) < top_k and remaining_indices:\n",
    "            last_idx = recommendations[-1]\n",
    "            last_coords = self.visit_area_df.iloc[last_idx][['X_COORD', 'Y_COORD']].values\n",
    "            \n",
    "            best_score = -float('inf')\n",
    "            best_idx = None\n",
    "            \n",
    "            for idx in remaining_indices:\n",
    "                if scores[idx] <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # ê±°ë¦¬ ê³„ì‚°\n",
    "                curr_coords = self.visit_area_df.iloc[idx][['X_COORD', 'Y_COORD']].values\n",
    "                distance = np.sqrt(np.sum((last_coords - curr_coords) ** 2))\n",
    "                \n",
    "                # ê±°ë¦¬ í˜ë„í‹° (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)\n",
    "                distance_score = 1 / (1 + distance * 0.1)  # ê±°ë¦¬ê°€ ë©€ìˆ˜ë¡ ì ìˆ˜ ê°ì†Œ\n",
    "                \n",
    "                # ë‹¤ì–‘ì„± ê³„ì‚°\n",
    "                similarities = []\n",
    "                for rec_idx in recommendations:\n",
    "                    sim = F.cosine_similarity(\n",
    "                        embeddings[idx:idx+1], \n",
    "                        embeddings[rec_idx:rec_idx+1]\n",
    "                    ).item()\n",
    "                    similarities.append(sim)\n",
    "                \n",
    "                diversity = 1 - max(similarities) if similarities else 1\n",
    "                \n",
    "                # ìµœì¢… ì ìˆ˜: ì„ í˜¸ë„ + ê±°ë¦¬ + ë‹¤ì–‘ì„±\n",
    "                relevance = scores[idx].item()\n",
    "                final_score = (\n",
    "                    0.4 * relevance +  # ì„ í˜¸ë„\n",
    "                    0.4 * distance_score +  # ê±°ë¦¬\n",
    "                    0.2 * diversity  # ë‹¤ì–‘ì„±\n",
    "                )\n",
    "                \n",
    "                if final_score > best_score:\n",
    "                    best_score = final_score\n",
    "                    best_idx = idx\n",
    "            \n",
    "            if best_idx is not None:\n",
    "                recommendations.append(best_idx)\n",
    "                remaining_indices.remove(best_idx)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _mmr_recommendation(self, scores, embeddings, top_k, diversity_weight):\n",
    "        \"\"\"ê¸°ì¡´ MMR ê¸°ë°˜ ì¶”ì²œ (fallback)\"\"\"\n",
    "        recommendations = []\n",
    "        remaining_indices = list(range(len(scores)))\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ ì¶”ì²œ\n",
    "        if remaining_indices:\n",
    "            valid_scores = [(i, scores[i].item()) for i in remaining_indices if scores[i] > 0]\n",
    "            if valid_scores:\n",
    "                valid_scores = sorted(valid_scores, key=lambda x: x[1], reverse=True)\n",
    "                top_candidates = valid_scores[:5]\n",
    "                if top_candidates:\n",
    "                    best_idx = random.choice(top_candidates)[0]\n",
    "                    recommendations.append(best_idx)\n",
    "                    remaining_indices.remove(best_idx)\n",
    "        \n",
    "        # ë‚˜ë¨¸ì§€ ì¶”ì²œ\n",
    "        for _ in range(min(top_k - 1, len(remaining_indices))):\n",
    "            if not remaining_indices:\n",
    "                break\n",
    "                \n",
    "            best_score = -float('inf')\n",
    "            best_idx = None\n",
    "            \n",
    "            for idx in remaining_indices:\n",
    "                relevance = scores[idx].item()\n",
    "                \n",
    "                if relevance < 0:\n",
    "                    continue\n",
    "                \n",
    "                # ë‹¤ì–‘ì„± ê³„ì‚°\n",
    "                similarities = []\n",
    "                for rec_idx in recommendations:\n",
    "                    sim = F.cosine_similarity(\n",
    "                        embeddings[idx:idx+1], \n",
    "                        embeddings[rec_idx:rec_idx+1]\n",
    "                    ).item()\n",
    "                    similarities.append(sim)\n",
    "                \n",
    "                diversity = 1 - max(similarities) if similarities else 1\n",
    "                final_score = (1 - diversity_weight) * relevance + diversity_weight * diversity\n",
    "                \n",
    "                if final_score > best_score:\n",
    "                    best_score = final_score\n",
    "                    best_idx = idx\n",
    "            \n",
    "            if best_idx is not None:\n",
    "                recommendations.append(best_idx)\n",
    "                remaining_indices.remove(best_idx)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _apply_preference_weights(self, scores, embeddings):\n",
    "        \"\"\"í”¼ë“œë°± ê¸°ë°˜ ì ìˆ˜ ì¡°ì •\"\"\"\n",
    "        if not self.preference_weights:\n",
    "            return scores\n",
    "            \n",
    "        adjusted_scores = scores.clone()\n",
    "        \n",
    "        for i, row in self.visit_area_df.iterrows():\n",
    "            if i >= len(adjusted_scores):\n",
    "                break\n",
    "                \n",
    "            area_type = row.get('VISIT_AREA_TYPE_CD', 0)\n",
    "            \n",
    "            # ì„ í˜¸ íƒ€ì…ì´ë©´ ì ìˆ˜ ì¦ê°€\n",
    "            if area_type in self.preference_weights.get('preferred_types', []):\n",
    "                adjusted_scores[i] *= 1.2\n",
    "            \n",
    "            # ë¹„ì„ í˜¸ íƒ€ì…ì´ë©´ ì ìˆ˜ ê°ì†Œ\n",
    "            if area_type in self.preference_weights.get('avoided_types', []):\n",
    "                adjusted_scores[i] *= 0.8\n",
    "        \n",
    "        return adjusted_scores\n",
    "    \n",
    "    def update_with_feedback(self, liked_items, disliked_items, embeddings):\n",
    "        \"\"\"ì‚¬ìš©ì í”¼ë“œë°± ì—…ë°ì´íŠ¸\"\"\"\n",
    "        feedback = {\n",
    "            'liked': liked_items,\n",
    "            'disliked': disliked_items,\n",
    "            'embeddings': embeddings.cpu().numpy()\n",
    "        }\n",
    "        self.user_feedback_history.append(feedback)\n",
    "        \n",
    "        self.preference_weights = self._calculate_preference_weights()\n",
    "        \n",
    "        print(f\"âœ… í”¼ë“œë°± ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì¢‹ì•„ìš” {len(liked_items)}ê°œ, ì‹«ì–´ìš” {len(disliked_items)}ê°œ\")\n",
    "        \n",
    "        return self.preference_weights\n",
    "    \n",
    "    def _calculate_preference_weights(self):\n",
    "        \"\"\"í”¼ë“œë°± íˆìŠ¤í† ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„ í˜¸ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°\"\"\"\n",
    "        if not self.user_feedback_history:\n",
    "            return None\n",
    "            \n",
    "        liked_features = []\n",
    "        disliked_features = []\n",
    "        \n",
    "        for feedback in self.user_feedback_history:\n",
    "            for item_idx in feedback['liked']:\n",
    "                if item_idx < len(self.visit_area_df):\n",
    "                    liked_features.append(self.visit_area_df.iloc[item_idx])\n",
    "            \n",
    "            for item_idx in feedback['disliked']:\n",
    "                if item_idx < len(self.visit_area_df):\n",
    "                    disliked_features.append(self.visit_area_df.iloc[item_idx])\n",
    "        \n",
    "        preferred_types = [item.get('VISIT_AREA_TYPE_CD', 0) for item in liked_features]\n",
    "        avoided_types = [item.get('VISIT_AREA_TYPE_CD', 0) for item in disliked_features]\n",
    "        \n",
    "        return {\n",
    "            'preferred_types': list(set(preferred_types)),\n",
    "            'avoided_types': list(set(avoided_types)),\n",
    "            'preferred_regions': [(item.get('X_COORD', 0), item.get('Y_COORD', 0)) for item in liked_features]\n",
    "        }\n",
    "\n",
    "class OptimizedRouteGenerator:\n",
    "    def __init__(self, distance_threshold_km=50):  # ì„ê³„ê°’ì„ 50kmë¡œ ì¤„ì„\n",
    "        self.distance_threshold_km = distance_threshold_km\n",
    "        \n",
    "    \n",
    "    def calculate_distance(self, coord1, coord2):\n",
    "        from math import radians, cos, sin, sqrt, atan2\n",
    "        try:\n",
    "            R = 6371  # ì§€êµ¬ ë°˜ê²½(km)\n",
    "\n",
    "            lat1, lon1 = radians(coord1[1]), radians(coord1[0])\n",
    "            lat2, lon2 = radians(coord2[1]), radians(coord2[0])\n",
    "\n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "\n",
    "            a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "            c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "            distance = R * c\n",
    "            return distance\n",
    "        except:\n",
    "            # ì˜ˆì™¸ ë°œìƒ ì‹œ ë§¤ìš° í° ê°’ì„ ë°˜í™˜í•´ ì´ìƒì¹˜ ì²˜ë¦¬\n",
    "            return float('inf')\n",
    "        \n",
    "        \n",
    "    def _two_opt_improvement(self, route, coords):\n",
    "        \"\"\"2-opt ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²½ë¡œ ê°œì„ \"\"\"\n",
    "        n = len(route)\n",
    "        if n <= 3:\n",
    "            return route\n",
    "            \n",
    "        improved = True\n",
    "        best_route = route[:]\n",
    "        \n",
    "        while improved:\n",
    "            improved = False\n",
    "            for i in range(1, n - 2):\n",
    "                for j in range(i + 1, n):\n",
    "                    if j - i == 1:\n",
    "                        continue\n",
    "                    \n",
    "                    new_route = best_route[:]\n",
    "                    new_route[i:j] = reversed(new_route[i:j])\n",
    "                    \n",
    "                    if self._route_distance(new_route, coords) < self._route_distance(best_route, coords):\n",
    "                        best_route = new_route\n",
    "                        improved = True\n",
    "        \n",
    "        return best_route\n",
    "    \n",
    "    def _route_distance(self, route, coords):\n",
    "        \"\"\"ê²½ë¡œì˜ ì´ ê±°ë¦¬ ê³„ì‚°\"\"\"\n",
    "        total_distance = 0\n",
    "        for i in range(len(route) - 1):\n",
    "            total_distance += self.calculate_distance(\n",
    "                coords[route[i]], \n",
    "                coords[route[i + 1]]\n",
    "            )\n",
    "        return total_distance\n",
    "    \n",
    "    def _solve_tsp_simple(self, locations):\n",
    "        \"\"\"ê°„ë‹¨í•œ TSP í•´ë²•\"\"\"\n",
    "        if len(locations) <= 2:\n",
    "            return locations\n",
    "            \n",
    "        coords = np.array([loc['coords'] for loc in locations])\n",
    "        n = len(coords)\n",
    "        \n",
    "        # Nearest Neighbor\n",
    "        unvisited = set(range(1, n))\n",
    "        current = 0\n",
    "        route = [0]\n",
    "        \n",
    "        while unvisited:\n",
    "            nearest = min(unvisited, \n",
    "                         key=lambda x: self.calculate_distance(coords[current], coords[x]))\n",
    "            route.append(nearest)\n",
    "            unvisited.remove(nearest)\n",
    "            current = nearest\n",
    "        \n",
    "        # 2-opt improvement\n",
    "        route = self._two_opt_improvement(route, coords)\n",
    "        \n",
    "        return [locations[i] for i in route]\n",
    "        \n",
    "    def remove_outliers(self, coords, locations):\n",
    "        \"\"\"ê±°ë¦¬ ì´ìƒì¹˜ ì œê±°\"\"\"\n",
    "        if len(coords) <= 2:\n",
    "            return coords, locations\n",
    "            \n",
    "        # ì¤‘ì‹¬ì  ê³„ì‚°\n",
    "        center = np.mean(coords, axis=0)\n",
    "        \n",
    "        # ê° ì ê³¼ ì¤‘ì‹¬ì ì˜ ê±°ë¦¬ ê³„ì‚°\n",
    "        distances = [self.calculate_distance(coord, center) for coord in coords]\n",
    "        \n",
    "        # ê±°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ (ë„ˆë¬´ ë¨¼ ê³³ ì œì™¸)\n",
    "        filtered_indices = []\n",
    "        for i, dist in enumerate(distances):\n",
    "            if dist <= self.distance_threshold_km:\n",
    "                filtered_indices.append(i)\n",
    "        \n",
    "        # ìµœì†Œ ì¥ì†Œ ìˆ˜ ìœ ì§€\n",
    "        if len(filtered_indices) < 6:\n",
    "            # ê±°ë¦¬ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ê¹Œìš´ ê³³ë¶€í„° ì„ íƒ\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            filtered_indices = sorted_indices[:min(10, len(distances))].tolist()\n",
    "        \n",
    "        filtered_coords = [coords[i] for i in filtered_indices]\n",
    "        filtered_locations = [locations[i] for i in filtered_indices]\n",
    "        \n",
    "        return np.array(filtered_coords), filtered_locations\n",
    "        \n",
    "    def generate_daily_routes(self, recommendations, visit_area_df, travel_duration, \n",
    "                            optimization_method='region_based'):\n",
    "        \"\"\"ì§€ì—­ ê¸°ë°˜ ì¼ë³„ ìµœì  ê²½ë¡œ ìƒì„±\"\"\"\n",
    "        if travel_duration <= 0:\n",
    "            travel_duration = 1\n",
    "            \n",
    "        coords = []\n",
    "        locations = []\n",
    "        \n",
    "        for idx in recommendations:\n",
    "            if idx < len(visit_area_df):\n",
    "                row = visit_area_df.iloc[idx]\n",
    "                coords.append([row['X_COORD'], row['Y_COORD']])\n",
    "                locations.append({\n",
    "                    'id': row['NEW_VISIT_AREA_ID'],\n",
    "                    'name': row['VISIT_AREA_NM'],\n",
    "                    'coords': [row['X_COORD'], row['Y_COORD']],\n",
    "                    'idx': idx,\n",
    "                    'type': row.get('VISIT_AREA_TYPE_CD', 0)\n",
    "                })\n",
    "        \n",
    "        if len(coords) == 0:\n",
    "            return {}\n",
    "            \n",
    "        coords = np.array(coords)\n",
    "        coords = np.nan_to_num(coords, nan=0.0)\n",
    "        \n",
    "        # ì´ìƒì¹˜ ì œê±°\n",
    "        coords, locations = self.remove_outliers(coords, locations)\n",
    "        \n",
    "        # ì¼ìˆ˜ì— ë§ê²Œ ì¥ì†Œ ìˆ˜ ì¡°ì •\n",
    "        places_per_day = max(3, min(5, len(locations) // travel_duration))\n",
    "        total_places = min(places_per_day * travel_duration, len(locations))\n",
    "        \n",
    "        # ì§€ì—­ë³„ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì¼ì • ìƒì„±\n",
    "        if travel_duration == 1:\n",
    "            # 1ì¼ ì—¬í–‰: TSPë¡œ ìµœì  ê²½ë¡œë§Œ ìƒì„±\n",
    "            optimized_order = self._solve_tsp_with_start(locations)\n",
    "            return {0: optimized_order[:places_per_day]}\n",
    "        else:\n",
    "            # ë‹¤ì¼ ì—¬í–‰: ì§€ì—­ë³„ë¡œ ë¬¶ê¸°\n",
    "            return self._create_regional_routes(coords, locations, travel_duration, places_per_day)\n",
    "    \n",
    "    def _create_regional_routes(self, coords, locations, travel_duration, places_per_day):\n",
    "        \"\"\"ì§€ì—­ë³„ë¡œ ë¬¶ì–´ì„œ ì¼ì • ìƒì„±\"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        \n",
    "        # ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •\n",
    "        n_clusters = min(travel_duration, len(locations) // 2)\n",
    "        \n",
    "        if n_clusters < 2:\n",
    "            # ì¥ì†Œê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê· ë“± ë¶„ë°°\n",
    "            return self._equal_distribution(locations, travel_duration, places_per_day)\n",
    "        \n",
    "        # K-means í´ëŸ¬ìŠ¤í„°ë§\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(coords)\n",
    "        \n",
    "        # í´ëŸ¬ìŠ¤í„°ë³„ ê·¸ë£¹í™”\n",
    "        clusters = {}\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append((i, locations[i]))\n",
    "        \n",
    "        # í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ ê³„ì‚° (ìˆœì„œ ê²°ì •ìš©)\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        cluster_order = self._order_clusters(cluster_centers)\n",
    "        \n",
    "        # ì¼ìë³„ ë°°ì •\n",
    "        daily_routes = {}\n",
    "        locations_per_day = len(locations) // travel_duration\n",
    "        remainder = len(locations) % travel_duration\n",
    "        \n",
    "        current_day = 0\n",
    "        current_day_locations = []\n",
    "        \n",
    "        for cluster_idx in cluster_order:\n",
    "            if cluster_idx in clusters:\n",
    "                cluster_locations = [loc for _, loc in clusters[cluster_idx]]\n",
    "                \n",
    "                # TSPë¡œ í´ëŸ¬ìŠ¤í„° ë‚´ ìµœì  ê²½ë¡œ\n",
    "                if len(cluster_locations) > 1:\n",
    "                    cluster_locations = self._solve_tsp_simple(cluster_locations)\n",
    "                \n",
    "                for loc in cluster_locations:\n",
    "                    current_day_locations.append(loc)\n",
    "                    \n",
    "                    # ì¼ìë³„ í• ë‹¹ëŸ‰ ì²´í¬\n",
    "                    day_quota = locations_per_day + (1 if current_day < remainder else 0)\n",
    "                    if len(current_day_locations) >= day_quota:\n",
    "                        daily_routes[current_day] = current_day_locations\n",
    "                        current_day += 1\n",
    "                        current_day_locations = []\n",
    "                        \n",
    "                        if current_day >= travel_duration:\n",
    "                            break\n",
    "                \n",
    "                if current_day >= travel_duration:\n",
    "                    break\n",
    "        \n",
    "        # ë‚¨ì€ ì¥ì†Œ ì²˜ë¦¬\n",
    "        if current_day_locations and current_day < travel_duration:\n",
    "            daily_routes[current_day] = current_day_locations\n",
    "        \n",
    "        # ê° ì¼ìë³„ ì¥ì†Œ ìˆ˜ ì¡°ì •\n",
    "        return self._adjust_daily_balance(daily_routes, places_per_day)\n",
    "    \n",
    "    def _order_clusters(self, cluster_centers):\n",
    "        \"\"\"í´ëŸ¬ìŠ¤í„°ë¥¼ ê°€ê¹Œìš´ ìˆœì„œë¡œ ì •ë ¬\"\"\"\n",
    "        n_clusters = len(cluster_centers)\n",
    "        if n_clusters <= 1:\n",
    "            return list(range(n_clusters))\n",
    "        \n",
    "        # ì²« í´ëŸ¬ìŠ¤í„°ëŠ” ê°€ì¥ ë‚¨ìª½ (ë˜ëŠ” ì„œìª½)\n",
    "        start_idx = np.argmin(cluster_centers[:, 1])  # Y ì¢Œí‘œ ê¸°ì¤€\n",
    "        \n",
    "        visited = [start_idx]\n",
    "        current = start_idx\n",
    "        \n",
    "        while len(visited) < n_clusters:\n",
    "            min_dist = float('inf')\n",
    "            next_idx = None\n",
    "            \n",
    "            for i in range(n_clusters):\n",
    "                if i not in visited:\n",
    "                    dist = self.calculate_distance(\n",
    "                        cluster_centers[current], \n",
    "                        cluster_centers[i]\n",
    "                    )\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        next_idx = i\n",
    "            \n",
    "            if next_idx is not None:\n",
    "                visited.append(next_idx)\n",
    "                current = next_idx\n",
    "        \n",
    "        return visited\n",
    "    \n",
    "    def _equal_distribution(self, locations, travel_duration, places_per_day):\n",
    "        \"\"\"ê· ë“± ë¶„ë°°\"\"\"\n",
    "        daily_routes = {}\n",
    "        locations_per_day = len(locations) // travel_duration\n",
    "        remainder = len(locations) % travel_duration\n",
    "        \n",
    "        start_idx = 0\n",
    "        for day in range(travel_duration):\n",
    "            count = locations_per_day + (1 if day < remainder else 0)\n",
    "            count = min(count, places_per_day)  # ì¼ë³„ ìµœëŒ€ ì¥ì†Œ ìˆ˜ ì œí•œ\n",
    "            daily_routes[day] = locations[start_idx:start_idx + count]\n",
    "            start_idx += count\n",
    "        \n",
    "        return daily_routes\n",
    "    \n",
    "    def _adjust_daily_balance(self, daily_routes, target_size):\n",
    "        \"\"\"ì¼ë³„ ì¥ì†Œ ìˆ˜ ê· í˜• ì¡°ì •\"\"\"\n",
    "        adjusted = {}\n",
    "        \n",
    "        for day, locations in daily_routes.items():\n",
    "            if len(locations) > target_size:\n",
    "                # ë„ˆë¬´ ë§ìœ¼ë©´ ì˜ë¼ë‚´ê¸°\n",
    "                adjusted[day] = locations[:target_size]\n",
    "            elif len(locations) < 2:\n",
    "                # ë„ˆë¬´ ì ìœ¼ë©´ ë‹¤ë¥¸ ë‚ ì—ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "                continue\n",
    "            else:\n",
    "                adjusted[day] = locations\n",
    "        \n",
    "        return adjusted\n",
    "    \n",
    "    def _solve_tsp_with_start(self, locations):\n",
    "        \"\"\"ì‹œì‘ì ì„ ê³ ë ¤í•œ TSP\"\"\"\n",
    "        if len(locations) <= 2:\n",
    "            return locations\n",
    "        \n",
    "        coords = np.array([loc['coords'] for loc in locations])\n",
    "        n = len(coords)\n",
    "        \n",
    "        # ê°€ì¥ ì ‘ê·¼í•˜ê¸° ì‰¬ìš´ ê³³ì„ ì‹œì‘ì ìœ¼ë¡œ (ê°€ì¥ ì„œìª½ ë˜ëŠ” ë‚¨ìª½)\n",
    "        start = np.argmin(coords[:, 1])  # Y ì¢Œí‘œ ê¸°ì¤€\n",
    "        \n",
    "        # Nearest Neighbor from start\n",
    "        unvisited = set(range(n))\n",
    "        unvisited.remove(start)\n",
    "        current = start\n",
    "        route = [start]\n",
    "        \n",
    "        while unvisited:\n",
    "            nearest = min(unvisited, \n",
    "                         key=lambda x: self.calculate_distance(coords[current], coords[x]))\n",
    "            route.append(nearest)\n",
    "            unvisited.remove(nearest)\n",
    "            current = nearest\n",
    "        \n",
    "        # 2-opt improvement\n",
    "        route = self._two_opt_improvement(route, coords)\n",
    "        \n",
    "        return [locations[i] for i in route]\n",
    "        \n",
    "\n",
    "def process_travel_input(travel_info: dict):\n",
    "    \"\"\"ì—¬í–‰ ì •ë³´ ì „ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
    "    travel_feature_cols = [\n",
    "        'TOTAL_COST_BINNED_ENCODED', 'WITH_PET', 'MONTH', 'DURATION',\n",
    "        'MVMN_ê¸°íƒ€', 'MVMN_ëŒ€ì¤‘êµí†µ', 'MVMN_ìê°€ìš©',\n",
    "        'TRAVEL_PURPOSE_1', 'TRAVEL_PURPOSE_2', 'TRAVEL_PURPOSE_3',\n",
    "        'TRAVEL_PURPOSE_4', 'TRAVEL_PURPOSE_5', 'TRAVEL_PURPOSE_6',\n",
    "        'TRAVEL_PURPOSE_7', 'TRAVEL_PURPOSE_8', 'TRAVEL_PURPOSE_9',\n",
    "        'WHOWITH_2ì¸ì—¬í–‰', 'WHOWITH_ê°€ì¡±ì—¬í–‰', 'WHOWITH_ê¸°íƒ€',\n",
    "        'WHOWITH_ë‹¨ë…ì—¬í–‰', 'WHOWITH_ì¹œêµ¬/ì§€ì¸ ì—¬í–‰'\n",
    "    ]\n",
    "    \n",
    "    # ë°˜ë ¤ë™ë¬¼ ë™ë°˜\n",
    "    travel_info['mission_ENC'] = travel_info['mission_ENC'].strip().split(',')\n",
    "    travel_info['WITH_PET'] = 1 if '0' in travel_info['mission_ENC'] else 0\n",
    "        \n",
    "    # ì—¬í–‰ ëª©ì \n",
    "    for i in range(1, 10):\n",
    "        travel_info[f'TRAVEL_PURPOSE_{i}'] = 1 if str(i) in travel_info['mission_ENC'] else 0\n",
    "        \n",
    "    # ë‚ ì§œ ì²˜ë¦¬\n",
    "    dates = travel_info['date_range'].split(' - ')\n",
    "    start_date = datetime.strptime(dates[0].strip(), \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(dates[1].strip(), \"%Y-%m-%d\")\n",
    "    \n",
    "    travel_info['MONTH'] = end_date.month\n",
    "    travel_info['DURATION'] = (end_date - start_date).days + 1  # +1 ì¶”ê°€\n",
    "    \n",
    "    # êµí†µìˆ˜ë‹¨\n",
    "    for m in ['ìê°€ìš©', 'ëŒ€ì¤‘êµí†µ', 'ê¸°íƒ€']:\n",
    "        travel_info[f\"MVMN_{m}\"] = 0\n",
    "    \n",
    "    if travel_info['MVMN_NM_ENC'] == '1':\n",
    "        travel_info['MVMN_ìê°€ìš©'] = 1\n",
    "    elif travel_info['MVMN_NM_ENC'] == '2':\n",
    "        travel_info['MVMN_ëŒ€ì¤‘êµí†µ'] = 1\n",
    "    else:\n",
    "        travel_info['MVMN_ê¸°íƒ€'] = 1\n",
    "    \n",
    "    # ë™í–‰ì\n",
    "    whowith_onehot = [0] * 5\n",
    "    idx = int(travel_info['whowith_ENC']) - 1\n",
    "    if 0 <= idx < 5:\n",
    "        whowith_onehot[idx] = 1\n",
    "    \n",
    "    travel_info.update({\n",
    "        'WHOWITH_ë‹¨ë…ì—¬í–‰': whowith_onehot[0],\n",
    "        'WHOWITH_2ì¸ì—¬í–‰': whowith_onehot[1],\n",
    "        'WHOWITH_ê°€ì¡±ì—¬í–‰': whowith_onehot[2],\n",
    "        'WHOWITH_ì¹œêµ¬/ì§€ì¸ ì—¬í–‰': whowith_onehot[3],\n",
    "        'WHOWITH_ê¸°íƒ€': whowith_onehot[4],\n",
    "    })\n",
    "    \n",
    "    # ë¹„ìš©\n",
    "    travel_info['TOTAL_COST_BINNED_ENCODED'] = int(travel_info['TOTAL_COST'])\n",
    "    \n",
    "    # ìµœì¢… ë²¡í„° ìƒì„±\n",
    "    travel_vector = [int(travel_info.get(k, 0)) for k in travel_feature_cols]\n",
    "    \n",
    "    return np.array([travel_vector]).astype(np.float32)\n",
    "\n",
    "def simulate_user_feedback():\n",
    "    \"\"\"ì‚¬ìš©ì í”¼ë“œë°± ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
    "    feedback_options = [\n",
    "        {\"liked\": [], \"disliked\": [0, 2]},  # ì²« ë²ˆì§¸ì™€ ì„¸ ë²ˆì§¸ ì¥ì†Œ ì‹«ì–´ìš”\n",
    "        {\"liked\": [1], \"disliked\": [4, 7]},  # ë‘ ë²ˆì§¸ ì¥ì†Œ ì¢‹ì•„ìš”, ë‹¤ë¥¸ ê³³ë“¤ ì‹«ì–´ìš”\n",
    "        {\"liked\": [0, 3], \"disliked\": [5]},  # ë³µìˆ˜ ì¢‹ì•„ìš”/ì‹«ì–´ìš”\n",
    "    ]\n",
    "    \n",
    "    return random.choice(feedback_options)\n",
    "\n",
    "def main_feedback_test(travel_example):\n",
    "    print(\"ğŸš€ ê°œì„ ëœ í”¼ë“œë°± ê¸°ë°˜ ê²½ë¡œ ëŒ€ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    travel_tensor = process_travel_input(travel_example)\n",
    "    travel_duration = int(travel_tensor[0, 3])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "    print(f\"ğŸ“… ì—¬í–‰ ê¸°ê°„: {travel_example['date_range']} ({travel_tensor[0, 3]:.0f}ì¼)\")\n",
    "    \n",
    "    travel_context_tensor = torch.tensor(travel_tensor, dtype=torch.float32).to(device)\n",
    "    \n",
    "    recommender = SmartRecommendationEngine(device)\n",
    "    \n",
    "    # ì´ˆê¸° ì¶”ì²œ (í•„í„°ë§ ì ìš©, ê±°ë¦¬ ê³ ë ¤)\n",
    "    recommendations, embeddings, _ = recommender.get_recommendations(\n",
    "        travel_context_tensor, top_k=30, diversity_weight=0.3, \n",
    "        filter_useless=True, consider_distance=True\n",
    "    )\n",
    "    \n",
    "    optimized_routes, unique_recommendations = recommender.optimize_routes(recommendations, travel_tensor)\n",
    "    \n",
    "    print(\"\\nğŸ—“ï¸ ì´ˆê¸° ì—¬í–‰ ì¼ì • (ìµœì í™” ë° í•„í„°ë§ ì ìš©):\")\n",
    "    total_places = 0\n",
    "    for day, route in sorted(optimized_routes.items()):\n",
    "        print(f\"\\nğŸ“… Day {day + 1}:\")\n",
    "        for i, loc in enumerate(route):\n",
    "            print(f\" {i+1}. [{loc['id']:3d}] {loc['name']}\")\n",
    "        total_places += len(route)\n",
    "    print(f\"\\nì´ {total_places}ê°œ ì¥ì†Œ ì¶”ì²œ\")\n",
    "    \n",
    "    # í”¼ë“œë°± ë¼ìš´ë“œ ë°˜ë³µ\n",
    "    for round_num in range(1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ”„ í”¼ë“œë°± ë¼ìš´ë“œ {round_num + 1}\")\n",
    "        \n",
    "        feedback = simulate_user_feedback()\n",
    "        optimized_routes = recommender.feedback_model(feedback, travel_context_tensor, travel_duration, unique_recommendations, embeddings)\n",
    "        \n",
    "        print(\"\\nğŸ¯ í”¼ë“œë°± ë°˜ì˜ í›„ ìµœì í™”ëœ ì—¬í–‰ ì¼ì •:\")\n",
    "        total_places = 0\n",
    "        for day, route in sorted(optimized_routes.items()):\n",
    "            print(f\"\\nğŸ“… Day {day + 1}:\")\n",
    "            for i, loc in enumerate(route):\n",
    "                print(f\" {i+1}. [{loc['id']:3d}] {loc['name']}\")\n",
    "            total_places += len(route)\n",
    "        print(f\"\\nì´ {total_places}ê°œ ì¥ì†Œ ì¶”ì²œ\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ì—¬í–‰ ì •ë³´ (2ì¼ ì—¬í–‰)\n",
    "    travel_example = {\n",
    "        'mission_ENC': '0,1,2',\n",
    "        'date_range': '2025-09-28 - 2025-09-29',  # 2ì¼ ì—¬í–‰ìœ¼ë¡œ ë³€ê²½\n",
    "        'start_date': '',\n",
    "        'end_date': '',\n",
    "        'TOTAL_COST': '2',\n",
    "        'MVMN_NM_ENC': '2',\n",
    "        'whowith_ENC': '2',\n",
    "        'mission_type': 'normal'\n",
    "    }\n",
    "    main_feedback_test(travel_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
