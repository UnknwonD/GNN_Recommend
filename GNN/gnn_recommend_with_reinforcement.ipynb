{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ í”¼ë“œë°± ê¸°ë°˜ ê²½ë¡œ ëŒ€ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘!\n",
      "============================================================\n",
      "ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: cpu\n",
      "\n",
      "ğŸ—“ï¸ ì´ˆê¸° ì—¬í–‰ ì¼ì • (ìµœì í™”):\n",
      "\n",
      "ğŸ“… Day 1:\n",
      " - [634] ê·¸ëœë“œ ì¸í„°ì»¨í‹°ë„¨íƒˆ ì„œìš¸ íŒŒë¥´ë‚˜ìŠ¤\n",
      " - [2399] ê·¸ëœë“œ í•˜ì–íŠ¸ ì„œìš¸\n",
      " - [2367] ë…¼ ë“œë¼ì´\n",
      " - [7810] ë³´ë¬¸ì‚¬\n",
      " - [1799] ì¡°ì•½ëŒ ìˆ¯ë¶ˆë‹­ê°ˆë¹„\n",
      " - [3168] ê²½í¬ëŒ€\n",
      " - [350] ì´ì¸ íœ´ê²Œì†Œ ì²œì•ˆ ë°©í–¥\n",
      "\n",
      "ğŸ“… Day 2:\n",
      " - [8094] ë™êµ¬ ê³µì˜ì£¼ì°¨ë¹Œë”©\n",
      " - [5015] ì‚¬ì²œ ì‹œì™¸ë²„ìŠ¤í„°ë¯¸ë„\n",
      " - [184] ì œì£¼ êµ­ì œê³µí•­\n",
      "\n",
      "ğŸ”„ í”¼ë“œë°± ë¼ìš´ë“œ 1\n",
      "âœ… í”¼ë“œë°± ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì¢‹ì•„ìš” 1ê°œ, ì‹«ì–´ìš” 2ê°œ\n",
      "\n",
      "ğŸ¯ í”¼ë“œë°± ë°˜ì˜ í›„ ìµœì í™”ëœ ì—¬í–‰ ì¼ì •:\n",
      "\n",
      "ğŸ“… Day 1:\n",
      " - [4098] ì ì‹¤ì¢…í•©ìš´ë™ì¥\n",
      " - [982] ì½”ì—‘ìŠ¤\n",
      " - [8233] ì¤‘ë‘ ì•„íŠ¸ì„¼í„°\n",
      " - [187] ì„œìš¸ì‹œë¦½ë¯¸ìˆ ê´€ ì„œì†Œë¬¸ ë³¸ê´€\n",
      " - [9178] ì¸ì²œ SSG ëœë”ìŠ¤ í•„ë“œ\n",
      " - [432] ì½©ì¹˜ë…¸ ì½˜í¬ë¦¬íŠ¸\n",
      " - [1446] ê°€í‰ ë®¤ì§ ë¹Œë¦¬ì§€ ìŒì•… ì—­ 1939\n",
      " - [8200] ì´ì¬íš¨ ê°¤ëŸ¬ë¦¬\n",
      " - [6217] í•‘í¬ íƒ€ì´ê±°\n",
      " - [184] ì œì£¼ êµ­ì œê³µí•­\n",
      "\n",
      "ğŸ”„ í”¼ë“œë°± ë¼ìš´ë“œ 2\n",
      "âœ… í”¼ë“œë°± ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì¢‹ì•„ìš” 0ê°œ, ì‹«ì–´ìš” 2ê°œ\n",
      "\n",
      "ğŸ¯ í”¼ë“œë°± ë°˜ì˜ í›„ ìµœì í™”ëœ ì—¬í–‰ ì¼ì •:\n",
      "\n",
      "ğŸ“… Day 1:\n",
      " - [8094] ë™êµ¬ ê³µì˜ì£¼ì°¨ë¹Œë”©\n",
      " - [184] ì œì£¼ êµ­ì œê³µí•­\n",
      "\n",
      "ğŸ“… Day 2:\n",
      " - [4097] ì²œì•ˆì‚¼ê±°ë¦¬íœ´ê²Œì†Œ ì„œìš¸ ë°©í–¥\n",
      " - [350] ì´ì¸ íœ´ê²Œì†Œ ì²œì•ˆ ë°©í–¥\n",
      " - [2101] ì„œìš¸ ê°ˆì‚° ì´ˆë“±í•™êµ í›„ë¬¸\n",
      " - [2399] ê·¸ëœë“œ í•˜ì–íŠ¸ ì„œìš¸\n",
      " - [3047] ìµì§€ ì¥ì‚¬\n",
      " - [8028] ëŒ€ë£¡ì‹œì¥\n",
      " - [6684] ì´ë§ˆíŠ¸ ì¶˜ì²œì \n",
      " - [3168] ê²½í¬ëŒ€\n",
      "\n",
      "ğŸ”„ í”¼ë“œë°± ë¼ìš´ë“œ 3\n",
      "âœ… í”¼ë“œë°± ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì¢‹ì•„ìš” 2ê°œ, ì‹«ì–´ìš” 1ê°œ\n",
      "\n",
      "ğŸ¯ í”¼ë“œë°± ë°˜ì˜ í›„ ìµœì í™”ëœ ì—¬í–‰ ì¼ì •:\n",
      "\n",
      "ğŸ“… Day 1:\n",
      " - [570] ë¹„ì¼€ì´ ì—ë„ˆì§€ ì„ì‚° ì£¼ìœ ì†Œ\n",
      " - [3264] CU ì—¬ì˜ì˜ë¬´ì˜ˆë‹¤ìŒì \n",
      " - [2043] ìƒê·¹ ë†í˜‘ í•˜ë‚˜ë¡œë§ˆíŠ¸\n",
      " - [8551] ê´‘ê°œí† íƒœì™• ê´‘ì¥\n",
      " - [2399] ê·¸ëœë“œ í•˜ì–íŠ¸ ì„œìš¸\n",
      " - [404] ê¸€ë¨ë¹„ê¸€ë¨í•‘\n",
      " - [8028] ëŒ€ë£¡ì‹œì¥\n",
      " - [4418] ë™ì†¡ì „í†µì‹œì¥\n",
      " - [3168] ê²½í¬ëŒ€\n",
      " - [140] ë„¤ì´ì²˜ ë¹Œ\n",
      "\n",
      "âœ… ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ê²½ë¡œ ëŒ€ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, GATConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ImprovedTravelGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, travel_context_dim, \n",
    "                 num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gat1 = GATConv(in_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True)\n",
    "        self.gat2 = GATConv(hidden_channels, hidden_channels // num_heads, \n",
    "                           heads=num_heads, dropout=dropout, concat=True)\n",
    "        self.gat3 = GATConv(hidden_channels, out_channels, \n",
    "                           heads=1, dropout=dropout, concat=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.travel_encoder = nn.Sequential(\n",
    "            nn.Linear(travel_context_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.fusion_net = nn.Sequential(\n",
    "            nn.Linear(out_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.preference_head = nn.Sequential(\n",
    "            nn.Linear(out_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, data, travel_context, return_attention=False):\n",
    "        x = data['visit_area'].x\n",
    "        edge_index = data['visit_area', 'moved_to', 'visit_area'].edge_index\n",
    "        \n",
    "        x1 = self.gat1(x, edge_index)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        x2 = self.gat2(x1, edge_index)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2 + x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        graph_embedding = self.gat3(x2, edge_index)\n",
    "        graph_embedding = self.bn3(graph_embedding)\n",
    "        \n",
    "        travel_embedding = self.travel_encoder(travel_context)\n",
    "        travel_embedding_expanded = travel_embedding.expand(graph_embedding.size(0), -1)\n",
    "        \n",
    "        fused_features = torch.cat([graph_embedding, travel_embedding_expanded], dim=1)\n",
    "        final_embedding = self.fusion_net(fused_features)\n",
    "        \n",
    "        preference_scores = self.preference_head(final_embedding)\n",
    "        \n",
    "        return final_embedding, preference_scores\n",
    "\n",
    "class EnhancedDataProcessor:\n",
    "    def __init__(self):\n",
    "        self.visit_scaler = RobustScaler()\n",
    "        self.travel_scaler = StandardScaler()\n",
    "        \n",
    "    def process_visit_area_features(self, visit_area_df):\n",
    "        visit_area_df = visit_area_df.copy()\n",
    "        \n",
    "        # ì¢Œí‘œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "        visit_area_df['X_COORD'] = visit_area_df['X_COORD'].fillna(visit_area_df['X_COORD'].mean())\n",
    "        visit_area_df['Y_COORD'] = visit_area_df['Y_COORD'].fillna(visit_area_df['Y_COORD'].mean())\n",
    "        visit_area_df['VISIT_CHC_REASON_CD'] = visit_area_df['VISIT_CHC_REASON_CD'].fillna(0)\n",
    "        \n",
    "        features = visit_area_df[['X_COORD', 'Y_COORD']].copy()\n",
    "        \n",
    "        # One-hot encoding\n",
    "        type_onehot = pd.get_dummies(visit_area_df['VISIT_AREA_TYPE_CD'], prefix='type')\n",
    "        reason_onehot = pd.get_dummies(visit_area_df['VISIT_CHC_REASON_CD'].fillna(0), prefix='reason')\n",
    "        \n",
    "        # ì •ê·œí™”ëœ ë§Œì¡±ë„ ì ìˆ˜\n",
    "        for col in ['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']:\n",
    "            visit_area_df[col] = visit_area_df[col].fillna(3)\n",
    "            visit_area_df[f'{col}_norm'] = (visit_area_df[col] - 1) / 4.0\n",
    "        \n",
    "        # ì¸ê¸°ë„ ì ìˆ˜\n",
    "        visit_area_df['popularity_score'] = (\n",
    "            visit_area_df['DGSTFN_norm'] * 0.4 + \n",
    "            visit_area_df['REVISIT_INTENTION_norm'] * 0.3 + \n",
    "            visit_area_df['RCMDTN_INTENTION_norm'] * 0.3\n",
    "        )\n",
    "        \n",
    "        # ëª¨ë“  íŠ¹ì„± ê²°í•©\n",
    "        features = pd.concat([\n",
    "            features, type_onehot, reason_onehot,\n",
    "            visit_area_df[['DGSTFN_norm', 'REVISIT_INTENTION_norm', 'RCMDTN_INTENTION_norm', 'popularity_score']]\n",
    "        ], axis=1)\n",
    "        \n",
    "        return self.visit_scaler.fit_transform(features.values.astype(np.float32))\n",
    "    \n",
    "    def create_enhanced_edges(self, move_df, visit_area_df):\n",
    "        edges = []\n",
    "        edge_weights = []\n",
    "        \n",
    "        for travel_id, group in move_df.groupby(\"TRAVEL_ID\"):\n",
    "            group = group.sort_values(\"TRIP_ID\").reset_index(drop=True)\n",
    "            \n",
    "            for i in range(1, len(group)):\n",
    "                from_id = group.loc[i-1, \"END_NEW_ID\"]\n",
    "                to_id = group.loc[i, \"END_NEW_ID\"]\n",
    "                \n",
    "                if pd.notna(from_id) and pd.notna(to_id):\n",
    "                    duration = group.loc[i, \"DURATION_MINUTES\"] if \"DURATION_MINUTES\" in group.columns else 0\n",
    "                    transport = group.loc[i, \"MVMN_CD_1\"] if \"MVMN_CD_1\" in group.columns else 0\n",
    "                    \n",
    "                    edges.append([int(from_id), int(to_id), duration, transport])\n",
    "                    edge_weights.append(1.0)  # ê¸°ë³¸ ê°€ì¤‘ì¹˜\n",
    "        \n",
    "        edges_df = pd.DataFrame(edges, columns=[\"FROM_ID\", \"TO_ID\", \"DURATION_MINUTES\", \"MVMN_CD_1\"])\n",
    "        \n",
    "        # êµí†µìˆ˜ë‹¨ ì›í•« ì¸ì½”ë”©\n",
    "        edges_df[\"MVMN_TYPE\"] = edges_df[\"MVMN_CD_1\"].apply(\n",
    "            lambda code: \"drive\" if code in [1,2,3] else \"public\" if code in [4,5,6,7,8,9,10,11,12,13,50] else \"other\"\n",
    "        )\n",
    "        edges_df[\"is_drive\"] = (edges_df[\"MVMN_TYPE\"] == \"drive\").astype(int)\n",
    "        edges_df[\"is_public\"] = (edges_df[\"MVMN_TYPE\"] == \"public\").astype(int)\n",
    "        edges_df[\"is_other\"] = (edges_df[\"MVMN_TYPE\"] == \"other\").astype(int)\n",
    "        \n",
    "        edge_index = torch.tensor(edges_df[[\"FROM_ID\", \"TO_ID\"]].values.T, dtype=torch.long)\n",
    "        edge_attr = torch.tensor(np.column_stack([\n",
    "            edges_df[[\"DURATION_MINUTES\"]].fillna(0).values,\n",
    "            edges_df[[\"is_drive\", \"is_public\", \"is_other\"]].values,\n",
    "            np.array(edge_weights).reshape(-1, 1)\n",
    "        ]), dtype=torch.float32)\n",
    "        \n",
    "        return edge_index, edge_attr\n",
    "\n",
    "class SmartRecommendationEngine:\n",
    "    def __init__(self, model, visit_area_df, device):\n",
    "        self.model = model\n",
    "        self.visit_area_df = visit_area_df\n",
    "        self.device = device\n",
    "        self.user_feedback_history = []\n",
    "        self.preference_weights = None\n",
    "        \n",
    "    def get_recommendations(self, data, travel_context, top_k=10, diversity_weight=0.3, \n",
    "                          excluded_ids=None):\n",
    "        \"\"\"\n",
    "        ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•œ ì¶”ì²œ (ì œì™¸í•  ID ëª©ë¡ ì§€ì›)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings, preference_scores = self.model(data, travel_context)\n",
    "            \n",
    "        scores = preference_scores.squeeze()\n",
    "        \n",
    "        # ì œì™¸í•  IDë“¤ì„ ë‚®ì€ ì ìˆ˜ë¡œ ì„¤ì •\n",
    "        if excluded_ids:\n",
    "            for exclude_id in excluded_ids:\n",
    "                # visit_area_dfì—ì„œ í•´ë‹¹ IDì˜ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "                matching_indices = self.visit_area_df[\n",
    "                    self.visit_area_df['NEW_VISIT_AREA_ID'] == exclude_id\n",
    "                ].index.tolist()\n",
    "                \n",
    "                for idx in matching_indices:\n",
    "                    if idx < len(scores):\n",
    "                        scores[idx] = -1.0  # ë§¤ìš° ë‚®ì€ ì ìˆ˜ë¡œ ì„¤ì •\n",
    "        \n",
    "        # í”¼ë“œë°± ê¸°ë°˜ ì ìˆ˜ ì¡°ì •\n",
    "        if self.preference_weights is not None:\n",
    "            scores = self._apply_preference_weights(scores, embeddings)\n",
    "        \n",
    "        # MMR ê¸°ë°˜ ì¶”ì²œ\n",
    "        recommendations = []\n",
    "        remaining_indices = list(range(len(scores)))\n",
    "        \n",
    "        # ì œì™¸ëœ ì¸ë±ìŠ¤ë“¤ì„ remaining_indicesì—ì„œ ì œê±°\n",
    "        if excluded_ids:\n",
    "            for exclude_id in excluded_ids:\n",
    "                matching_indices = self.visit_area_df[\n",
    "                    self.visit_area_df['NEW_VISIT_AREA_ID'] == exclude_id\n",
    "                ].index.tolist()\n",
    "                for idx in matching_indices:\n",
    "                    if idx in remaining_indices:\n",
    "                        remaining_indices.remove(idx)\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ ì¶”ì²œ\n",
    "        if remaining_indices:\n",
    "            valid_scores = [(i, scores[i].item()) for i in remaining_indices]\n",
    "            best_idx = max(valid_scores, key=lambda x: x[1])[0]\n",
    "            recommendations.append(best_idx)\n",
    "            remaining_indices.remove(best_idx)\n",
    "        \n",
    "        # ë‚˜ë¨¸ì§€ ì¶”ì²œ\n",
    "        for _ in range(min(top_k - 1, len(remaining_indices))):\n",
    "            if not remaining_indices:\n",
    "                break\n",
    "                \n",
    "            best_score = -float('inf')\n",
    "            best_idx = None\n",
    "            \n",
    "            for idx in remaining_indices:\n",
    "                relevance = scores[idx].item()\n",
    "                \n",
    "                if relevance < 0:  # ì œì™¸ëœ í•­ëª© ìŠ¤í‚µ\n",
    "                    continue\n",
    "                \n",
    "                # ë‹¤ì–‘ì„± ê³„ì‚°\n",
    "                similarities = []\n",
    "                for rec_idx in recommendations:\n",
    "                    sim = F.cosine_similarity(\n",
    "                        embeddings[idx:idx+1], \n",
    "                        embeddings[rec_idx:rec_idx+1]\n",
    "                    ).item()\n",
    "                    similarities.append(sim)\n",
    "                \n",
    "                diversity = 1 - max(similarities) if similarities else 1\n",
    "                final_score = (1 - diversity_weight) * relevance + diversity_weight * diversity\n",
    "                \n",
    "                if final_score > best_score:\n",
    "                    best_score = final_score\n",
    "                    best_idx = idx\n",
    "            \n",
    "            if best_idx is not None:\n",
    "                recommendations.append(best_idx)\n",
    "                remaining_indices.remove(best_idx)\n",
    "        \n",
    "        return recommendations, embeddings, preference_scores\n",
    "    \n",
    "    def _apply_preference_weights(self, scores, embeddings):\n",
    "        \"\"\"í”¼ë“œë°± ê¸°ë°˜ ì ìˆ˜ ì¡°ì •\"\"\"\n",
    "        if not self.preference_weights:\n",
    "            return scores\n",
    "            \n",
    "        # ì„ í˜¸ íƒ€ì…ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ì ìš©\n",
    "        adjusted_scores = scores.clone()\n",
    "        \n",
    "        for i, row in self.visit_area_df.iterrows():\n",
    "            if i >= len(adjusted_scores):\n",
    "                break\n",
    "                \n",
    "            area_type = row.get('VISIT_AREA_TYPE_CD', 0)\n",
    "            \n",
    "            # ì„ í˜¸ íƒ€ì…ì´ë©´ ì ìˆ˜ ì¦ê°€\n",
    "            if area_type in self.preference_weights.get('preferred_types', []):\n",
    "                adjusted_scores[i] *= 1.2\n",
    "            \n",
    "            # ë¹„ì„ í˜¸ íƒ€ì…ì´ë©´ ì ìˆ˜ ê°ì†Œ\n",
    "            if area_type in self.preference_weights.get('avoided_types', []):\n",
    "                adjusted_scores[i] *= 0.8\n",
    "        \n",
    "        return adjusted_scores\n",
    "    \n",
    "    def update_with_feedback(self, liked_items, disliked_items, embeddings):\n",
    "        \"\"\"ì‚¬ìš©ì í”¼ë“œë°± ì—…ë°ì´íŠ¸\"\"\"\n",
    "        feedback = {\n",
    "            'liked': liked_items,\n",
    "            'disliked': disliked_items,\n",
    "            'embeddings': embeddings.cpu().numpy()\n",
    "        }\n",
    "        self.user_feedback_history.append(feedback)\n",
    "        \n",
    "        # ì„ í˜¸ë„ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
    "        self.preference_weights = self._calculate_preference_weights()\n",
    "        \n",
    "        print(f\"âœ… í”¼ë“œë°± ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì¢‹ì•„ìš” {len(liked_items)}ê°œ, ì‹«ì–´ìš” {len(disliked_items)}ê°œ\")\n",
    "        \n",
    "        return self.preference_weights\n",
    "    \n",
    "    def _calculate_preference_weights(self):\n",
    "        \"\"\"í”¼ë“œë°± íˆìŠ¤í† ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„ í˜¸ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°\"\"\"\n",
    "        if not self.user_feedback_history:\n",
    "            return None\n",
    "            \n",
    "        liked_features = []\n",
    "        disliked_features = []\n",
    "        \n",
    "        for feedback in self.user_feedback_history:\n",
    "            for item_idx in feedback['liked']:\n",
    "                if item_idx < len(self.visit_area_df):\n",
    "                    liked_features.append(self.visit_area_df.iloc[item_idx])\n",
    "            \n",
    "            for item_idx in feedback['disliked']:\n",
    "                if item_idx < len(self.visit_area_df):\n",
    "                    disliked_features.append(self.visit_area_df.iloc[item_idx])\n",
    "        \n",
    "        preferred_types = [item.get('VISIT_AREA_TYPE_CD', 0) for item in liked_features]\n",
    "        avoided_types = [item.get('VISIT_AREA_TYPE_CD', 0) for item in disliked_features]\n",
    "        \n",
    "        return {\n",
    "            'preferred_types': list(set(preferred_types)),\n",
    "            'avoided_types': list(set(avoided_types)),\n",
    "            'preferred_regions': [(item.get('X_COORD', 0), item.get('Y_COORD', 0)) for item in liked_features]\n",
    "        }\n",
    "\n",
    "class OptimizedRouteGenerator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def generate_daily_routes(self, recommendations, visit_area_df, travel_duration, \n",
    "                            optimization_method='kmeans_tsp'):\n",
    "        \"\"\"ì¼ë³„ ìµœì  ê²½ë¡œ ìƒì„±\"\"\"\n",
    "        if travel_duration <= 0:\n",
    "            travel_duration = 1\n",
    "            \n",
    "        coords = []\n",
    "        locations = []\n",
    "        \n",
    "        for idx in recommendations:\n",
    "            if idx < len(visit_area_df):\n",
    "                row = visit_area_df.iloc[idx]\n",
    "                coords.append([row['X_COORD'], row['Y_COORD']])\n",
    "                locations.append({\n",
    "                    'id': row['NEW_VISIT_AREA_ID'],\n",
    "                    'name': row['VISIT_AREA_NM'],\n",
    "                    'coords': [row['X_COORD'], row['Y_COORD']],\n",
    "                    'idx': idx,\n",
    "                    'type': row.get('VISIT_AREA_TYPE_CD', 0)\n",
    "                })\n",
    "        \n",
    "        if len(coords) == 0:\n",
    "            return {}\n",
    "            \n",
    "        coords = np.array(coords)\n",
    "        # NaN ì²˜ë¦¬ (ì˜ˆ: 0ìœ¼ë¡œ ëŒ€ì²´)\n",
    "        coords = np.nan_to_num(coords, nan=0.0)\n",
    "\n",
    "        \n",
    "        # K-means í´ëŸ¬ìŠ¤í„°ë§\n",
    "        n_clusters = min(travel_duration, len(locations))\n",
    "        if n_clusters > 1:\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            day_labels = kmeans.fit_predict(coords)\n",
    "        else:\n",
    "            day_labels = np.zeros(len(locations))\n",
    "        \n",
    "        # í´ëŸ¬ìŠ¤í„° í›„ ë‹¨ì¼ Day ì œê±°\n",
    "        daily_groups = {}\n",
    "        for i, loc in enumerate(locations):\n",
    "            day = int(day_labels[i])\n",
    "            if day not in daily_groups:\n",
    "                daily_groups[day] = []\n",
    "            daily_groups[day].append(loc)\n",
    "\n",
    "        # Day ë‹¨ì¼ í´ëŸ¬ìŠ¤í„° ì œê±°: ìµœì†Œ 2ê°œ ì´ìƒìœ¼ë¡œ ìœ ì§€\n",
    "        for day, locs in list(daily_groups.items()):\n",
    "            if len(locs) == 1:\n",
    "                # Day1ë¡œ ì¬ë°°ì •\n",
    "                if 0 in daily_groups:\n",
    "                    daily_groups[0].extend(locs)\n",
    "                else:\n",
    "                    # Day1ì´ ì—†ìœ¼ë©´ Day2ë¡œ í•©ì¹¨\n",
    "                    target_day = next((d for d in daily_groups if d != day), 0)\n",
    "                    daily_groups[target_day].extend(locs)\n",
    "                del daily_groups[day]\n",
    "        \n",
    "        # TSP ìµœì í™”\n",
    "        optimized_routes = {}\n",
    "        for day, locations_day in daily_groups.items():\n",
    "            if len(locations_day) > 1:\n",
    "                optimized_order = self._solve_tsp_simple(locations_day)\n",
    "                optimized_routes[day] = optimized_order\n",
    "            else:\n",
    "                optimized_routes[day] = locations_day\n",
    "        \n",
    "        return optimized_routes\n",
    "    \n",
    "    def _solve_tsp_simple(self, locations):\n",
    "        \"\"\"ê°„ë‹¨í•œ TSP í•´ë²•\"\"\"\n",
    "        if len(locations) <= 2:\n",
    "            return locations\n",
    "            \n",
    "        coords = np.array([loc['coords'] for loc in locations])\n",
    "        n = len(coords)\n",
    "        \n",
    "        # ê±°ë¦¬ í–‰ë ¬\n",
    "        dist_matrix = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                dist_matrix[i][j] = np.linalg.norm(coords[i] - coords[j])\n",
    "        \n",
    "        # Nearest Neighbor\n",
    "        unvisited = set(range(1, n))\n",
    "        current = 0\n",
    "        route = [0]\n",
    "        \n",
    "        while unvisited:\n",
    "            nearest = min(unvisited, key=lambda x: dist_matrix[current][x])\n",
    "            route.append(nearest)\n",
    "            unvisited.remove(nearest)\n",
    "            current = nearest\n",
    "        \n",
    "        return [locations[i] for i in route]\n",
    "\n",
    "def process_travel_input(travel_info: dict):\n",
    "    \"\"\"ì—¬í–‰ ì •ë³´ ì „ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
    "    travel_feature_cols = [\n",
    "        'TOTAL_COST_BINNED_ENCODED', 'WITH_PET', 'MONTH', 'DURATION',\n",
    "        'MVMN_ê¸°íƒ€', 'MVMN_ëŒ€ì¤‘êµí†µ', 'MVMN_ìê°€ìš©',\n",
    "        'TRAVEL_PURPOSE_1', 'TRAVEL_PURPOSE_2', 'TRAVEL_PURPOSE_3',\n",
    "        'TRAVEL_PURPOSE_4', 'TRAVEL_PURPOSE_5', 'TRAVEL_PURPOSE_6',\n",
    "        'TRAVEL_PURPOSE_7', 'TRAVEL_PURPOSE_8', 'TRAVEL_PURPOSE_9',\n",
    "        'WHOWITH_2ì¸ì—¬í–‰', 'WHOWITH_ê°€ì¡±ì—¬í–‰', 'WHOWITH_ê¸°íƒ€',\n",
    "        'WHOWITH_ë‹¨ë…ì—¬í–‰', 'WHOWITH_ì¹œêµ¬/ì§€ì¸ ì—¬í–‰'\n",
    "    ]\n",
    "    \n",
    "    # ë°˜ë ¤ë™ë¬¼ ë™ë°˜\n",
    "    travel_info['mission_ENC'] = travel_info['mission_ENC'].strip().split(',')\n",
    "    travel_info['WITH_PET'] = 1 if '0' in travel_info['mission_ENC'] else 0\n",
    "        \n",
    "    # ì—¬í–‰ ëª©ì \n",
    "    for i in range(1, 10):\n",
    "        travel_info[f'TRAVEL_PURPOSE_{i}'] = 1 if str(i) in travel_info['mission_ENC'] else 0\n",
    "        \n",
    "    # ë‚ ì§œ ì²˜ë¦¬\n",
    "    dates = travel_info['date_range'].split(' - ')\n",
    "    start_date = datetime.strptime(dates[0].strip(), \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(dates[1].strip(), \"%Y-%m-%d\")\n",
    "    \n",
    "    travel_info['MONTH'] = end_date.month\n",
    "    travel_info['DURATION'] = (end_date - start_date).days\n",
    "    \n",
    "    # êµí†µìˆ˜ë‹¨\n",
    "    for m in ['ìê°€ìš©', 'ëŒ€ì¤‘êµí†µ', 'ê¸°íƒ€']:\n",
    "        travel_info[f\"MVMN_{m}\"] = 0\n",
    "    \n",
    "    if travel_info['MVMN_NM_ENC'] == '1':\n",
    "        travel_info['MVMN_ìê°€ìš©'] = 1\n",
    "    elif travel_info['MVMN_NM_ENC'] == '2':\n",
    "        travel_info['MVMN_ëŒ€ì¤‘êµí†µ'] = 1\n",
    "    else:\n",
    "        travel_info['MVMN_ê¸°íƒ€'] = 1\n",
    "    \n",
    "    # ë™í–‰ì\n",
    "    whowith_onehot = [0] * 5\n",
    "    idx = int(travel_info['whowith_ENC']) - 1\n",
    "    if 0 <= idx < 5:\n",
    "        whowith_onehot[idx] = 1\n",
    "    \n",
    "    travel_info.update({\n",
    "        'WHOWITH_ë‹¨ë…ì—¬í–‰': whowith_onehot[0],\n",
    "        'WHOWITH_2ì¸ì—¬í–‰': whowith_onehot[1],\n",
    "        'WHOWITH_ê°€ì¡±ì—¬í–‰': whowith_onehot[2],\n",
    "        'WHOWITH_ì¹œêµ¬/ì§€ì¸ ì—¬í–‰': whowith_onehot[3],\n",
    "        'WHOWITH_ê¸°íƒ€': whowith_onehot[4],\n",
    "    })\n",
    "    \n",
    "    # ë¹„ìš©\n",
    "    travel_info['TOTAL_COST_BINNED_ENCODED'] = int(travel_info['TOTAL_COST'])\n",
    "    \n",
    "    # ìµœì¢… ë²¡í„° ìƒì„±\n",
    "    travel_vector = [int(travel_info.get(k, 0)) for k in travel_feature_cols]\n",
    "    \n",
    "    return np.array([travel_vector]).astype(np.float32)\n",
    "\n",
    "def simulate_user_feedback():\n",
    "    \"\"\"ì‚¬ìš©ì í”¼ë“œë°± ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
    "    feedback_options = [\n",
    "        {\"liked\": [], \"disliked\": [0, 2]},  # ì²« ë²ˆì§¸ì™€ ì„¸ ë²ˆì§¸ ì¥ì†Œ ì‹«ì–´ìš”\n",
    "        {\"liked\": [1], \"disliked\": [4, 7]},  # ë‘ ë²ˆì§¸ ì¥ì†Œ ì¢‹ì•„ìš”, ë‹¤ë¥¸ ê³³ë“¤ ì‹«ì–´ìš”\n",
    "        {\"liked\": [0, 3], \"disliked\": [5]},  # ë³µìˆ˜ ì¢‹ì•„ìš”/ì‹«ì–´ìš”\n",
    "    ]\n",
    "    \n",
    "    return random.choice(feedback_options)\n",
    "\n",
    "def main_feedback_test():\n",
    "    print(\"ğŸš€ í”¼ë“œë°± ê¸°ë°˜ ê²½ë¡œ ëŒ€ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ê¸°ì¡´ ë°ì´í„° ë¡œë”©\n",
    "    move_path = \"../data/VL_csv/move_with_new_id_final.csv\"\n",
    "    travel_path = \"tn_travel_processed.csv\"\n",
    "    visit_area_path = \"../data/VL_csv/visit_area_with_new_id_final.csv\"\n",
    "\n",
    "    move_df = pd.read_csv(move_path)\n",
    "    travel_df = pd.read_csv(travel_path)\n",
    "    visit_area_df = pd.read_csv(visit_area_path)\n",
    "\n",
    "    processor = EnhancedDataProcessor()\n",
    "    visit_area_tensor = processor.process_visit_area_features(visit_area_df)\n",
    "    edge_index, edge_attr = processor.create_enhanced_edges(move_df, visit_area_df)\n",
    "\n",
    "    data = HeteroData()\n",
    "    data['visit_area'].x = torch.tensor(visit_area_tensor, dtype=torch.float32)\n",
    "    data['visit_area', 'moved_to', 'visit_area'].edge_index = edge_index\n",
    "    data['visit_area', 'moved_to', 'visit_area'].edge_attr = edge_attr\n",
    "\n",
    "    # ì—¬í–‰ ì •ë³´\n",
    "    travel_example = {\n",
    "        'mission_ENC': '0,1,2',\n",
    "        'date_range': '2025-09-28 - 2025-09-30',  # 3ì¼ ì—¬í–‰\n",
    "        'start_date': '',\n",
    "        'end_date': '',\n",
    "        'TOTAL_COST': '2',\n",
    "        'MVMN_NM_ENC': '2',\n",
    "        'whowith_ENC': '2',\n",
    "        'mission_type': 'normal'\n",
    "    }\n",
    "    travel_tensor = process_travel_input(travel_example)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "    model = ImprovedTravelGNN(\n",
    "        in_channels=visit_area_tensor.shape[1],\n",
    "        hidden_channels=128,\n",
    "        out_channels=64,\n",
    "        travel_context_dim=travel_tensor.shape[1],\n",
    "        num_heads=4,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    data = data.to(device)\n",
    "    travel_context_tensor = torch.tensor(travel_tensor, dtype=torch.float32).to(device)\n",
    "    recommender = SmartRecommendationEngine(model, visit_area_df, device)\n",
    "\n",
    "    # ì´ˆê¸° ì¶”ì²œ (ì¤‘ë³µ ì œê±°)\n",
    "    recommendations, embeddings, _ = recommender.get_recommendations(\n",
    "        data, travel_context_tensor, top_k=20, diversity_weight=0.3\n",
    "    )\n",
    "\n",
    "    # ì¤‘ë³µ ë°©ë¬¸ì§€ ì œê±°\n",
    "    unique_recommendations, seen_ids = [], set()\n",
    "    for idx in recommendations:\n",
    "        area_id = visit_area_df.iloc[idx]['NEW_VISIT_AREA_ID']\n",
    "        if area_id not in seen_ids and area_id != 0:\n",
    "            unique_recommendations.append(idx)\n",
    "            seen_ids.add(area_id)\n",
    "        if len(unique_recommendations) == 10:\n",
    "            break\n",
    "\n",
    "    # ìµœì í™” ê²½ë¡œ ìƒì„±\n",
    "    route_generator = OptimizedRouteGenerator()\n",
    "    travel_duration = int(travel_tensor[0, 3])\n",
    "    optimized_routes = route_generator.generate_daily_routes(\n",
    "        unique_recommendations, visit_area_df, travel_duration\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ—“ï¸ ì´ˆê¸° ì—¬í–‰ ì¼ì • (ìµœì í™”):\")\n",
    "    for day, route in sorted(optimized_routes.items()):\n",
    "        print(f\"\\nğŸ“… Day {day + 1}:\")\n",
    "        for loc in route:\n",
    "            print(f\" - [{loc['id']:3d}] {loc['name']}\")\n",
    "\n",
    "    # í”¼ë“œë°± ë¼ìš´ë“œ ë°˜ë³µ\n",
    "    for round_num in range(3):\n",
    "        print(f\"\\nğŸ”„ í”¼ë“œë°± ë¼ìš´ë“œ {round_num + 1}\")\n",
    "        feedback = simulate_user_feedback()\n",
    "        liked_indices = [recommendations[i] for i in feedback[\"liked\"] if i < len(recommendations)]\n",
    "        disliked_indices = [recommendations[i] for i in feedback[\"disliked\"] if i < len(recommendations)]\n",
    "\n",
    "        recommender.update_with_feedback(liked_indices, disliked_indices, embeddings)\n",
    "\n",
    "        # ì œì™¸ëœ í•­ëª© ë°˜ì˜\n",
    "        excluded_ids = {visit_area_df.iloc[idx]['NEW_VISIT_AREA_ID'] for idx in disliked_indices}\n",
    "        recommendations, embeddings, _ = recommender.get_recommendations(\n",
    "            data, travel_context_tensor, top_k=20, diversity_weight=0.3, excluded_ids=excluded_ids\n",
    "        )\n",
    "\n",
    "        unique_recommendations, seen_ids = [], set()\n",
    "        for idx in recommendations:\n",
    "            area_id = visit_area_df.iloc[idx]['NEW_VISIT_AREA_ID']\n",
    "            if area_id not in seen_ids and area_id not in excluded_ids and area_id != 0:\n",
    "                unique_recommendations.append(idx)\n",
    "                seen_ids.add(area_id)\n",
    "            if len(unique_recommendations) == 10:\n",
    "                break\n",
    "\n",
    "        optimized_routes = route_generator.generate_daily_routes(\n",
    "            unique_recommendations, visit_area_df, travel_duration\n",
    "        )\n",
    "\n",
    "        print(\"\\nğŸ¯ í”¼ë“œë°± ë°˜ì˜ í›„ ìµœì í™”ëœ ì—¬í–‰ ì¼ì •:\")\n",
    "        for day, route in sorted(optimized_routes.items()):\n",
    "            if len(route) < 2 and len(optimized_routes) > 1:\n",
    "                continue  # ë‹¨ì¼ ì¥ì†Œ Day ì œê±°\n",
    "            print(f\"\\nğŸ“… Day {day + 1}:\")\n",
    "            for loc in route:\n",
    "                print(f\" - [{loc['id']:3d}] {loc['name']}\")\n",
    "\n",
    "    print(\"\\nâœ… ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ê²½ë¡œ ëŒ€ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_feedback_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "âš™ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...\n",
      "âœ… ì €ì¥ ì™„ë£Œ!\n",
      "- travel_recommendation_model.pt: ëª¨ë¸ íŒŒë¼ë¯¸í„°\n",
      "- travel_data.pkl: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë° ìŠ¤ì¼€ì¼ëŸ¬\n"
     ]
    }
   ],
   "source": [
    "# save_model_and_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "def save_model_and_data():\n",
    "    \"\"\"ëª¨ë¸ê³¼ í•„ìš”í•œ ë°ì´í„°ë“¤ì„ ì €ì¥\"\"\"\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë”©\n",
    "    print(\"ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "    move_path = \"../data/VL_csv/move_with_new_id_final.csv\"\n",
    "    travel_path = \"tn_travel_processed.csv\"\n",
    "    visit_area_path = \"../data/VL_csv/visit_area_with_new_id_final.csv\"\n",
    "    \n",
    "    move_df = pd.read_csv(move_path)\n",
    "    travel_df = pd.read_csv(travel_path)\n",
    "    visit_area_df = pd.read_csv(visit_area_path)\n",
    "    \n",
    "    # 2. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    print(\"âš™ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "    processor = EnhancedDataProcessor()\n",
    "    visit_area_tensor = processor.process_visit_area_features(visit_area_df)\n",
    "    edge_index, edge_attr = processor.create_enhanced_edges(move_df, visit_area_df)\n",
    "    \n",
    "    # 3. ê·¸ë˜í”„ ë°ì´í„° ìƒì„±\n",
    "    data = HeteroData()\n",
    "    data['visit_area'].x = torch.tensor(visit_area_tensor, dtype=torch.float32)\n",
    "    data['visit_area', 'moved_to', 'visit_area'].edge_index = edge_index\n",
    "    data['visit_area', 'moved_to', 'visit_area'].edge_attr = edge_attr\n",
    "    \n",
    "    # 4. ëª¨ë¸ ì´ˆê¸°í™” (ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ImprovedTravelGNN(\n",
    "        in_channels=visit_area_tensor.shape[1],\n",
    "        hidden_channels=128,\n",
    "        out_channels=64,\n",
    "        travel_context_dim=25,  # travel feature ê°œìˆ˜\n",
    "        num_heads=4,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    # 5. ì €ì¥í•  ë°ì´í„° ì¤€ë¹„\n",
    "    save_data = {\n",
    "        'visit_area_df': visit_area_df,\n",
    "        'graph_data': data,\n",
    "        'visit_scaler': processor.visit_scaler,\n",
    "        'travel_scaler': processor.travel_scaler,\n",
    "        'device': str(device)\n",
    "    }\n",
    "    \n",
    "    # 6. íŒŒì¼ ì €ì¥\n",
    "    print(\"ğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...\")\n",
    "    \n",
    "    # ëª¨ë¸ ì €ì¥ (.pt)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'in_channels': visit_area_tensor.shape[1],\n",
    "            'hidden_channels': 128,\n",
    "            'out_channels': 64,\n",
    "            'travel_context_dim': 25,\n",
    "            'num_heads': 4,\n",
    "            'dropout': 0.2\n",
    "        }\n",
    "    }, 'travel_recommendation_model.pt')\n",
    "    \n",
    "    # ë°ì´í„° ì €ì¥ (.pkl)\n",
    "    with open('travel_data.pkl', 'wb') as f:\n",
    "        pickle.dump(save_data, f)\n",
    "    \n",
    "    print(\"âœ… ì €ì¥ ì™„ë£Œ!\")\n",
    "    print(\"- travel_recommendation_model.pt: ëª¨ë¸ íŒŒë¼ë¯¸í„°\")\n",
    "    print(\"- travel_data.pkl: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë° ìŠ¤ì¼€ì¼ëŸ¬\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_model_and_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
