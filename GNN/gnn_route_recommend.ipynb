{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï£ºÏöî ÌååÏùº Í≤ΩÎ°ú\n",
    "move_path = \"../data/VL_csv/tn_move_his_·Ñã·Öµ·ÑÉ·Ö©·Üº·ÑÇ·Ö¢·Ñã·Öß·Ü®_Cleaned_E.csv\"\n",
    "user_path = \"../data/VL_csv/tn_traveller_master_·Ñã·Öß·Ñí·Ö¢·Üº·ÑÄ·Ö¢·Ü® Master_E_preprocessed.csv\"\n",
    "travel_path = \"tn_travel_processed.csv\"\n",
    "visit_area_path = \"../data/VL_csv/tn_visit_area_info_·Ñá·Ö°·Üº·ÑÜ·ÖÆ·Ü´·Ñå·Öµ·Ñå·Ö•·Üº·Ñá·Ö©_Cleaned_E.csv\"\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎî©\n",
    "move_df = pd.read_csv(move_path)\n",
    "user_df = pd.read_csv(user_path)\n",
    "travel_df = pd.read_csv(travel_path)\n",
    "visit_area_df = pd.read_csv(visit_area_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HeteroData created successfully!\n",
      "HeteroData(\n",
      "  visit_area={ x=[21384, 34] },\n",
      "  (visit_area, moved_to, visit_area)={\n",
      "    edge_index=[2, 18742],\n",
      "    edge_attr=[18742, 4],\n",
      "  }\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2560/2560 [01:18<00:00, 32.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Loss: 0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2560/2560 [01:24<00:00, 30.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/10] Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2560/2560 [01:34<00:00, 27.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/10] Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2560/2560 [01:30<00:00, 28.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/10] Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2560/2560 [01:16<00:00, 33.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/10] Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2560/2560 [01:15<00:00, 33.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/10] Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2560/2560 [01:15<00:00, 33.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/10] Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2560/2560 [01:21<00:00, 31.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/10] Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2560/2560 [01:22<00:00, 31.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/10] Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2560/2560 [01:21<00:00, 31.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/10] Loss: 0.0004\n",
      "üéâ ÌïôÏäµ ÏôÑÎ£å!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch import nn, optim\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visit_area feature Ï†ÑÏ≤òÎ¶¨\n",
    "visit_area_df['X_COORD'] = visit_area_df['X_COORD'].fillna(visit_area_df['X_COORD'].mean())\n",
    "visit_area_df['Y_COORD'] = visit_area_df['Y_COORD'].fillna(visit_area_df['Y_COORD'].mean())\n",
    "visit_area_df['VISIT_CHC_REASON_CD'] = visit_area_df['VISIT_CHC_REASON_CD'].fillna(0)\n",
    "for col in ['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']:\n",
    "    visit_area_df[col] = visit_area_df[col].fillna(3)\n",
    "\n",
    "features = visit_area_df[['X_COORD', 'Y_COORD']].copy()\n",
    "type_onehot = pd.get_dummies(visit_area_df['VISIT_AREA_TYPE_CD'], prefix='type')\n",
    "reason_onehot = pd.get_dummies(visit_area_df['VISIT_CHC_REASON_CD'], prefix='reason')\n",
    "visit_area_df['DGSTFN_norm'] = (visit_area_df['DGSTFN'] - 1) / 4.0\n",
    "visit_area_df['REVISIT_norm'] = (visit_area_df['REVISIT_INTENTION'] - 1) / 4.0\n",
    "visit_area_df['RCMDTN_norm'] = (visit_area_df['RCMDTN_INTENTION'] - 1) / 4.0\n",
    "features = pd.concat([features, type_onehot, reason_onehot,\n",
    "                      visit_area_df[['DGSTFN_norm', 'REVISIT_norm', 'RCMDTN_norm']]], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "visit_area_tensor = scaler.fit_transform(features.to_numpy(dtype=np.float32))\n",
    "\n",
    "# travel_context (batch)\n",
    "excluded_cols = ['Unnamed: 0', 'TRAVEL_ID', 'TRAVELER_ID']\n",
    "travel_feature_cols = [col for col in travel_df.columns if col not in excluded_cols]\n",
    "travel_tensor = travel_df[travel_feature_cols].fillna(0).astype(np.float32).to_numpy()  # (2560, 21)\n",
    "travel_context_tensor = torch.tensor(travel_tensor, dtype=torch.float32)\n",
    "\n",
    "# edge_index ÏÉùÏÑ±\n",
    "move_df[\"START_DT_MIN\"] = pd.to_datetime(move_df[\"START_DT_MIN\"], errors=\"coerce\")\n",
    "move_df[\"END_DT_MIN\"] = pd.to_datetime(move_df[\"END_DT_MIN\"], errors=\"coerce\")\n",
    "cols = [\"TRAVEL_ID\", \"TRIP_ID\", \"START_VISIT_AREA_ID\", \"END_VISIT_AREA_ID\",\n",
    "        \"START_DT_MIN\", \"END_DT_MIN\", \"MVMN_CD_1\"]\n",
    "move_df = move_df[cols].copy()\n",
    "\n",
    "edges = []\n",
    "for travel_id, group in move_df.sort_values([\"TRAVEL_ID\", \"TRIP_ID\"]).groupby(\"TRAVEL_ID\"):\n",
    "    group = group.reset_index(drop=True)\n",
    "    n = len(group)\n",
    "    if n < 2:\n",
    "        continue\n",
    "    start_time = group.loc[0, \"START_DT_MIN\"]\n",
    "    end_time = group.loc[1, \"END_DT_MIN\"]\n",
    "    if pd.notna(start_time) and pd.notna(end_time):\n",
    "        duration = (end_time - start_time).total_seconds() / 60\n",
    "        from_id = group.loc[0, \"START_VISIT_AREA_ID\"]\n",
    "        to_id = group.loc[1, \"END_VISIT_AREA_ID\"]\n",
    "        transport = group.loc[1, \"MVMN_CD_1\"]\n",
    "        if pd.notna(from_id) and pd.notna(to_id) and pd.notna(transport):\n",
    "            edges.append([int(from_id), int(to_id), duration, int(transport)])\n",
    "    for i in range(1, n - 1):\n",
    "        t1 = group.loc[i, \"END_DT_MIN\"]\n",
    "        t2 = group.loc[i + 1, \"END_DT_MIN\"]\n",
    "        if pd.notna(t1) and pd.notna(t2):\n",
    "            duration = (t2 - t1).total_seconds() / 60\n",
    "            from_id = group.loc[i, \"END_VISIT_AREA_ID\"]\n",
    "            to_id = group.loc[i + 1, \"END_VISIT_AREA_ID\"]\n",
    "            transport = group.loc[i + 1, \"MVMN_CD_1\"]\n",
    "            if pd.notna(from_id) and pd.notna(to_id) and pd.notna(transport):\n",
    "                edges.append([int(from_id), int(to_id), duration, int(transport)])\n",
    "\n",
    "edges_df = pd.DataFrame(edges, columns=[\n",
    "    \"FROM_VISIT_AREA_ID\", \"TO_VISIT_AREA_ID\", \"DURATION_MINUTES\", \"MVMN_CD_1\"\n",
    "])\n",
    "\n",
    "# Î∞©Î¨∏ÏßÄ ID ‚Üí index Îß§Ìïë\n",
    "visit_area_id_list = visit_area_df[\"VISIT_AREA_ID\"].tolist()\n",
    "visit_area_id_to_index = {vid: idx for idx, vid in enumerate(visit_area_id_list)}\n",
    "\n",
    "edges_df[\"FROM_IDX\"] = edges_df[\"FROM_VISIT_AREA_ID\"].map(visit_area_id_to_index)\n",
    "edges_df[\"TO_IDX\"] = edges_df[\"TO_VISIT_AREA_ID\"].map(visit_area_id_to_index)\n",
    "\n",
    "# edge_index / edge_attr\n",
    "edge_index = torch.tensor([\n",
    "    edges_df[\"FROM_IDX\"].tolist(),\n",
    "    edges_df[\"TO_IDX\"].tolist()\n",
    "], dtype=torch.long)\n",
    "edge_attr = torch.tensor(\n",
    "    edges_df[[\"DURATION_MINUTES\"]].fillna(0).astype(np.float32).to_numpy(), dtype=torch.float32\n",
    ")\n",
    "# Ïù¥ÎèôÏàòÎã® one-hot\n",
    "edges_df[\"MVMN_TYPE\"] = edges_df[\"MVMN_CD_1\"].apply(lambda code: \"drive\" if code in [1,2,3] else \"public\" if code in [4,5,6,7,8,9,10,11,12,13,50] else \"other\")\n",
    "edges_df[\"is_drive\"] = (edges_df[\"MVMN_TYPE\"] == \"drive\").astype(int)\n",
    "edges_df[\"is_public\"] = (edges_df[\"MVMN_TYPE\"] == \"public\").astype(int)\n",
    "edges_df[\"is_other\"] = (edges_df[\"MVMN_TYPE\"] == \"other\").astype(int)\n",
    "edge_attr = torch.cat([\n",
    "    edge_attr,\n",
    "    torch.tensor(edges_df[[\"is_drive\", \"is_public\", \"is_other\"]].to_numpy(), dtype=torch.float32)\n",
    "], dim=1)\n",
    "\n",
    "# 5Ô∏è‚É£ HeteroData\n",
    "data = HeteroData()\n",
    "data['visit_area'].x = torch.tensor(visit_area_tensor, dtype=torch.float32)\n",
    "data['visit_area', 'moved_to', 'visit_area'].edge_index = edge_index\n",
    "data['visit_area', 'moved_to', 'visit_area'].edge_attr = edge_attr\n",
    "\n",
    "print(\"‚úÖ HeteroData created successfully!\")\n",
    "print(data)\n",
    "\n",
    "# 6Ô∏èGNN Î™®Îç∏\n",
    "class TravelGNN(nn.Module):\n",
    "    def __init__(self, visit_area_input_dim=34, travel_context_dim=21, hidden_dim=64):\n",
    "        super(TravelGNN, self).__init__()\n",
    "        self.lin_in = nn.Linear(visit_area_input_dim + travel_context_dim, hidden_dim)\n",
    "        self.conv1 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.lin_out = nn.Linear(hidden_dim, visit_area_input_dim)  # ÏµúÏ¢Ö Ï∂úÎ†• 34Ï∞®ÏõêÏúºÎ°ú ÏàòÏ†ï!\n",
    "\n",
    "    def forward(self, data, travel_context):\n",
    "        x = data['visit_area'].x  # (N, 34)\n",
    "        context_expand = travel_context.repeat_interleave(\n",
    "            repeats=x.size(0) // travel_context.size(0), dim=0\n",
    "        )\n",
    "        x = torch.cat([x, context_expand], dim=1)\n",
    "        x = self.lin_in(x)\n",
    "\n",
    "        edge_index = data['visit_area', 'moved_to', 'visit_area'].edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        out = self.lin_out(x)  # (N, 34)\n",
    "        return out\n",
    "\n",
    "# 7Ô∏èÌïôÏäµ Î£®ÌîÑ\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TravelGNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "data = data.to(device)\n",
    "travel_context_tensor = travel_context_tensor.to(device)\n",
    "target = data['visit_area'].x.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i in tqdm(range(travel_context_tensor.size(0))):\n",
    "        travel_context = travel_context_tensor[i:i+1, :]  # (1, 21)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data, travel_context)\n",
    "        loss = criterion(out, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / travel_context_tensor.size(0)\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"üéâ ÌïôÏäµ ÏôÑÎ£å!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ïó¨Ìñâ Ï†ïÎ≥¥\n",
    "def process_travel_input(travel_info:dict):\n",
    "    from datetime import datetime\n",
    "    travel_feature_cols = [\n",
    "        'TOTAL_COST_BINNED_ENCODED',\n",
    "        'WITH_PET',\n",
    "        'MONTH',\n",
    "        'DURATION',\n",
    "        'MVMN_Í∏∞ÌÉÄ',\n",
    "        'MVMN_ÎåÄÏ§ëÍµêÌÜµ',\n",
    "        'MVMN_ÏûêÍ∞ÄÏö©',\n",
    "        'TRAVEL_PURPOSE_1',\n",
    "        'TRAVEL_PURPOSE_2',\n",
    "        'TRAVEL_PURPOSE_3',\n",
    "        'TRAVEL_PURPOSE_4',\n",
    "        'TRAVEL_PURPOSE_5',\n",
    "        'TRAVEL_PURPOSE_6',\n",
    "        'TRAVEL_PURPOSE_7',\n",
    "        'TRAVEL_PURPOSE_8',\n",
    "        'TRAVEL_PURPOSE_9',\n",
    "        'WHOWITH_2Ïù∏Ïó¨Ìñâ',\n",
    "        'WHOWITH_Í∞ÄÏ°±Ïó¨Ìñâ',\n",
    "        'WHOWITH_Í∏∞ÌÉÄ',\n",
    "        'WHOWITH_Îã®ÎèÖÏó¨Ìñâ',\n",
    "        'WHOWITH_ÏπúÍµ¨/ÏßÄÏù∏ Ïó¨Ìñâ']\n",
    "    \n",
    "    \n",
    "    # mission_ENCÏóê 0 = Î∞òÎ†§ÎèôÎ¨º ÎèôÎ∞ò (WITH_PET)\n",
    "    travel_info['mission_ENC'] = travel_info['mission_ENC'].strip().split(',')\n",
    "    if '0' in travel_info['mission_ENC']:\n",
    "        travel_info['WITH_PET'] = 1\n",
    "    else:\n",
    "        travel_info['WITH_PET'] = 0\n",
    "        \n",
    "    # TRAVEL_PURPOSE_1 ~~ TRAVEL_PURPOSE_9 (0ÏúºÎ°ú Îì§Ïñ¥Ïò® ÏûÖÎ†•ÏùÄ Ï†úÍ±∞Ìï¥Ï§òÏïºÎê®) \n",
    "    for i in range(1,10):\n",
    "        if str(i) in travel_info['mission_ENC']:\n",
    "            travel_info[f'TRAVEL_PURPOSE_{i}'] = 1\n",
    "        else:\n",
    "            travel_info[f'TRAVEL_PURPOSE_{i}'] = 0\n",
    "        \n",
    "    # MONTH\n",
    "    dates = travel_info['date_range'].split(' - ')\n",
    "    travel_info['start_date'] = datetime.strptime(dates[0].strip(), \"%Y-%m-%d\")\n",
    "    travel_info['end_date'] = datetime.strptime(dates[1].strip(), \"%Y-%m-%d\")\n",
    "    \n",
    "    travel_info['MONTH'] = travel_info['end_date'].month\n",
    "    \n",
    "    # DURATION\n",
    "    travel_info['DURATION'] = (travel_info['end_date'] - travel_info['start_date']).days\n",
    "    \n",
    "    # MNVM_Í∏∞ÌÉÄ, MVMN_ÎåÄÏ§ëÍµêÌÜµ, MVMN_ÏûêÍ∞ÄÏö©\n",
    "    for m in ['ÏûêÍ∞ÄÏö©', 'ÎåÄÏ§ëÍµêÌÜµ', 'Í∏∞ÌÉÄ']:\n",
    "        travel_info[f\"MVMN_{m}\"] = False\n",
    "    \n",
    "    if travel_info['MVMN_NM_ENC'] == '1':\n",
    "        travel_info['MVMN_ÏûêÍ∞ÄÏö©'] = True\n",
    "    elif travel_info['MVMN_NM_ENC'] == '2':\n",
    "        travel_info['MVMN_ÎåÄÏ§ëÍµêÌÜµ'] = True\n",
    "    else:\n",
    "        travel_info['MVMN_Í∏∞ÌÉÄ'] = True\n",
    "    \n",
    "    # WHOWITHÎäî 1Î∂ÄÌÑ∞ 5ÍπåÏßÄ Ïà´ÏûêÎ°ú Îì§Ïñ¥Ïò¥ -> ÏõêÌï´ Ïù∏ÏΩîÎî©ÏúºÎ°ú ÏàòÏ†ïÌï† Í≤É\n",
    "    # dictÏóê Îì§Ïñ¥Ïò§Îäî Ïà´Ïûê ÏùòÎØ∏: WHOWITH_Îã®ÎèÖÏó¨Ìñâ, WHOWITH_2Ïù∏Ïó¨Ìñâ, WHOWITH_Í∞ÄÏ°±Ïó¨Ìñâ, WHOWITH_ÏπúÍµ¨/ÏßÄÏù∏Ïó¨Ìñâ, WHOWITH_Í∏∞ÌÉÄ\n",
    "    whowith_onehot = [0] * 5\n",
    "    idx = int(travel_info['whowith_ENC']) - 1\n",
    "    if 0 <= idx < 5:\n",
    "        whowith_onehot[idx] = 1\n",
    "    \n",
    "    travel_info.update({\n",
    "    'WHOWITH_Îã®ÎèÖÏó¨Ìñâ': whowith_onehot[0],\n",
    "    'WHOWITH_2Ïù∏Ïó¨Ìñâ': whowith_onehot[1],\n",
    "    'WHOWITH_Í∞ÄÏ°±Ïó¨Ìñâ': whowith_onehot[2],\n",
    "    'WHOWITH_ÏπúÍµ¨/ÏßÄÏù∏ Ïó¨Ìñâ': whowith_onehot[3],\n",
    "    'WHOWITH_Í∏∞ÌÉÄ': whowith_onehot[4],\n",
    "    })\n",
    "    \n",
    "    # TOTAL_COST_BINNED_ENCODED\n",
    "    travel_info['TOTAL_COST_BINNED_ENCODED'] = travel_info['TOTAL_COST'][-1]\n",
    "    \n",
    "    # Ïª¨Îüº ÌïÑÌÑ∞ÎßÅ (ÏàúÏÑúÏóê ÎßûÍ≤å)\n",
    "    travel_info = {k: int(travel_info[k]) for k in travel_feature_cols}\n",
    "    \n",
    "    return pd.DataFrame([travel_info]).fillna(0).astype(np.float32).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ï¥àÍ∏∞ Ï∂îÏ≤ú top-10: [2305270002, 2308140010, 2305290005, 2305250003, 2308260005, 2307010002, 2309010006, 2306030006, 2307020002, 2305240004]\n",
      "üö´ ÎûúÎç§ÏúºÎ°ú ÏÑ†ÌÉùÎêú Ïã´Ïñ¥Ïöî Ïû•ÏÜå ID: [2308140010, 2306030006, 2307020002]\n",
      "[126.492769, 33.507079] [126.907292, 37.5160148]\n",
      "üåÄ Ïã´Ïñ¥Ïöî Ïû•ÏÜå 2308140010 (middle) ‚Üí ÎåÄÏ≤¥ Ï∂îÏ≤ú 2305270002\n",
      "[126.9780638, 37.6587628] [127.02502436907358, 37.476058309968245]\n",
      "üåÄ Ïã´Ïñ¥Ïöî Ïû•ÏÜå 2306030006 (middle) ‚Üí ÎåÄÏ≤¥ Ï∂îÏ≤ú 2308250004\n",
      "[127.5308669, 37.5129607] [127.02502436907358, 37.476058309968245]\n",
      "üåÄ Ïã´Ïñ¥Ïöî Ïû•ÏÜå 2307020002 (middle) ‚Üí ÎåÄÏ≤¥ Ï∂îÏ≤ú 2305280005\n",
      "‚ú® Ïã´Ïñ¥Ïöî Î∞òÏòÅ ÌõÑ Ï∂îÏ≤ú top-10: [2305270002, 2305290005, 2305250003, 2308260005, 2307010002, 2309010006, 2305240004, 2305270002, 2307080004, 2308270002]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Ïó¨Ìñâ Ï†ïÎ≥¥ Ï†ÑÏ≤òÎ¶¨\n",
    "test_travel = {\n",
    "    'mission_ENC': '0,1',\n",
    "    'date_range': '2025-09-28 - 2025-10-31',\n",
    "    'start_date': '',\n",
    "    'end_date': '',\n",
    "    'TOTAL_COST': '1',\n",
    "    'MVMN_NM_ENC': '2',\n",
    "    'whowith_ENC': '1',\n",
    "    'mission_type': 'normal'\n",
    "}\n",
    "test_travel_tensor = process_travel_input(test_travel)\n",
    "test_travel_tensor = torch.tensor(test_travel_tensor, dtype=torch.float32).to(device)\n",
    "\n",
    "# GNN Ï∂îÎ°†\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_visit_area_embeddings = model(data, test_travel_tensor)  # (21384, 34)\n",
    "\n",
    "# Ï∂îÏ≤ú top-10\n",
    "scores = predicted_visit_area_embeddings.norm(dim=1)\n",
    "topk_indices = torch.topk(scores, k=10).indices\n",
    "topk_recommend_area_ids = [visit_area_df.iloc[idx][\"VISIT_AREA_ID\"] for idx in topk_indices.tolist()]\n",
    "print(\"‚úÖ Ï¥àÍ∏∞ Ï∂îÏ≤ú top-10:\", topk_recommend_area_ids)\n",
    "\n",
    "# ÎûúÎç§ÏúºÎ°ú Ïã´Ïñ¥Ïöî ÌëúÏãú\n",
    "import random\n",
    "num_dislike = 3\n",
    "disliked_area_ids = random.sample(topk_recommend_area_ids, k=num_dislike)\n",
    "print(\"üö´ ÎûúÎç§ÏúºÎ°ú ÏÑ†ÌÉùÎêú Ïã´Ïñ¥Ïöî Ïû•ÏÜå ID:\", disliked_area_ids)\n",
    "\n",
    "# start/middle/end ÏûêÎèô Í≤∞Ï†ï & Ï¢åÌëú Í∏∞Î∞ò best index Ï∞æÍ∏∞\n",
    "def find_best_replacement(disliked_rows, prev_coords, next_coords, mode):\n",
    "    best_idx = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    for idx, row in disliked_rows.iterrows():\n",
    "        x = row[\"X_COORD\"]\n",
    "        y = row[\"Y_COORD\"]\n",
    "        \n",
    "        if mode == \"start\":\n",
    "            dist_next = np.sqrt((x - next_coords[0])**2 + (y - next_coords[1])**2)\n",
    "            total_dist = dist_next\n",
    "        elif mode == \"end\":\n",
    "            dist_prev = np.sqrt((x - prev_coords[0])**2 + (y - prev_coords[1])**2)\n",
    "            total_dist = dist_prev\n",
    "        else:  # middle\n",
    "            dist_prev = np.sqrt((x - prev_coords[0])**2 + (y - prev_coords[1])**2)\n",
    "            dist_next = np.sqrt((x - next_coords[0])**2 + (y - next_coords[1])**2)\n",
    "            total_dist = dist_prev + dist_next\n",
    "        \n",
    "        if total_dist < min_distance:\n",
    "            min_distance = total_dist\n",
    "            best_idx = idx\n",
    "    return best_idx\n",
    "\n",
    "\n",
    "for disliked_id in disliked_area_ids:\n",
    "    disliked_rows = visit_area_df[visit_area_df[\"VISIT_AREA_ID\"] == disliked_id]\n",
    "    \n",
    "    dislike_loc = topk_recommend_area_ids.index(disliked_id)\n",
    "    \n",
    "    mode = \"start\" if dislike_loc == 0 else \"end\" if dislike_loc == len(topk_indices) - 1 else \"middle\"\n",
    "    \n",
    "    if mode == 'start':        \n",
    "        prev_coords = visit_area_df[visit_area_df[\"VISIT_AREA_ID\"] == disliked_id][['X_COORD', 'Y_COORD']].values.tolist()[0]\n",
    "        next_coords = visit_area_df[visit_area_df[\"VISIT_AREA_ID\"] == topk_recommend_area_ids[dislike_loc+1]][['X_COORD', 'Y_COORD']].values.tolist()[0]\n",
    "    elif mode == 'end':\n",
    "        prev_coords = visit_area_df[visit_area_df[\"VISIT_AREA_ID\"] == topk_recommend_area_ids[dislike_loc-1]][['X_COORD', 'Y_COORD']].values.tolist()[0]\n",
    "        next_coords = visit_area_df[visit_area_df[\"VISIT_AREA_ID\"] == disliked_id][['X_COORD', 'Y_COORD']].values.tolist()[0]\n",
    "    else:\n",
    "        prev_coords = visit_area_df[visit_area_df[\"VISIT_AREA_ID\"] == topk_recommend_area_ids[dislike_loc-1]][['X_COORD', 'Y_COORD']].values.tolist()[0]\n",
    "        next_coords = visit_area_df[visit_area_df[\"VISIT_AREA_ID\"] == topk_recommend_area_ids[dislike_loc+1]][['X_COORD', 'Y_COORD']].values.tolist()[0]\n",
    "    print(prev_coords, next_coords)\n",
    "    best_idx = find_best_replacement(disliked_rows, prev_coords, next_coords, mode)\n",
    "    \n",
    "    disliked_emb = predicted_visit_area_embeddings[best_idx]\n",
    "    distances = torch.norm(predicted_visit_area_embeddings - disliked_emb, dim=1)\n",
    "    distances[best_idx] = 1e9  # ÏûêÍ∏∞ ÏûêÏã† Ï†úÏô∏\n",
    "    \n",
    "    replacement_idx = torch.argmin(distances).item()\n",
    "    replacement_id = visit_area_df.iloc[replacement_idx][\"VISIT_AREA_ID\"]\n",
    "    \n",
    "    print(f\"üåÄ Ïã´Ïñ¥Ïöî Ïû•ÏÜå {disliked_id} ({mode}) ‚Üí ÎåÄÏ≤¥ Ï∂îÏ≤ú {replacement_id}\")\n",
    "\n",
    "# Ïã´Ïñ¥Ïöî Î∞òÏòÅ ÌõÑ top-10 Ï∂îÏ≤ú\n",
    "disliked_indices = visit_area_df[visit_area_df[\"VISIT_AREA_ID\"].isin(disliked_area_ids)].index.tolist()\n",
    "scores_post = scores.clone()\n",
    "scores_post[disliked_indices] = -1e9  # Ï†úÏô∏\n",
    "\n",
    "topk_indices_post = torch.topk(scores_post, k=10).indices\n",
    "topk_recommend_area_ids_post = [visit_area_df.iloc[idx][\"VISIT_AREA_ID\"] for idx in topk_indices_post.tolist()]\n",
    "print(\"‚ú® Ïã´Ïñ¥Ïöî Î∞òÏòÅ ÌõÑ Ï∂îÏ≤ú top-10:\", topk_recommend_area_ids_post)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
