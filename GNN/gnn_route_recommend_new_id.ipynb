{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì£¼ìš” íŒŒì¼ ê²½ë¡œ\n",
    "move_path = \"../data/VL_csv/move_with_new_id_final.csv\"\n",
    "travel_path = \"tn_travel_processed.csv\"\n",
    "visit_area_path = \"../data/VL_csv/visit_area_with_new_id_final.csv\"\n",
    "\n",
    "# ë°ì´í„° ë¡œë”©\n",
    "move_df = pd.read_csv(move_path)\n",
    "travel_df = pd.read_csv(travel_path)\n",
    "visit_area_df = pd.read_csv(visit_area_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GNN í•™ìŠµìš© ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 2ï¸âƒ£ visit_area feature ìƒì„±\n",
    "visit_area_df['X_COORD'] = visit_area_df['X_COORD'].fillna(visit_area_df['X_COORD'].mean())\n",
    "visit_area_df['Y_COORD'] = visit_area_df['Y_COORD'].fillna(visit_area_df['Y_COORD'].mean())\n",
    "visit_area_df['VISIT_CHC_REASON_CD'] = visit_area_df['VISIT_CHC_REASON_CD'].fillna(0)\n",
    "for col in ['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']:\n",
    "    visit_area_df[col] = visit_area_df[col].fillna(3)\n",
    "\n",
    "features = visit_area_df[['X_COORD', 'Y_COORD']].copy()\n",
    "type_onehot = pd.get_dummies(visit_area_df['VISIT_AREA_TYPE_CD'], prefix='type')\n",
    "reason_onehot = pd.get_dummies(visit_area_df['VISIT_CHC_REASON_CD'], prefix='reason')\n",
    "visit_area_df['DGSTFN_norm'] = (visit_area_df['DGSTFN'] - 1) / 4.0\n",
    "visit_area_df['REVISIT_norm'] = (visit_area_df['REVISIT_INTENTION'] - 1) / 4.0\n",
    "visit_area_df['RCMDTN_norm'] = (visit_area_df['RCMDTN_INTENTION'] - 1) / 4.0\n",
    "features = pd.concat([features, type_onehot, reason_onehot,\n",
    "                      visit_area_df[['DGSTFN_norm', 'REVISIT_norm', 'RCMDTN_norm']]], axis=1)\n",
    "scaler = StandardScaler()\n",
    "visit_area_tensor = scaler.fit_transform(features.to_numpy(dtype=np.float32))\n",
    "\n",
    "# 3ï¸âƒ£ edge_index, edge_attr ìƒì„±\n",
    "edges = []\n",
    "for travel_id, group in move_df.groupby(\"TRAVEL_ID\"):\n",
    "    group = group.sort_values(\"TRIP_ID\").reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        from_id = group.loc[i-1, \"END_NEW_ID\"]\n",
    "        to_id = group.loc[i, \"END_NEW_ID\"]\n",
    "        duration = group.loc[i, \"DURATION_MINUTES\"] if \"DURATION_MINUTES\" in group.columns else 0\n",
    "        transport = group.loc[i, \"MVMN_CD_1\"]\n",
    "        if pd.notna(from_id) and pd.notna(to_id):\n",
    "            edges.append([int(from_id), int(to_id), duration, transport])\n",
    "\n",
    "edges_df = pd.DataFrame(edges, columns=[\"FROM_ID\", \"TO_ID\", \"DURATION_MINUTES\", \"MVMN_CD_1\"])\n",
    "edge_index = torch.tensor(edges_df[[\"FROM_ID\", \"TO_ID\"]].to_numpy().T, dtype=torch.long)\n",
    "edge_attr = edges_df[[\"DURATION_MINUTES\"]].fillna(0).astype(np.float32).to_numpy()\n",
    "edges_df[\"MVMN_TYPE\"] = edges_df[\"MVMN_CD_1\"].apply(lambda code: \"drive\" if code in [1,2,3] else \"public\" if code in [4,5,6,7,8,9,10,11,12,13,50] else \"other\")\n",
    "edges_df[\"is_drive\"] = (edges_df[\"MVMN_TYPE\"] == \"drive\").astype(int)\n",
    "edges_df[\"is_public\"] = (edges_df[\"MVMN_TYPE\"] == \"public\").astype(int)\n",
    "edges_df[\"is_other\"] = (edges_df[\"MVMN_TYPE\"] == \"other\").astype(int)\n",
    "edge_attr = torch.tensor(np.hstack([edge_attr, edges_df[[\"is_drive\", \"is_public\", \"is_other\"]].to_numpy()]), dtype=torch.float32)\n",
    "\n",
    "# 4ï¸âƒ£ travel_tensor ìƒì„±\n",
    "excluded_cols = ['Unnamed: 0', 'TRAVEL_ID', 'TRAVELER_ID']\n",
    "travel_feature_cols = [col for col in travel_df.columns if col not in excluded_cols]\n",
    "travel_tensor = travel_df[travel_feature_cols].fillna(0).astype(np.float32).to_numpy()\n",
    "\n",
    "# 5ï¸âƒ£ ìµœì¢… numpy ì €ì¥\n",
    "np.save(\"../data/VL_csv/visit_area_tensor.npy\", visit_area_tensor)\n",
    "np.save(\"../data/VL_csv/edge_index.npy\", edge_index.numpy())\n",
    "np.save(\"../data/VL_csv/edge_attr.npy\", edge_attr.numpy())\n",
    "np.save(\"../data/VL_csv/travel_tensor.npy\", travel_tensor)\n",
    "\n",
    "print(\"âœ… GNN í•™ìŠµìš© ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# 1ï¸âƒ£ numpyë¡œë¶€í„° ë°ì´í„° ë¡œë“œ\n",
    "visit_area_tensor = np.load(\"../data/VL_csv/visit_area_tensor.npy\")\n",
    "edge_index = np.load(\"../data/VL_csv/edge_index.npy\")\n",
    "edge_attr = np.load(\"../data/VL_csv/edge_attr.npy\")\n",
    "travel_tensor = np.load(\"../data/VL_csv/travel_tensor.npy\")\n",
    "\n",
    "# 2ï¸âƒ£ HeteroData ìƒì„±\n",
    "data = HeteroData()\n",
    "data['visit_area'].x = torch.tensor(visit_area_tensor, dtype=torch.float32)\n",
    "data['visit_area', 'moved_to', 'visit_area'].edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "data['visit_area', 'moved_to', 'visit_area'].edge_attr = torch.tensor(edge_attr, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  visit_area={ x=[21384, 34] },\n",
       "  (visit_area, moved_to, visit_area)={\n",
       "    edge_index=[2, 16232],\n",
       "    edge_attr=[16232, 4],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import random\n",
    "\n",
    "class TravelGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, travel_context_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Linear(out_channels + travel_context_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # ğŸš¨ ì¶œë ¥ í¬ê¸°ë¥¼ targetê³¼ ë§ì¶¤\n",
    "        self.lin_out = nn.Linear(out_channels, 34)\n",
    "\n",
    "    def forward(self, data, travel_context):\n",
    "        x, edge_index = data['visit_area'].x, data['visit_area', 'moved_to', 'visit_area'].edge_index\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = torch.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        travel_context_expanded = travel_context.expand(x.size(0), -1)\n",
    "        x = torch.cat([x, travel_context_expanded], dim=1)\n",
    "        x = self.residual(x)\n",
    "        return self.lin_out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬í–‰ ì •ë³´\n",
    "def process_travel_input(travel_info:dict):\n",
    "    from datetime import datetime\n",
    "    travel_feature_cols = [\n",
    "        'TOTAL_COST_BINNED_ENCODED',\n",
    "        'WITH_PET',\n",
    "        'MONTH',\n",
    "        'DURATION',\n",
    "        'MVMN_ê¸°íƒ€',\n",
    "        'MVMN_ëŒ€ì¤‘êµí†µ',\n",
    "        'MVMN_ìê°€ìš©',\n",
    "        'TRAVEL_PURPOSE_1',\n",
    "        'TRAVEL_PURPOSE_2',\n",
    "        'TRAVEL_PURPOSE_3',\n",
    "        'TRAVEL_PURPOSE_4',\n",
    "        'TRAVEL_PURPOSE_5',\n",
    "        'TRAVEL_PURPOSE_6',\n",
    "        'TRAVEL_PURPOSE_7',\n",
    "        'TRAVEL_PURPOSE_8',\n",
    "        'TRAVEL_PURPOSE_9',\n",
    "        'WHOWITH_2ì¸ì—¬í–‰',\n",
    "        'WHOWITH_ê°€ì¡±ì—¬í–‰',\n",
    "        'WHOWITH_ê¸°íƒ€',\n",
    "        'WHOWITH_ë‹¨ë…ì—¬í–‰',\n",
    "        'WHOWITH_ì¹œêµ¬/ì§€ì¸ ì—¬í–‰']\n",
    "    \n",
    "    \n",
    "    # mission_ENCì— 0 = ë°˜ë ¤ë™ë¬¼ ë™ë°˜ (WITH_PET)\n",
    "    travel_info['mission_ENC'] = travel_info['mission_ENC'].strip().split(',')\n",
    "    if '0' in travel_info['mission_ENC']:\n",
    "        travel_info['WITH_PET'] = 1\n",
    "    else:\n",
    "        travel_info['WITH_PET'] = 0\n",
    "        \n",
    "    # TRAVEL_PURPOSE_1 ~~ TRAVEL_PURPOSE_9 (0ìœ¼ë¡œ ë“¤ì–´ì˜¨ ì…ë ¥ì€ ì œê±°í•´ì¤˜ì•¼ë¨) \n",
    "    for i in range(1,10):\n",
    "        if str(i) in travel_info['mission_ENC']:\n",
    "            travel_info[f'TRAVEL_PURPOSE_{i}'] = 1\n",
    "        else:\n",
    "            travel_info[f'TRAVEL_PURPOSE_{i}'] = 0\n",
    "        \n",
    "    # MONTH\n",
    "    dates = travel_info['date_range'].split(' - ')\n",
    "    travel_info['start_date'] = datetime.strptime(dates[0].strip(), \"%Y-%m-%d\")\n",
    "    travel_info['end_date'] = datetime.strptime(dates[1].strip(), \"%Y-%m-%d\")\n",
    "    \n",
    "    travel_info['MONTH'] = travel_info['end_date'].month\n",
    "    \n",
    "    # DURATION\n",
    "    travel_info['DURATION'] = (travel_info['end_date'] - travel_info['start_date']).days\n",
    "    \n",
    "    # MNVM_ê¸°íƒ€, MVMN_ëŒ€ì¤‘êµí†µ, MVMN_ìê°€ìš©\n",
    "    for m in ['ìê°€ìš©', 'ëŒ€ì¤‘êµí†µ', 'ê¸°íƒ€']:\n",
    "        travel_info[f\"MVMN_{m}\"] = False\n",
    "    \n",
    "    if travel_info['MVMN_NM_ENC'] == '1':\n",
    "        travel_info['MVMN_ìê°€ìš©'] = True\n",
    "    elif travel_info['MVMN_NM_ENC'] == '2':\n",
    "        travel_info['MVMN_ëŒ€ì¤‘êµí†µ'] = True\n",
    "    else:\n",
    "        travel_info['MVMN_ê¸°íƒ€'] = True\n",
    "    \n",
    "    # WHOWITHëŠ” 1ë¶€í„° 5ê¹Œì§€ ìˆ«ìë¡œ ë“¤ì–´ì˜´ -> ì›í•« ì¸ì½”ë”©ìœ¼ë¡œ ìˆ˜ì •í•  ê²ƒ\n",
    "    # dictì— ë“¤ì–´ì˜¤ëŠ” ìˆ«ì ì˜ë¯¸: WHOWITH_ë‹¨ë…ì—¬í–‰, WHOWITH_2ì¸ì—¬í–‰, WHOWITH_ê°€ì¡±ì—¬í–‰, WHOWITH_ì¹œêµ¬/ì§€ì¸ì—¬í–‰, WHOWITH_ê¸°íƒ€\n",
    "    whowith_onehot = [0] * 5\n",
    "    idx = int(travel_info['whowith_ENC']) - 1\n",
    "    if 0 <= idx < 5:\n",
    "        whowith_onehot[idx] = 1\n",
    "    \n",
    "    travel_info.update({\n",
    "    'WHOWITH_ë‹¨ë…ì—¬í–‰': whowith_onehot[0],\n",
    "    'WHOWITH_2ì¸ì—¬í–‰': whowith_onehot[1],\n",
    "    'WHOWITH_ê°€ì¡±ì—¬í–‰': whowith_onehot[2],\n",
    "    'WHOWITH_ì¹œêµ¬/ì§€ì¸ ì—¬í–‰': whowith_onehot[3],\n",
    "    'WHOWITH_ê¸°íƒ€': whowith_onehot[4],\n",
    "    })\n",
    "    \n",
    "    # TOTAL_COST_BINNED_ENCODED\n",
    "    travel_info['TOTAL_COST_BINNED_ENCODED'] = travel_info['TOTAL_COST'][-1]\n",
    "    \n",
    "    # ì»¬ëŸ¼ í•„í„°ë§ (ìˆœì„œì— ë§ê²Œ)\n",
    "    travel_info = {k: int(travel_info[k]) for k in travel_feature_cols}\n",
    "    \n",
    "    return pd.DataFrame([travel_info]).fillna(0).astype(np.float32).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬í–‰ ì •ë³´ ì „ì²˜ë¦¬\n",
    "test_travel = {\n",
    "    'mission_ENC': '0,1',\n",
    "    'date_range': '2025-09-28 - 2025-09-29',\n",
    "    'start_date': '',\n",
    "    'end_date': '',\n",
    "    'TOTAL_COST': '1',\n",
    "    'MVMN_NM_ENC': '2',\n",
    "    'whowith_ENC': '1',\n",
    "    'mission_type': 'normal'\n",
    "}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "test_travel_tensor = process_travel_input(test_travel)\n",
    "test_travel_tensor = torch.tensor(test_travel_tensor, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.6782\n",
      "Epoch 100, Loss: 0.3395\n",
      "Epoch 150, Loss: 0.1440\n",
      "Epoch 200, Loss: 0.0609\n",
      "Epoch 250, Loss: 0.0397\n",
      "Epoch 300, Loss: 0.0284\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ì„ ì–¸ ë¶€ë¶„\n",
    "model = TravelGNN(in_channels=34, hidden_channels=64, out_channels=64, travel_context_dim=test_travel_tensor.shape[1]).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()  # ì˜ˆ: feature reconstruction\n",
    "\n",
    "data = data.to(device)\n",
    "\n",
    "# ì˜ˆì‹œ: travel_tensor ì¤‘ ì²« ì—¬í–‰ì •ë³´ë¡œ í•™ìŠµ\n",
    "travel_context_tensor = torch.tensor(travel_tensor[0:1], dtype=torch.float32).to(device)\n",
    "target = data['visit_area'].x  # ì˜ˆ: ë°©ë¬¸ì§€ feature ìì²´ ë³µì›\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data, travel_context_tensor)\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ˆê¸° ì¶”ì²œ top-10 (ì¤‘ë³µ ì œê±°ë¨):\n",
      "1. 184 | ì œì£¼ êµ­ì œê³µí•­\n",
      "2. 9262 | ê°œí•­ì¥ ë¬¸í™”ë§ˆë‹¹\n",
      "3. 869 | í˜„ëŒ€ í”„ë¦¬ë¯¸ì—„ ì•„ìš¸ë › ì†¡ë„ì \n",
      "4. 3619 | ì ì‹¤ì¢…í•©ìš´ë™ì¥ ì˜¬ë¦¼í”½ ì£¼ê²½ê¸°ì¥\n",
      "5. 182 | ì„œìš¸ ì¥ë¯¸ì¶•ì œ\n",
      "6. 4769 | ë¶€ì²œêµ­ì œíŒíƒ€ìŠ¤í‹±ì˜í™”ì œ\n",
      "7. 3189 | ì„œê°€ ì•¤ ì¿¡ KTX ì„œìš¸ ì—­ì‚¬ì \n",
      "8. 3063 | ì •ë¦‰ë™ êµí†µê´‘ì¥\n",
      "9. 8701 | ìš©ì¸ ì–´ë¦°ì´ ìƒìƒì˜ ìˆ²\n",
      "10. 7570 | ì„±ì•”ì•„íŠ¸ì„¼í„°\n",
      "\n",
      "ğŸš« ëœë¤ìœ¼ë¡œ ì„ íƒëœ ì‹«ì–´ìš” ì¥ì†Œ ID: [3063, 3619, 184]\n",
      "ğŸŒ€ ì‹«ì–´ìš” ì¥ì†Œ 3063 (middle) â†’ ëŒ€ì²´ ì¶”ì²œ 3196 | ë°°ë‹¤ë¦¬ ìƒíƒœê³µì›\n",
      "ğŸŒ€ ì‹«ì–´ìš” ì¥ì†Œ 3619 (middle) â†’ ëŒ€ì²´ ì¶”ì²œ 1785 | í¬ë¦¬ìŠ¤ ì›”ë“œ\n",
      "ğŸŒ€ ì‹«ì–´ìš” ì¥ì†Œ 184 (start) â†’ ëŒ€ì²´ ì¶”ì²œ 8896 | ì°¨ì´ë‚˜íƒ€ìš´ì \n",
      "\n",
      "âœ¨ ìµœì¢… ëŒ€ì²´ + ì¤‘ë³µ ì œê±°ëœ top-10 ì¶”ì²œ (ì™„ì„±):\n",
      "1. 8896 | ì°¨ì´ë‚˜íƒ€ìš´ì \n",
      "2. 9262 | ê°œí•­ì¥ ë¬¸í™”ë§ˆë‹¹\n",
      "3. 869 | í˜„ëŒ€ í”„ë¦¬ë¯¸ì—„ ì•„ìš¸ë › ì†¡ë„ì \n",
      "4. 1785 | í¬ë¦¬ìŠ¤ ì›”ë“œ\n",
      "5. 182 | ì„œìš¸ ì¥ë¯¸ì¶•ì œ\n",
      "6. 4769 | ë¶€ì²œêµ­ì œíŒíƒ€ìŠ¤í‹±ì˜í™”ì œ\n",
      "7. 3189 | ì„œê°€ ì•¤ ì¿¡ KTX ì„œìš¸ ì—­ì‚¬ì \n",
      "8. 3196 | ë°°ë‹¤ë¦¬ ìƒíƒœê³µì›\n",
      "9. 8701 | ìš©ì¸ ì–´ë¦°ì´ ìƒìƒì˜ ìˆ²\n",
      "10. 7570 | ì„±ì•”ì•„íŠ¸ì„¼í„°\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# ì—¬í–‰ ì •ë³´ ì „ì²˜ë¦¬\n",
    "test_travel = {\n",
    "    'mission_ENC': '0,1',\n",
    "    'date_range': '2025-09-28 - 2025-09-29',\n",
    "    'start_date': '',\n",
    "    'end_date': '',\n",
    "    'TOTAL_COST': '1',\n",
    "    'MVMN_NM_ENC': '2',\n",
    "    'whowith_ENC': '1',\n",
    "    'mission_type': 'normal'\n",
    "}\n",
    "test_travel_tensor = process_travel_input(test_travel)\n",
    "test_travel_tensor = torch.tensor(test_travel_tensor, dtype=torch.float32).to(device)\n",
    "\n",
    "# GNN ì¶”ë¡ \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_visit_area_embeddings = model(data, test_travel_tensor)\n",
    "\n",
    "# âœ… ì¶”ì²œ top-10 (ì¤‘ë³µ ì œê±°)\n",
    "scores = predicted_visit_area_embeddings.norm(dim=1)\n",
    "top_indices_ranked = torch.topk(scores, k=len(scores)).indices.tolist()\n",
    "used_ids = set()\n",
    "unique_topk_list = []\n",
    "for idx in top_indices_ranked:\n",
    "    candidate = visit_area_df.iloc[idx]\n",
    "    cand_id = candidate[\"NEW_VISIT_AREA_ID\"]\n",
    "    if cand_id not in used_ids:\n",
    "        unique_topk_list.append([\n",
    "            cand_id, candidate[\"VISIT_AREA_NM\"],\n",
    "            candidate[\"X_COORD\"], candidate[\"Y_COORD\"]\n",
    "        ])\n",
    "        used_ids.add(cand_id)\n",
    "    if len(unique_topk_list) == 10:\n",
    "        break\n",
    "\n",
    "print(\"âœ… ì´ˆê¸° ì¶”ì²œ top-10 (ì¤‘ë³µ ì œê±°ë¨):\")\n",
    "for i, (area_id, area_nm, _, _) in enumerate(unique_topk_list, 1):\n",
    "    print(f\"{i}. {area_id} | {area_nm}\")\n",
    "\n",
    "# ëœë¤ìœ¼ë¡œ ì‹«ì–´ìš” í‘œì‹œ\n",
    "num_dislike = 3\n",
    "disliked_area_ids = random.sample([area_id for area_id, _, _, _ in unique_topk_list], k=num_dislike)\n",
    "print(\"\\nğŸš« ëœë¤ìœ¼ë¡œ ì„ íƒëœ ì‹«ì–´ìš” ì¥ì†Œ ID:\", disliked_area_ids)\n",
    "\n",
    "# ëŒ€ì²´ ì¶”ì²œ\n",
    "final_topk_list = unique_topk_list.copy()\n",
    "used_area_ids = set(area_id for area_id, _, _, _ in final_topk_list)\n",
    "\n",
    "for disliked_id in disliked_area_ids:\n",
    "    dislike_loc = [area_id for area_id, _, _, _ in final_topk_list].index(disliked_id)\n",
    "    mode = \"start\" if dislike_loc == 0 else \"end\" if dislike_loc == len(final_topk_list) - 1 else \"middle\"\n",
    "    \n",
    "    if mode == \"start\":\n",
    "        next_coords = final_topk_list[dislike_loc+1][2:4]\n",
    "    elif mode == \"end\":\n",
    "        prev_coords = final_topk_list[dislike_loc-1][2:4]\n",
    "    else:\n",
    "        prev_coords = final_topk_list[dislike_loc-1][2:4]\n",
    "        next_coords = final_topk_list[dislike_loc+1][2:4]\n",
    "    \n",
    "    best_replacement_id, best_replacement_nm, best_coords = None, None, None\n",
    "    min_total_dist = float('inf')\n",
    "\n",
    "    for idx in top_indices_ranked:\n",
    "        candidate = visit_area_df.iloc[idx]\n",
    "        cand_coords = [candidate[\"X_COORD\"], candidate[\"Y_COORD\"]]\n",
    "        cand_id = candidate[\"NEW_VISIT_AREA_ID\"]\n",
    "        if cand_id in used_area_ids:\n",
    "            continue\n",
    "\n",
    "        if mode == \"start\":\n",
    "            total_dist = np.linalg.norm(np.array(cand_coords) - np.array(next_coords))\n",
    "        elif mode == \"end\":\n",
    "            total_dist = np.linalg.norm(np.array(cand_coords) - np.array(prev_coords))\n",
    "        else:\n",
    "            dist_prev = np.linalg.norm(np.array(cand_coords) - np.array(prev_coords))\n",
    "            dist_next = np.linalg.norm(np.array(cand_coords) - np.array(next_coords))\n",
    "            total_dist = dist_prev + dist_next\n",
    "\n",
    "        if total_dist < min_total_dist:\n",
    "            min_total_dist = total_dist\n",
    "            best_replacement_id = cand_id\n",
    "            best_replacement_nm = candidate[\"VISIT_AREA_NM\"]\n",
    "            best_coords = cand_coords\n",
    "\n",
    "        if min_total_dist == 0:\n",
    "            break\n",
    "\n",
    "    print(f\"ğŸŒ€ ì‹«ì–´ìš” ì¥ì†Œ {disliked_id} ({mode}) â†’ ëŒ€ì²´ ì¶”ì²œ {best_replacement_id} | {best_replacement_nm}\")\n",
    "    final_topk_list[dislike_loc] = [best_replacement_id, best_replacement_nm, best_coords[0], best_coords[1]]\n",
    "    used_area_ids.add(best_replacement_id)\n",
    "\n",
    "print(\"\\nâœ¨ ìµœì¢… ëŒ€ì²´ + ì¤‘ë³µ ì œê±°ëœ top-10 ì¶”ì²œ (ì™„ì„±):\")\n",
    "for idx, (area_id, area_nm, _, _) in enumerate(final_topk_list, 1):\n",
    "    print(f\"{idx}. {area_id} | {area_nm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŒŸ ìµœì¢… ì—¬í–‰ ì¼ì • (ì´ 2ì¼, Day1â†’Day2 ì—°ê²° ìµœì í™” + ê±°ë¦¬ê¸°ë°˜ í•„í„°ë§):\n",
      "\n",
      "ğŸ“… Day 1 (ì„ê³„ê°’ 2.18):\n",
      " - 8896 | ì°¨ì´ë‚˜íƒ€ìš´ì  (ê±°ë¦¬ 0.92)\n",
      " - 869 | í˜„ëŒ€ í”„ë¦¬ë¯¸ì—„ ì•„ìš¸ë › ì†¡ë„ì  (ê±°ë¦¬ 0.82)\n",
      " - 1785 | í¬ë¦¬ìŠ¤ ì›”ë“œ (ê±°ë¦¬ 0.94)\n",
      " âŒ 3189 | ì„œê°€ ì•¤ ì¿¡ KTX ì„œìš¸ ì—­ì‚¬ì  (ê±°ë¦¬ 3.07 > 2.18 â†’ ì œì™¸ë¨)\n",
      " - 3196 | ë°°ë‹¤ë¦¬ ìƒíƒœê³µì› (ê±°ë¦¬ 0.55)\n",
      "\n",
      "ğŸ“… Day 2 (ì„ê³„ê°’ 0.29):\n",
      " - 8701 | ìš©ì¸ ì–´ë¦°ì´ ìƒìƒì˜ ìˆ² (ê±°ë¦¬ 0.27)\n",
      " âŒ 9262 | ê°œí•­ì¥ ë¬¸í™”ë§ˆë‹¹ (ê±°ë¦¬ 0.36 > 0.29 â†’ ì œì™¸ë¨)\n",
      " - 182 | ì„œìš¸ ì¥ë¯¸ì¶•ì œ (ê±°ë¦¬ 0.06)\n",
      " - 4769 | ë¶€ì²œêµ­ì œíŒíƒ€ìŠ¤í‹±ì˜í™”ì œ (ê±°ë¦¬ 0.06)\n",
      " - 7570 | ì„±ì•”ì•„íŠ¸ì„¼í„° (ê±°ë¦¬ 0.10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# âœ… Dayë³„ í´ëŸ¬ìŠ¤í„°ë§ (ì˜ˆ: ì—¬í–‰ê¸°ê°„ 2ì¼)\n",
    "travel_duration = int(test_travel_tensor[0, 3].item())\n",
    "k = min((travel_duration + 1), len(final_topk_list))\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# top-k embedding ì¶”ì¶œ\n",
    "embeddings_topk = predicted_visit_area_embeddings[\n",
    "    [visit_area_df[visit_area_df[\"NEW_VISIT_AREA_ID\"] == area_id].index[0]\n",
    "     for area_id, _, _, _ in final_topk_list]\n",
    "].cpu().numpy()\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "day_labels = kmeans.fit_predict(embeddings_topk)\n",
    "\n",
    "# âœ… Dayë³„ ê·¸ë£¹ + Day1â†’Day2 ì—°ê²°ì„± ìµœì í™”\n",
    "day_groups = {day: [] for day in range(k)}\n",
    "for idx, label in enumerate(day_labels):\n",
    "    day_groups[label].append(final_topk_list[idx])\n",
    "\n",
    "# Dayë³„ ì¤‘ì‹¬ì¢Œí‘œ\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Day1 ë§ˆì§€ë§‰ â†’ Day2 ì²« ì¥ì†Œ ì—°ê²° ìµœì í™”\n",
    "if k >= 2:\n",
    "    day1_last_coords = day_groups[0][-1][2:4]\n",
    "    best_idx, min_dist = -1, float('inf')\n",
    "    for i, (_, _, x, y) in enumerate(day_groups[1]):\n",
    "        dist = np.linalg.norm(np.array(day1_last_coords) - np.array([x, y]))\n",
    "        if dist < min_dist:\n",
    "            min_dist, best_idx = dist, i\n",
    "    day_groups[1] = [day_groups[1][best_idx]] + [day_groups[1][i] for i in range(len(day_groups[1])) if i != best_idx]\n",
    "\n",
    "# âœ… ìµœì¢… ì¶œë ¥ + Dayë³„ ê±°ë¦¬ ê¸°ë°˜ í•„í„°ë§\n",
    "print(f\"\\nğŸŒŸ ìµœì¢… ì—¬í–‰ ì¼ì • (ì´ {k}ì¼, Day1â†’Day2 ì—°ê²° ìµœì í™” + ê±°ë¦¬ê¸°ë°˜ í•„í„°ë§):\")\n",
    "for day in range(k):\n",
    "    # Dayë³„ X, Y ì¢Œí‘œë§Œ ì¶”ì¶œ\n",
    "    cluster_points_coords = np.array([[x, y] for _, _, x, y in day_groups[day]])\n",
    "    centroid_coords = cluster_points_coords.mean(axis=0)  # Dayë³„ ì¤‘ì‹¬ (2ì°¨ì›)\n",
    "\n",
    "    # X, Y ê¸°ë°˜ ê±°ë¦¬\n",
    "    dists = np.linalg.norm(cluster_points_coords - centroid_coords, axis=1)\n",
    "    avg_dist = np.mean(dists)\n",
    "    dist_threshold = avg_dist+np.std(dists)\n",
    "\n",
    "    print(f\"\\nğŸ“… Day {day+1} (ì„ê³„ê°’ {dist_threshold:.2f}):\")\n",
    "    for i, (area_id, area_nm, x, y) in enumerate(day_groups[day]):\n",
    "        dist = dists[i]\n",
    "        if dist <= dist_threshold:\n",
    "            print(f\" - {area_id} | {area_nm} (ê±°ë¦¬ {dist:.2f})\")\n",
    "        else:\n",
    "            print(f\" âŒ {area_id} | {area_nm} (ê±°ë¦¬ {dist:.2f} > {dist_threshold:.2f} â†’ ì œì™¸ë¨)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
