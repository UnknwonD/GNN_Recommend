{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï£ºÏöî ÌååÏùº Í≤ΩÎ°ú\n",
    "move_path = \"../data/VL_csv/move_with_new_id_final.csv\"\n",
    "travel_path = \"tn_travel_processed.csv\"\n",
    "visit_area_path = \"../data/VL_csv/visit_area_with_new_id_final.csv\"\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎî©\n",
    "move_df = pd.read_csv(move_path)\n",
    "travel_df = pd.read_csv(travel_path)\n",
    "visit_area_df = pd.read_csv(visit_area_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GNN ÌïôÏäµÏö© Ï†ÑÏ≤òÎ¶¨ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 2Ô∏è‚É£ visit_area feature ÏÉùÏÑ±\n",
    "visit_area_df['X_COORD'] = visit_area_df['X_COORD'].fillna(visit_area_df['X_COORD'].mean())\n",
    "visit_area_df['Y_COORD'] = visit_area_df['Y_COORD'].fillna(visit_area_df['Y_COORD'].mean())\n",
    "visit_area_df['VISIT_CHC_REASON_CD'] = visit_area_df['VISIT_CHC_REASON_CD'].fillna(0)\n",
    "for col in ['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']:\n",
    "    visit_area_df[col] = visit_area_df[col].fillna(3)\n",
    "\n",
    "features = visit_area_df[['X_COORD', 'Y_COORD']].copy()\n",
    "type_onehot = pd.get_dummies(visit_area_df['VISIT_AREA_TYPE_CD'], prefix='type')\n",
    "reason_onehot = pd.get_dummies(visit_area_df['VISIT_CHC_REASON_CD'], prefix='reason')\n",
    "visit_area_df['DGSTFN_norm'] = (visit_area_df['DGSTFN'] - 1) / 4.0\n",
    "visit_area_df['REVISIT_norm'] = (visit_area_df['REVISIT_INTENTION'] - 1) / 4.0\n",
    "visit_area_df['RCMDTN_norm'] = (visit_area_df['RCMDTN_INTENTION'] - 1) / 4.0\n",
    "features = pd.concat([features, type_onehot, reason_onehot,\n",
    "                      visit_area_df[['DGSTFN_norm', 'REVISIT_norm', 'RCMDTN_norm']]], axis=1)\n",
    "scaler = StandardScaler()\n",
    "visit_area_tensor = scaler.fit_transform(features.to_numpy(dtype=np.float32))\n",
    "\n",
    "# 3Ô∏è‚É£ edge_index, edge_attr ÏÉùÏÑ±\n",
    "edges = []\n",
    "for travel_id, group in move_df.groupby(\"TRAVEL_ID\"):\n",
    "    group = group.sort_values(\"TRIP_ID\").reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        from_id = group.loc[i-1, \"END_NEW_ID\"]\n",
    "        to_id = group.loc[i, \"END_NEW_ID\"]\n",
    "        duration = group.loc[i, \"DURATION_MINUTES\"] if \"DURATION_MINUTES\" in group.columns else 0\n",
    "        transport = group.loc[i, \"MVMN_CD_1\"]\n",
    "        if pd.notna(from_id) and pd.notna(to_id):\n",
    "            edges.append([int(from_id), int(to_id), duration, transport])\n",
    "\n",
    "edges_df = pd.DataFrame(edges, columns=[\"FROM_ID\", \"TO_ID\", \"DURATION_MINUTES\", \"MVMN_CD_1\"])\n",
    "edge_index = torch.tensor(edges_df[[\"FROM_ID\", \"TO_ID\"]].to_numpy().T, dtype=torch.long)\n",
    "edge_attr = edges_df[[\"DURATION_MINUTES\"]].fillna(0).astype(np.float32).to_numpy()\n",
    "edges_df[\"MVMN_TYPE\"] = edges_df[\"MVMN_CD_1\"].apply(lambda code: \"drive\" if code in [1,2,3] else \"public\" if code in [4,5,6,7,8,9,10,11,12,13,50] else \"other\")\n",
    "edges_df[\"is_drive\"] = (edges_df[\"MVMN_TYPE\"] == \"drive\").astype(int)\n",
    "edges_df[\"is_public\"] = (edges_df[\"MVMN_TYPE\"] == \"public\").astype(int)\n",
    "edges_df[\"is_other\"] = (edges_df[\"MVMN_TYPE\"] == \"other\").astype(int)\n",
    "edge_attr = torch.tensor(np.hstack([edge_attr, edges_df[[\"is_drive\", \"is_public\", \"is_other\"]].to_numpy()]), dtype=torch.float32)\n",
    "\n",
    "# 4Ô∏è‚É£ travel_tensor ÏÉùÏÑ±\n",
    "excluded_cols = ['Unnamed: 0', 'TRAVEL_ID', 'TRAVELER_ID']\n",
    "travel_feature_cols = [col for col in travel_df.columns if col not in excluded_cols]\n",
    "travel_tensor = travel_df[travel_feature_cols].fillna(0).astype(np.float32).to_numpy()\n",
    "\n",
    "# 5Ô∏è‚É£ ÏµúÏ¢Ö numpy Ï†ÄÏû•\n",
    "np.save(\"../data/VL_csv/visit_area_tensor.npy\", visit_area_tensor)\n",
    "np.save(\"../data/VL_csv/edge_index.npy\", edge_index.numpy())\n",
    "np.save(\"../data/VL_csv/edge_attr.npy\", edge_attr.numpy())\n",
    "np.save(\"../data/VL_csv/travel_tensor.npy\", travel_tensor)\n",
    "\n",
    "print(\"‚úÖ GNN ÌïôÏäµÏö© Ï†ÑÏ≤òÎ¶¨ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• ÏôÑÎ£å!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# 1Ô∏è‚É£ numpyÎ°úÎ∂ÄÌÑ∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "visit_area_tensor = np.load(\"../data/VL_csv/visit_area_tensor.npy\")\n",
    "edge_index = np.load(\"../data/VL_csv/edge_index.npy\")\n",
    "edge_attr = np.load(\"../data/VL_csv/edge_attr.npy\")\n",
    "travel_tensor = np.load(\"../data/VL_csv/travel_tensor.npy\")\n",
    "\n",
    "# 2Ô∏è‚É£ HeteroData ÏÉùÏÑ±\n",
    "data = HeteroData()\n",
    "data['visit_area'].x = torch.tensor(visit_area_tensor, dtype=torch.float32)\n",
    "data['visit_area', 'moved_to', 'visit_area'].edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "data['visit_area', 'moved_to', 'visit_area'].edge_attr = torch.tensor(edge_attr, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  visit_area={ x=[21384, 34] },\n",
       "  (visit_area, moved_to, visit_area)={\n",
       "    edge_index=[2, 16232],\n",
       "    edge_attr=[16232, 4],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import random\n",
    "\n",
    "class TravelGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, travel_context_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Linear(out_channels + travel_context_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # üö® Ï∂úÎ†• ÌÅ¨Í∏∞Î•º targetÍ≥º ÎßûÏ∂§\n",
    "        self.lin_out = nn.Linear(out_channels, 34)\n",
    "\n",
    "    def forward(self, data, travel_context):\n",
    "        x, edge_index = data['visit_area'].x, data['visit_area', 'moved_to', 'visit_area'].edge_index\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = torch.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        travel_context_expanded = travel_context.expand(x.size(0), -1)\n",
    "        x = torch.cat([x, travel_context_expanded], dim=1)\n",
    "        x = self.residual(x)\n",
    "        return self.lin_out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ïó¨Ìñâ Ï†ïÎ≥¥\n",
    "def process_travel_input(travel_info:dict):\n",
    "    from datetime import datetime\n",
    "    travel_feature_cols = [\n",
    "        'TOTAL_COST_BINNED_ENCODED',\n",
    "        'WITH_PET',\n",
    "        'MONTH',\n",
    "        'DURATION',\n",
    "        'MVMN_Í∏∞ÌÉÄ',\n",
    "        'MVMN_ÎåÄÏ§ëÍµêÌÜµ',\n",
    "        'MVMN_ÏûêÍ∞ÄÏö©',\n",
    "        'TRAVEL_PURPOSE_1',\n",
    "        'TRAVEL_PURPOSE_2',\n",
    "        'TRAVEL_PURPOSE_3',\n",
    "        'TRAVEL_PURPOSE_4',\n",
    "        'TRAVEL_PURPOSE_5',\n",
    "        'TRAVEL_PURPOSE_6',\n",
    "        'TRAVEL_PURPOSE_7',\n",
    "        'TRAVEL_PURPOSE_8',\n",
    "        'TRAVEL_PURPOSE_9',\n",
    "        'WHOWITH_2Ïù∏Ïó¨Ìñâ',\n",
    "        'WHOWITH_Í∞ÄÏ°±Ïó¨Ìñâ',\n",
    "        'WHOWITH_Í∏∞ÌÉÄ',\n",
    "        'WHOWITH_Îã®ÎèÖÏó¨Ìñâ',\n",
    "        'WHOWITH_ÏπúÍµ¨/ÏßÄÏù∏ Ïó¨Ìñâ']\n",
    "    \n",
    "    \n",
    "    # mission_ENCÏóê 0 = Î∞òÎ†§ÎèôÎ¨º ÎèôÎ∞ò (WITH_PET)\n",
    "    travel_info['mission_ENC'] = travel_info['mission_ENC'].strip().split(',')\n",
    "    if '0' in travel_info['mission_ENC']:\n",
    "        travel_info['WITH_PET'] = 1\n",
    "    else:\n",
    "        travel_info['WITH_PET'] = 0\n",
    "        \n",
    "    # TRAVEL_PURPOSE_1 ~~ TRAVEL_PURPOSE_9 (0ÏúºÎ°ú Îì§Ïñ¥Ïò® ÏûÖÎ†•ÏùÄ Ï†úÍ±∞Ìï¥Ï§òÏïºÎê®) \n",
    "    for i in range(1,10):\n",
    "        if str(i) in travel_info['mission_ENC']:\n",
    "            travel_info[f'TRAVEL_PURPOSE_{i}'] = 1\n",
    "        else:\n",
    "            travel_info[f'TRAVEL_PURPOSE_{i}'] = 0\n",
    "        \n",
    "    # MONTH\n",
    "    dates = travel_info['date_range'].split(' - ')\n",
    "    travel_info['start_date'] = datetime.strptime(dates[0].strip(), \"%Y-%m-%d\")\n",
    "    travel_info['end_date'] = datetime.strptime(dates[1].strip(), \"%Y-%m-%d\")\n",
    "    \n",
    "    travel_info['MONTH'] = travel_info['end_date'].month\n",
    "    \n",
    "    # DURATION\n",
    "    travel_info['DURATION'] = (travel_info['end_date'] - travel_info['start_date']).days\n",
    "    \n",
    "    # MNVM_Í∏∞ÌÉÄ, MVMN_ÎåÄÏ§ëÍµêÌÜµ, MVMN_ÏûêÍ∞ÄÏö©\n",
    "    for m in ['ÏûêÍ∞ÄÏö©', 'ÎåÄÏ§ëÍµêÌÜµ', 'Í∏∞ÌÉÄ']:\n",
    "        travel_info[f\"MVMN_{m}\"] = False\n",
    "    \n",
    "    if travel_info['MVMN_NM_ENC'] == '1':\n",
    "        travel_info['MVMN_ÏûêÍ∞ÄÏö©'] = True\n",
    "    elif travel_info['MVMN_NM_ENC'] == '2':\n",
    "        travel_info['MVMN_ÎåÄÏ§ëÍµêÌÜµ'] = True\n",
    "    else:\n",
    "        travel_info['MVMN_Í∏∞ÌÉÄ'] = True\n",
    "    \n",
    "    # WHOWITHÎäî 1Î∂ÄÌÑ∞ 5ÍπåÏßÄ Ïà´ÏûêÎ°ú Îì§Ïñ¥Ïò¥ -> ÏõêÌï´ Ïù∏ÏΩîÎî©ÏúºÎ°ú ÏàòÏ†ïÌï† Í≤É\n",
    "    # dictÏóê Îì§Ïñ¥Ïò§Îäî Ïà´Ïûê ÏùòÎØ∏: WHOWITH_Îã®ÎèÖÏó¨Ìñâ, WHOWITH_2Ïù∏Ïó¨Ìñâ, WHOWITH_Í∞ÄÏ°±Ïó¨Ìñâ, WHOWITH_ÏπúÍµ¨/ÏßÄÏù∏Ïó¨Ìñâ, WHOWITH_Í∏∞ÌÉÄ\n",
    "    whowith_onehot = [0] * 5\n",
    "    idx = int(travel_info['whowith_ENC']) - 1\n",
    "    if 0 <= idx < 5:\n",
    "        whowith_onehot[idx] = 1\n",
    "    \n",
    "    travel_info.update({\n",
    "    'WHOWITH_Îã®ÎèÖÏó¨Ìñâ': whowith_onehot[0],\n",
    "    'WHOWITH_2Ïù∏Ïó¨Ìñâ': whowith_onehot[1],\n",
    "    'WHOWITH_Í∞ÄÏ°±Ïó¨Ìñâ': whowith_onehot[2],\n",
    "    'WHOWITH_ÏπúÍµ¨/ÏßÄÏù∏ Ïó¨Ìñâ': whowith_onehot[3],\n",
    "    'WHOWITH_Í∏∞ÌÉÄ': whowith_onehot[4],\n",
    "    })\n",
    "    \n",
    "    # TOTAL_COST_BINNED_ENCODED\n",
    "    travel_info['TOTAL_COST_BINNED_ENCODED'] = travel_info['TOTAL_COST'][-1]\n",
    "    \n",
    "    # Ïª¨Îüº ÌïÑÌÑ∞ÎßÅ (ÏàúÏÑúÏóê ÎßûÍ≤å)\n",
    "    travel_info = {k: int(travel_info[k]) for k in travel_feature_cols}\n",
    "    \n",
    "    return pd.DataFrame([travel_info]).fillna(0).astype(np.float32).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ïó¨Ìñâ Ï†ïÎ≥¥ Ï†ÑÏ≤òÎ¶¨\n",
    "test_travel = {\n",
    "    'mission_ENC': '0,1',\n",
    "    'date_range': '2025-09-28 - 2025-09-29',\n",
    "    'start_date': '',\n",
    "    'end_date': '',\n",
    "    'TOTAL_COST': '1',\n",
    "    'MVMN_NM_ENC': '2',\n",
    "    'whowith_ENC': '1',\n",
    "    'mission_type': 'normal'\n",
    "}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "test_travel_tensor = process_travel_input(test_travel)\n",
    "test_travel_tensor = torch.tensor(test_travel_tensor, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.6782\n",
      "Epoch 100, Loss: 0.3395\n",
      "Epoch 150, Loss: 0.1440\n",
      "Epoch 200, Loss: 0.0609\n",
      "Epoch 250, Loss: 0.0397\n",
      "Epoch 300, Loss: 0.0284\n"
     ]
    }
   ],
   "source": [
    "# Î™®Îç∏ ÏÑ†Ïñ∏ Î∂ÄÎ∂Ñ\n",
    "model = TravelGNN(in_channels=34, hidden_channels=64, out_channels=64, travel_context_dim=test_travel_tensor.shape[1]).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()  # Ïòà: feature reconstruction\n",
    "\n",
    "data = data.to(device)\n",
    "\n",
    "# ÏòàÏãú: travel_tensor Ï§ë Ï≤´ Ïó¨ÌñâÏ†ïÎ≥¥Î°ú ÌïôÏäµ\n",
    "travel_context_tensor = torch.tensor(travel_tensor[0:1], dtype=torch.float32).to(device)\n",
    "target = data['visit_area'].x  # Ïòà: Î∞©Î¨∏ÏßÄ feature ÏûêÏ≤¥ Î≥µÏõê\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data, travel_context_tensor)\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ï¥àÍ∏∞ Ï∂îÏ≤ú top-10 (Ï§ëÎ≥µ Ï†úÍ±∞Îê®):\n",
      "1. 184 | Ï†úÏ£º Íµ≠Ï†úÍ≥µÌï≠\n",
      "2. 9262 | Í∞úÌï≠Ïû• Î¨∏ÌôîÎßàÎãπ\n",
      "3. 869 | ÌòÑÎåÄ ÌîÑÎ¶¨ÎØ∏ÏóÑ ÏïÑÏö∏Î†õ ÏÜ°ÎèÑÏ†ê\n",
      "4. 3619 | Ïû†Ïã§Ï¢ÖÌï©Ïö¥ÎèôÏû• Ïò¨Î¶ºÌîΩ Ï£ºÍ≤ΩÍ∏∞Ïû•\n",
      "5. 182 | ÏÑúÏö∏ Ïû•ÎØ∏Ï∂ïÏ†ú\n",
      "6. 4769 | Î∂ÄÏ≤úÍµ≠Ï†úÌåêÌÉÄÏä§Ìã±ÏòÅÌôîÏ†ú\n",
      "7. 3189 | ÏÑúÍ∞Ä Ïï§ Ïø° KTX ÏÑúÏö∏ Ïó≠ÏÇ¨Ï†ê\n",
      "8. 3063 | Ï†ïÎ¶âÎèô ÍµêÌÜµÍ¥ëÏû•\n",
      "9. 8701 | Ïö©Ïù∏ Ïñ¥Î¶∞Ïù¥ ÏÉÅÏÉÅÏùò Ïà≤\n",
      "10. 7570 | ÏÑ±ÏïîÏïÑÌä∏ÏÑºÌÑ∞\n",
      "\n",
      "üö´ ÎûúÎç§ÏúºÎ°ú ÏÑ†ÌÉùÎêú Ïã´Ïñ¥Ïöî Ïû•ÏÜå ID: [3063, 3619, 184]\n",
      "üåÄ Ïã´Ïñ¥Ïöî Ïû•ÏÜå 3063 (middle) ‚Üí ÎåÄÏ≤¥ Ï∂îÏ≤ú 3196 | Î∞∞Îã§Î¶¨ ÏÉùÌÉúÍ≥µÏõê\n",
      "üåÄ Ïã´Ïñ¥Ïöî Ïû•ÏÜå 3619 (middle) ‚Üí ÎåÄÏ≤¥ Ï∂îÏ≤ú 1785 | ÌÅ¨Î¶¨Ïä§ ÏõîÎìú\n",
      "üåÄ Ïã´Ïñ¥Ïöî Ïû•ÏÜå 184 (start) ‚Üí ÎåÄÏ≤¥ Ï∂îÏ≤ú 8896 | Ï∞®Ïù¥ÎÇòÌÉÄÏö¥Ï†ê\n",
      "\n",
      "‚ú® ÏµúÏ¢Ö ÎåÄÏ≤¥ + Ï§ëÎ≥µ Ï†úÍ±∞Îêú top-10 Ï∂îÏ≤ú (ÏôÑÏÑ±):\n",
      "1. 8896 | Ï∞®Ïù¥ÎÇòÌÉÄÏö¥Ï†ê\n",
      "2. 9262 | Í∞úÌï≠Ïû• Î¨∏ÌôîÎßàÎãπ\n",
      "3. 869 | ÌòÑÎåÄ ÌîÑÎ¶¨ÎØ∏ÏóÑ ÏïÑÏö∏Î†õ ÏÜ°ÎèÑÏ†ê\n",
      "4. 1785 | ÌÅ¨Î¶¨Ïä§ ÏõîÎìú\n",
      "5. 182 | ÏÑúÏö∏ Ïû•ÎØ∏Ï∂ïÏ†ú\n",
      "6. 4769 | Î∂ÄÏ≤úÍµ≠Ï†úÌåêÌÉÄÏä§Ìã±ÏòÅÌôîÏ†ú\n",
      "7. 3189 | ÏÑúÍ∞Ä Ïï§ Ïø° KTX ÏÑúÏö∏ Ïó≠ÏÇ¨Ï†ê\n",
      "8. 3196 | Î∞∞Îã§Î¶¨ ÏÉùÌÉúÍ≥µÏõê\n",
      "9. 8701 | Ïö©Ïù∏ Ïñ¥Î¶∞Ïù¥ ÏÉÅÏÉÅÏùò Ïà≤\n",
      "10. 7570 | ÏÑ±ÏïîÏïÑÌä∏ÏÑºÌÑ∞\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Ïó¨Ìñâ Ï†ïÎ≥¥ Ï†ÑÏ≤òÎ¶¨\n",
    "test_travel = {\n",
    "    'mission_ENC': '0,1',\n",
    "    'date_range': '2025-09-28 - 2025-09-29',\n",
    "    'start_date': '',\n",
    "    'end_date': '',\n",
    "    'TOTAL_COST': '1',\n",
    "    'MVMN_NM_ENC': '2',\n",
    "    'whowith_ENC': '1',\n",
    "    'mission_type': 'normal'\n",
    "}\n",
    "test_travel_tensor = process_travel_input(test_travel)\n",
    "test_travel_tensor = torch.tensor(test_travel_tensor, dtype=torch.float32).to(device)\n",
    "\n",
    "# GNN Ï∂îÎ°†\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_visit_area_embeddings = model(data, test_travel_tensor)\n",
    "\n",
    "# ‚úÖ Ï∂îÏ≤ú top-10 (Ï§ëÎ≥µ Ï†úÍ±∞)\n",
    "scores = predicted_visit_area_embeddings.norm(dim=1)\n",
    "top_indices_ranked = torch.topk(scores, k=len(scores)).indices.tolist()\n",
    "used_ids = set()\n",
    "unique_topk_list = []\n",
    "for idx in top_indices_ranked:\n",
    "    candidate = visit_area_df.iloc[idx]\n",
    "    cand_id = candidate[\"NEW_VISIT_AREA_ID\"]\n",
    "    if cand_id not in used_ids:\n",
    "        unique_topk_list.append([\n",
    "            cand_id, candidate[\"VISIT_AREA_NM\"],\n",
    "            candidate[\"X_COORD\"], candidate[\"Y_COORD\"]\n",
    "        ])\n",
    "        used_ids.add(cand_id)\n",
    "    if len(unique_topk_list) == 10:\n",
    "        break\n",
    "\n",
    "print(\"‚úÖ Ï¥àÍ∏∞ Ï∂îÏ≤ú top-10 (Ï§ëÎ≥µ Ï†úÍ±∞Îê®):\")\n",
    "for i, (area_id, area_nm, _, _) in enumerate(unique_topk_list, 1):\n",
    "    print(f\"{i}. {area_id} | {area_nm}\")\n",
    "\n",
    "# ÎûúÎç§ÏúºÎ°ú Ïã´Ïñ¥Ïöî ÌëúÏãú\n",
    "num_dislike = 3\n",
    "disliked_area_ids = random.sample([area_id for area_id, _, _, _ in unique_topk_list], k=num_dislike)\n",
    "print(\"\\nüö´ ÎûúÎç§ÏúºÎ°ú ÏÑ†ÌÉùÎêú Ïã´Ïñ¥Ïöî Ïû•ÏÜå ID:\", disliked_area_ids)\n",
    "\n",
    "# ÎåÄÏ≤¥ Ï∂îÏ≤ú\n",
    "final_topk_list = unique_topk_list.copy()\n",
    "used_area_ids = set(area_id for area_id, _, _, _ in final_topk_list)\n",
    "\n",
    "for disliked_id in disliked_area_ids:\n",
    "    dislike_loc = [area_id for area_id, _, _, _ in final_topk_list].index(disliked_id)\n",
    "    mode = \"start\" if dislike_loc == 0 else \"end\" if dislike_loc == len(final_topk_list) - 1 else \"middle\"\n",
    "    \n",
    "    if mode == \"start\":\n",
    "        next_coords = final_topk_list[dislike_loc+1][2:4]\n",
    "    elif mode == \"end\":\n",
    "        prev_coords = final_topk_list[dislike_loc-1][2:4]\n",
    "    else:\n",
    "        prev_coords = final_topk_list[dislike_loc-1][2:4]\n",
    "        next_coords = final_topk_list[dislike_loc+1][2:4]\n",
    "    \n",
    "    best_replacement_id, best_replacement_nm, best_coords = None, None, None\n",
    "    min_total_dist = float('inf')\n",
    "\n",
    "    for idx in top_indices_ranked:\n",
    "        candidate = visit_area_df.iloc[idx]\n",
    "        cand_coords = [candidate[\"X_COORD\"], candidate[\"Y_COORD\"]]\n",
    "        cand_id = candidate[\"NEW_VISIT_AREA_ID\"]\n",
    "        if cand_id in used_area_ids:\n",
    "            continue\n",
    "\n",
    "        if mode == \"start\":\n",
    "            total_dist = np.linalg.norm(np.array(cand_coords) - np.array(next_coords))\n",
    "        elif mode == \"end\":\n",
    "            total_dist = np.linalg.norm(np.array(cand_coords) - np.array(prev_coords))\n",
    "        else:\n",
    "            dist_prev = np.linalg.norm(np.array(cand_coords) - np.array(prev_coords))\n",
    "            dist_next = np.linalg.norm(np.array(cand_coords) - np.array(next_coords))\n",
    "            total_dist = dist_prev + dist_next\n",
    "\n",
    "        if total_dist < min_total_dist:\n",
    "            min_total_dist = total_dist\n",
    "            best_replacement_id = cand_id\n",
    "            best_replacement_nm = candidate[\"VISIT_AREA_NM\"]\n",
    "            best_coords = cand_coords\n",
    "\n",
    "        if min_total_dist == 0:\n",
    "            break\n",
    "\n",
    "    print(f\"üåÄ Ïã´Ïñ¥Ïöî Ïû•ÏÜå {disliked_id} ({mode}) ‚Üí ÎåÄÏ≤¥ Ï∂îÏ≤ú {best_replacement_id} | {best_replacement_nm}\")\n",
    "    final_topk_list[dislike_loc] = [best_replacement_id, best_replacement_nm, best_coords[0], best_coords[1]]\n",
    "    used_area_ids.add(best_replacement_id)\n",
    "\n",
    "print(\"\\n‚ú® ÏµúÏ¢Ö ÎåÄÏ≤¥ + Ï§ëÎ≥µ Ï†úÍ±∞Îêú top-10 Ï∂îÏ≤ú (ÏôÑÏÑ±):\")\n",
    "for idx, (area_id, area_nm, _, _) in enumerate(final_topk_list, 1):\n",
    "    print(f\"{idx}. {area_id} | {area_nm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåü ÏµúÏ¢Ö Ïó¨Ìñâ ÏùºÏ†ï (Ï¥ù 2Ïùº, Day1‚ÜíDay2 Ïó∞Í≤∞ ÏµúÏ†ÅÌôî + Í±∞Î¶¨Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ):\n",
      "\n",
      "üìÖ Day 1 (ÏûÑÍ≥ÑÍ∞í 2.18):\n",
      " - 8896 | Ï∞®Ïù¥ÎÇòÌÉÄÏö¥Ï†ê (Í±∞Î¶¨ 0.92)\n",
      " - 869 | ÌòÑÎåÄ ÌîÑÎ¶¨ÎØ∏ÏóÑ ÏïÑÏö∏Î†õ ÏÜ°ÎèÑÏ†ê (Í±∞Î¶¨ 0.82)\n",
      " - 1785 | ÌÅ¨Î¶¨Ïä§ ÏõîÎìú (Í±∞Î¶¨ 0.94)\n",
      " ‚ùå 3189 | ÏÑúÍ∞Ä Ïï§ Ïø° KTX ÏÑúÏö∏ Ïó≠ÏÇ¨Ï†ê (Í±∞Î¶¨ 3.07 > 2.18 ‚Üí Ï†úÏô∏Îê®)\n",
      " - 3196 | Î∞∞Îã§Î¶¨ ÏÉùÌÉúÍ≥µÏõê (Í±∞Î¶¨ 0.55)\n",
      "\n",
      "üìÖ Day 2 (ÏûÑÍ≥ÑÍ∞í 0.29):\n",
      " - 8701 | Ïö©Ïù∏ Ïñ¥Î¶∞Ïù¥ ÏÉÅÏÉÅÏùò Ïà≤ (Í±∞Î¶¨ 0.27)\n",
      " ‚ùå 9262 | Í∞úÌï≠Ïû• Î¨∏ÌôîÎßàÎãπ (Í±∞Î¶¨ 0.36 > 0.29 ‚Üí Ï†úÏô∏Îê®)\n",
      " - 182 | ÏÑúÏö∏ Ïû•ÎØ∏Ï∂ïÏ†ú (Í±∞Î¶¨ 0.06)\n",
      " - 4769 | Î∂ÄÏ≤úÍµ≠Ï†úÌåêÌÉÄÏä§Ìã±ÏòÅÌôîÏ†ú (Í±∞Î¶¨ 0.06)\n",
      " - 7570 | ÏÑ±ÏïîÏïÑÌä∏ÏÑºÌÑ∞ (Í±∞Î¶¨ 0.10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ‚úÖ DayÎ≥Ñ ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ (Ïòà: Ïó¨ÌñâÍ∏∞Í∞Ñ 2Ïùº)\n",
    "travel_duration = int(test_travel_tensor[0, 3].item())\n",
    "k = min((travel_duration + 1), len(final_topk_list))\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# top-k embedding Ï∂îÏ∂ú\n",
    "embeddings_topk = predicted_visit_area_embeddings[\n",
    "    [visit_area_df[visit_area_df[\"NEW_VISIT_AREA_ID\"] == area_id].index[0]\n",
    "     for area_id, _, _, _ in final_topk_list]\n",
    "].cpu().numpy()\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "day_labels = kmeans.fit_predict(embeddings_topk)\n",
    "\n",
    "# ‚úÖ DayÎ≥Ñ Í∑∏Î£π + Day1‚ÜíDay2 Ïó∞Í≤∞ÏÑ± ÏµúÏ†ÅÌôî\n",
    "day_groups = {day: [] for day in range(k)}\n",
    "for idx, label in enumerate(day_labels):\n",
    "    day_groups[label].append(final_topk_list[idx])\n",
    "\n",
    "# DayÎ≥Ñ Ï§ëÏã¨Ï¢åÌëú\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Day1 ÎßàÏßÄÎßâ ‚Üí Day2 Ï≤´ Ïû•ÏÜå Ïó∞Í≤∞ ÏµúÏ†ÅÌôî\n",
    "if k >= 2:\n",
    "    day1_last_coords = day_groups[0][-1][2:4]\n",
    "    best_idx, min_dist = -1, float('inf')\n",
    "    for i, (_, _, x, y) in enumerate(day_groups[1]):\n",
    "        dist = np.linalg.norm(np.array(day1_last_coords) - np.array([x, y]))\n",
    "        if dist < min_dist:\n",
    "            min_dist, best_idx = dist, i\n",
    "    day_groups[1] = [day_groups[1][best_idx]] + [day_groups[1][i] for i in range(len(day_groups[1])) if i != best_idx]\n",
    "\n",
    "# ‚úÖ ÏµúÏ¢Ö Ï∂úÎ†• + DayÎ≥Ñ Í±∞Î¶¨ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ\n",
    "print(f\"\\nüåü ÏµúÏ¢Ö Ïó¨Ìñâ ÏùºÏ†ï (Ï¥ù {k}Ïùº, Day1‚ÜíDay2 Ïó∞Í≤∞ ÏµúÏ†ÅÌôî + Í±∞Î¶¨Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ):\")\n",
    "for day in range(k):\n",
    "    # DayÎ≥Ñ X, Y Ï¢åÌëúÎßå Ï∂îÏ∂ú\n",
    "    cluster_points_coords = np.array([[x, y] for _, _, x, y in day_groups[day]])\n",
    "    centroid_coords = cluster_points_coords.mean(axis=0)  # DayÎ≥Ñ Ï§ëÏã¨ (2Ï∞®Ïõê)\n",
    "\n",
    "    # X, Y Í∏∞Î∞ò Í±∞Î¶¨\n",
    "    dists = np.linalg.norm(cluster_points_coords - centroid_coords, axis=1)\n",
    "    avg_dist = np.mean(dists)\n",
    "    dist_threshold = avg_dist+np.std(dists)\n",
    "\n",
    "    print(f\"\\nüìÖ Day {day+1} (ÏûÑÍ≥ÑÍ∞í {dist_threshold:.2f}):\")\n",
    "    for i, (area_id, area_nm, x, y) in enumerate(day_groups[day]):\n",
    "        dist = dists[i]\n",
    "        if dist <= dist_threshold:\n",
    "            print(f\" - {area_id} | {area_nm} (Í±∞Î¶¨ {dist:.2f})\")\n",
    "        else:\n",
    "            print(f\" ‚ùå {area_id} | {area_nm} (Í±∞Î¶¨ {dist:.2f} > {dist_threshold:.2f} ‚Üí Ï†úÏô∏Îê®)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
